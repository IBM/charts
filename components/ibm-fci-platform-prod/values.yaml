global:

  persistence:
    # By default, installation assumes that there is a dynamic provisioner
    # that allocates persistent volues using the storageClassName provided
    # above.
    #
    # If a dynamic provisioner is not in use, change useDynamicProvisioning
    # to false and if appropriate, set createPv to true.
    useDynamicProvisioning: true

  # createPv should be left false in cases where the PVs are auto-provisioned
  # or precreated by an administrator.  However, it's possible to use an NFS
  # server, set global.nfsServer to the value, and then set createPv to true,
  # and when installing the chart, the PVs will be created automatically using
  # the NFS Server.  Note that the NFS server must support NFS v4.1 or later.
  createPv: false

  # In some cases, when persistent volumes are created, the filesystem is
  # in a state where the pods will not have permission to write files to the
  # filesystem because they do not have permission.  This is often (but not
  # always) the case with auto-provisioners.
  # Setting this to true will create an initContainer that will run with
  # root-level priviledges in order to mount the volume, fix the filesystem
  # permissions, and then terminate.  If the PVs that are allocated already
  # have appropriate permissions, this field may be set to false.
  pvRequiresPermissionsFix: false

  # runAsUser is the user id that containers requiring updates to file system permissions
  # run as if pvRequiresPermissionsFix is set to true
  runAsUser: 1000

  cloudpakInstanceId: "override"

  networkPolicy:
    ## Enable creation of NetworkPolicy resources.
    ##
    enabled: False

    ## add additional selectors to allow ingress from other namespaces or pods
    # e.g.
    #     - namespaceSelector:
    #         matchLabels:
    #           network.openshift.io/policy-group: mylabel
    ingressSelector:

    # The IP address of the Kubernetes API server
    # The port and the IP address of the Kubernetes API server can be found by
    # running the following command:
    # oc get endpoints --namespace default kubernetes
    kubernetesIp:

    #The port that the Kubernetes API server runs on
    kubernetesPort:

  # Provide the subdomain of the host used to access the cluster for
  # HTTP/HTTPS traffic.  This often can be determined by running this command:
  # oc get route console -n openshift-console | grep -v 'HOST/PORT' | awk '{print $2}'
  # This provides the URL of the OpenShift console.  After removing the
  # hostname up to the first dot, the remaining value will be the subdomain
  # required here.  For example, if the output of the command is:
  # console-openshift-console.apps.fyrecluster.os.fyre.ibm.com
  # then enter `apps.fyrecluster.os.fyre.ibm.com` here.
  # Or type this command
  # oc get route console -n openshift-console | grep -v 'HOST/PORT' | awk '{print $2}' | cut -d'.' -f 2-
  # which should give you `apps.fyrecluster.os.fyre.ibm.com`
  # If unsure, verify with the administrator that configured the OpenShift
  # cluster.  There can be a hardware loadbalancer in front of the cluster
  # making it impossible to determine the correct URL automatically.
  httpsIngressSubdomain: localhost.localdomain

  # Provide the subdomain of the host used to access the cluster for TCP/IP
  # traffic.  This is much more difficult to determine automatically.  In
  # some cases, this is the same as the httpsIngressSubdomain.  In other
  # cases, this could be a different value.  For example, in IBM Cloud (ibm.com/cloud),
  # the correct value for this can be determined by running this command:
  # oc get cm -n kube-system ibm-cloud-cluster-ingress-info -o yaml | grep ' ingress-subdomain'
  # which may output a value like:
  # sl-roks-441333-2ff4384e3bbe791290bc084e4f9f6c9a-0000.tor01.containers.appdomain.cloud
  # If unsure, verify with the administrator that configured the OpenShift
  # cluster.  There can be a hardware loadbalancer in front of the cluster
  # making it impossible to determine the correct URL automatically.
  tcpIngressSubdomain: localhost.localdomain

  # Change to CLOUD_PAK to use DB2 from Cloud Pak
  DB2_PROVISIONER: 'CLOUD_PAK'
  DB2_CONNECTION_NAME: "FCI_DB2"

  # generateInternalPasswords determines whether secrets for internal
  # services are randomly generated.  Set to false to use a pre-existing
  # secret instead of randomly generated values.  In this case, all
  # values must be precreated for all secrets.  See the knowledge center
  # for a list of all of the secrets and the keys that need to be created.
  generateInternalPasswords: true

  # externalSecretName is the name of the OpenShift secret object that
  # contains passwords to external services, such as LDAP.  This value
  # does not normally need to be changed.
  externalSecretName: fci-platform-external-secrets-env

  # The product name to brand the login page
  productName: "IBM Financial Crimes Insight"

  # Set to "true" to update the branding of the Cloud Pak for Data login page.
  # Set to "false" otherwise.
  updateProductName: "true"

  swidtag:
    # Indicate license used for this installation.
    # For production use, set file to: ibm.com_IBM_Financial_Crimes_Insight_Cartridge_for_IBM_Cloud_Pak_for_Data-6.6.0.swidtag
    # For non-production use, set file to: ibm.com_IBM_Cloud_Pak_for_Data_Financial_Crimes_Insight_for_Non-Production_Environment-6.6.0.swidtag
    # For FCI for Watson Private use, set file to: ibm.com_IBM_Financial_Crimes_Insight_with_Watson_Private-6.6.0.swidtag
    file: 'ibm.com_IBM_Cloud_Pak_for_Data_Financial_Crimes_Insight_for_Non-Production_Environment-6.6.0.swidtag'

  cryptoUtils:
    image:
      repository: ibmcom/fci-crypto-utils
      tag: <BUILD_TAG>
      pullPolicy: "IfNotPresent"

    resources:
      requests:
        memory: "16Mi"
        cpu: "0.01"
      limits:
        memory: "512Mi"
        cpu: "0.2"

    # Make a note of the fully qualified domain name of your OpenShift nodes (master and workers)
    # for your installation.
    #
    # You MUST do one of the following.  Failure to do so will result in an installation failure.
    #
    # 1) Enter the values for CERT_DN, CERT_SAN_DNS_SPEC, and CERT_SAN_IP_SPEC.
    #    This generates new, replacement SSL keystore artifacts (including a new
    #    self-signed certificate).  This technique is often used for 'proof of concept' installations.
    #
    # 2) Obtain SSL/TLS certificates from a trusted certificate authority (CA)
    #    and configure the FCI according the documentation section called Managing SSL/TLS universal keystores.
    #    This technique is recommended for production installations.

    config:
      # REGENERATE_CERTS - this property determines if new, replacement universal SSL
      #                   keystore artifacts are generated.
      #
      #                   Valid values are: true -or- false.
      #
      #                   If true is specified, then confirm the certificate information
      #                   specified in the associated properties are correct for your
      #                   installation.
      #
      #                   If false is specified, then certificates will only be generated if
      #                   they do not already exist.
      REGENERATE_CERTS: false

      # SECRET_NAME is the name of the OpenShift secret object that contains the FCI
      # keystore artifacts.  This typically does not need to be changed.
      SECRET_NAME: 'platform-secret-files'

      # USER_SECRET_NAME is the name of the pre-defined secret that contains a user's
      # private key and certificate chain in PEM format.
      # The private key must be 'key.pem'.
      # The certificate chain must be 'crt.pem'.
      # If this secret exists and the FCI keystore secret has not been created already,
      # then the FCI keystore artifacts will be created from the key/certificate in this
      # pre-existing secret.
      # Enclose the values with ''
      USER_SECRET_NAME: 'fci-user-secret-files'
      # CERT_DN - use this property to set the Distinguished Name for the SSL
      #           certificate. This does not have to match up or have similar values
      #           for the CN or DN values of your LDAP (if your install specifies an LDAP server).
      #           Typically the default value below is fine for most installations.
      #
      #           Example: 'CN=FCI_server,OU=FCI_platform,O=FCI_Development,C=US'
      #
      CERT_DN: 'CN=FCI_server,OU=FCI_platform,O=FCI_Development,C=US'
      #
      # CERT_SAN_DNS_SPEC - use this property to set the DNS names to be
      #                     included in the certificate SubjectAlternativeName
      #                     extension.
      #
      #                     Run the following command to get the httpsIngressSubdomain
      #  oc get route console -n openshift-console | grep -v 'HOST/PORT' | awk '{print $2}' | cut -d'.' -f 2-
      #
      #                     The format is a comma-separated list of DNS names.
      #                     Wildcard domain specifications are acceptable.
      #                     There MUST BE no spaces before or after the comma in this list!!
      #                     This must include openshfit cluster information and hdp servers information
      #
      #                     You MUST have at least two entries for the OpenShift cluster
      #                     1) the full DNS name of your httpsIngressSubdomain (for example fciroute.yourco.com)
      #                     2) *.full DNS name of your httpsIngressSubdomain (for example *.fciroute.yourco.com)
      #                     If your https and tcp ingress are different you would have 4 entries for OpenShift
      #
      #                     Then you must include either the dns names for each HDP server or *.subdomain that includes
      #                     all HDP servers.  Specifying each HDP server indivdually is more secure as it limits only
      #                     those individual HDP servers to communicate to the openshift cluster
      #
      #                     For OpenShift include a wildcard for the subdomain and the domain itself (see example2 below)
      #
      #                     The localhost,127.0.0.1 at the end of this CERT_SANS_DNS_SPEC entry is optional
      #
      #                     Example1: CERT_SANS_DNS_SPEC = '*.localdomain,localdomain,localhost,127.0.0.1'
      #
      #                     Example2: The servers or virtual machines (nodes) for your install have
      #                     httpsIngressSubdomain that matches your tcpIngressSubdomain for an OpenShift 4.3.x install
      #                     the httpsIngressSubdomain is: fciroute.yourco.com
      #                     The Hadoop nodes have dns names like ambari.yourhdp.com, hdp1.yourhdp.com, hdp2.yourhdp.com
      #                     this line would need to look like either (note if all of your hdp machines are under
      #                     .yourhdp.com, you can simply use *.yourhdp.com, NOTE: your HDP machines must be directly under
      #                     .yourhdp.com for *.yourhdp.com to work, the HDP host cannot be hdp1.lab.yourhdp.com for example):
      #
      #                     CERT_SANS_DNS_SPEC = '*.fciroute.yourco.com,fciroute.yourco.com,*.yourhdp.com,localhost,127.0.0.1'
      #                     or
      # CERT_SANS_DNS_SPEC = '*.fciroute.yourco.com,fciroute.yourco.com,ambari.yourhdp.com,hdp1.yourhdp.com,hdp1.yourhdp.com,hdp2.yourhdp.com,localhost,127.0.0.1'
      #
      #                     Example3, same as Example 2 but the tcp ingress is tcpfciroute.yourco.com
      #                     everything else is the same as Example2 (this could be an OpenShift 3.11.x install)
      #
      # CERT_SANS_DNS_SPEC = '*.fciroute.yourco.com,fciroute.yourco.com,*.tcpfciroute.yourco.com,tcpfciroute.yourco.com,*.yourhdp.com'
      #
      CERT_SAN_DNS_SPEC: '*.localdomain,localhost,127.0.0.1'
      #
      # CERT_SAN_IP_SPEC - use this property to set IP addresses to be
      #                    included in the certificate SubjectAlternativeName
      #                    extension.
      #
      #                    Typically this line is a comma-separated list of IP addresses.
      #
      #                    Example: CERT_SAN_IP_SPEC = '127.0.0.1'
      #                    Typically this line is left unchanged.
      CERT_SAN_IP_SPEC: '127.0.0.1'
      # Set to false to generate unique certificates (recommended for production use)
      USE_DEFAULT_CERTS: false

##########################################################################
###
### With a few exceptions around specific components, not many fields
### need to be modified below here.  An example of where a number
### of changes are required would be to enable an LDAP server in
### security-auth section or enabling/disabling individual components.
###
##########################################################################

  commonScripts:
    image:
      repository: ibmcom/fci-common-scripts
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent
    resources:
      limits:
        cpu: 200m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 64Mi

  nginxSslProxy:
    image:
      repository: ibmcom/fci-nginx-ssl-proxy
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent
    resources:
      limits:
        cpu: 100m
        memory: 64Mi
      requests:
        cpu: 100m
        memory: 64Mi

  # SMTP Authentication to use for email alerting. Refer alertmanagerConfig and elastalertConfig further down this file
  smtp_auth_username: "<string>"
  smtp_auth_password: "<secret>"

  # The type of identity server being used.
  # Options include 'msad', 'sds', or 'open', or (Keep the ' around the value)
  # Note: FCI 6.6.0 no longer allows 'none' for basic registry
  # CLOUD_PAK -- Cloud Pak for Data authentication
  # msad -- Microsoft Active Directory
  # sds -- IBM Security Directory Server, formerly known as Tivoli Directory Server
  # open -- OpenLDAP
  # appid -- to enable authentication through IBM Cloud App ID service
  IDENTITY_SERVER_TYPE: 'CLOUD_PAK'

  auth:
    # Role definitions
    # Do not modify  the Role definitions below
    ROLE_ANALYST: 'analyst'
    ROLE_INVESTIGATOR: 'investigator'
    ROLE_SUPERVISOR: 'supervisor'
    ROLE_ADMIN: 'admin'
    ROLE_DATA_SCIENTIST: 'data_scientist'
    ROLE_EXECUTIVE: 'executive'
    ROLE_QUALITY_ASSURANCE: 'quality_assurance'
    ROLE_READ_ONLY: 'read_only'
    ROLE_CONFIDENTIAL: 'confidential'

    # UI configuration definitions
    # Do not modify the UI configurations below
    ROLE_SURVEILLANCE: 'si'
    ROLE_COMPLAINTS: 'si-complaints'
    ROLE_SI_VOICE: 'si-voice'
    ROLE_CASE_MANAGER: 'case_manager'
    ROLE_INSURANCE: 'insurance'
    ROLE_KYC: 'kyc'
    ROLE_GRAPH: 'graph'
    ROLE_ALERTS_INSIGHT: 'alerts_insight'
    ROLE_TOOLING: 'tooling'
    ROLE_TLS: 'tls'

    # Team definitions
    # NOTE: The team names must begin with TEAM_
    # These values can be changed but you must follow documentation
    # under the section called Changing Team Names (do a search for it in the
    # documentation). It is under topic Managing groups, roles, UI configurations, and teams
    TEAM_TRIAGE_TEAM: 'TriageTeam'
    TEAM_INVESTIGATION_TEAM: 'InvestigationTeam'
    TEAM_SUPERVISOR_TEAM: 'SupervisorTeam'
    TEAM_CONFIDENTIAL_TEAM: 'ConfidentialTeam'
    TEAM_VENDOR_TEAM: 'VendorTeam'
    TEAM_LEGAL_TEAM: 'LegalTeam'

  # Set the Additional Group ID settings that the containers should run as
  # This depends on the GID of the shared storage like NFS
  fsGroupConfig:
    supplementalGroups:
    - 0

  kerberos:
    # Frequency of kerberos ticket refresh
    KINIT_SECONDS: '3600'

    runAsUser: 1000
    resources:
      requests:
        memory: "64Mi"
        cpu: ".1"
      limits:
        memory: "512Mi"
        cpu: ".5"

  commonUI:
    # For ROKS and Cloud Pak for Data releases, add full hostname (subdomain of the ingress subdomain) of fci-common-ui-nginx route for http traffic
    SSO_HOSTNAME: "<roks_subdomain>"
    # Prior releases to 6.5, keep this to 3000 for the security-auth service port.
    SSO_PORT: "443"

  # CP4D Application Volume name to be created for analytics runtime
  FCII_APP_VOLUME_NAME: fciiappvolume

db2:
  config:
    DB2OLTP_VERSION: '11.5.5.0-cn1-x86_64'
    ARCHITECTURE: 'x86_64'
    DB2_CPU: '2'
    DATABASE_NAME: 'FCIDB'
    METADATA_DEDICATED: 'false'
    METADATA_DISPLAY_NAME: 'IBM FCI DB2'
    DB2_MEM: '5'
    META_STORAGE_SIZE: '10'
    DATA_STORAGE_SIZE: '10'
    BACKUP_STORAGE_SIZE: '10'
    META_STORAGE_ACCESS_MODE: 'ReadWriteMany'
    DATA_STORAGE_ACCESS_MODE: 'ReadWriteMany'
    BACKUP_STORAGE_ACCESS_MODE: 'ReadWriteMany'
    DEBUG_PROVISIONING: "false"
    CONNECTION_NAME: "FCI_DB2"
    PAGE_SIZE: "32768"
    DEDICATED: 'false'
    PAGE_SIZE: "32768"
    LOGFILSIZ: "25000"
    LOGPRIMARY: "10"
    LOGSECOND:  "100"
    APPLHEAPSZ: "2048"
    APP_CTL_HEAP_SZ: "1024"
    PCKCACHESZ: "AUTOMATIC"
    LOCKTIMEOUT: "30"

global-name-mgmt:
  enabled: false

  image:
    repository: ibmcom/fci-global-name-mgmt
    tag: <BUILD_TAG>
    pullPolicy: "IfNotPresent"

  gnmPvcSpec:
    accessModes:
    - ReadWriteMany
    size: 1Gi

kafka:
  # Set to False to disable Kafka in a non-production environment
  enabled: True

  # A cluster of three Kafka and Zookeeper instances is created by default.
  # In a non-production environment, a cluster of one Kafka and Zookeeper instances can be used by setting replicas to 1.
  replicas: 3
  zookeeperReplicaCount: 3

  pvcSpec:
    size: 10Gi

  image:
    repository: ibmcom/fci-kafka
    tag: "<BUILD_TAG>"
    pullPolicy: "IfNotPresent"

  ## Topic creation and configuration.
  ## The job will be run on a deployment only when the config has been changed.
  ## - If 'partitions' is specified we create the topic (with --if-not-exists.)
  ## - If 'partitions' is specified we 'alter' the number of partitions. This will
  ## silently and safely fail if the new setting isn't strictly larger than the old (i.e. a NOOP.) Do be aware of the
  ## implications for keyed topics (ref: https://docs.confluent.io/current/kafka/post-deployment.html#admin-operations)
  ## - If 'defaultConfig' is specified it's deleted from the topic configuration. If it isn't present,
  ## it will silently and safely fail.
  ## - If 'config' is specified it's added to the topic configuration.
  ##
  topics:
    - name: Party-General
      partitions: 1
    - name: Party-Match-Upload
      partitions: 1
    - name: Party-Resolved-Update
      partitions: 1
    - name: Party-Delete
      partitions: 1
    - name: FCI_IGA_AI_Data_Account
      partitions: 1
    - name: FCI_IGA_AI_Data_Transaction
      partitions: 1
    - name: FCDD_ML_CLASSIFIED
      partitions: 10
    - name: naas-news-task-topic
      partitions: 1
    - name: naas-article-metadata-topic
      partitions: 4
    - name: naas-article-content-topic
      partitions: 10
    - name: naas-article-results-topic
      partitions: 1
    - name: naas-news-mcd-task-topic
      partitions: 1
    - name: FCI_SEC_AuditRecords
      partitions: 1
    - name: sifs.email.in
      partitions: 10
      config: "retention.ms=86400000"
    - name: sifs.chat.in
      partitions: 10
    - name: sifs.alert.in
      partitions: 1
    - name: sifs.ecomm.in
      partitions: 10
    - name: sifs.voice.in
      partitions: 1
    - name: sifs.attach.in
      partitions: 10
      config: "retention.ms=86400000"

  ## Configuration Overrides. Specify any Kafka settings you would like set on the StatefulSet
  ## here in map format, as defined in the official docs.
  ## ref: https://kafka.apache.org/documentation/#brokerconfigs
  ## The end user does not need to edit any of these configurations
  configurationOverrides:
    "listener.security.protocol.map": "SSL:SSL,EXTERNAL:SSL"
    "inter.broker.listener.name": "SSL"
    "ssl.endpoint.identification.algorithm": ""

    # client authentication is requested, but a client without certs can still connect.
    # Change to "required" if client authentication is required.
    "ssl.client.auth": "requested"
    "confluent.support.metrics.enable": false
    "advertised.listeners": |-
     EXTERNAL://${MASTER_HOST}:${NODE_PORT}

  # First Listerner Port to hit the kafka externally.
  # Set to an empty string to use a random port
  nodeport:
    firstListenerPort: "31090"
  externalService:
    annotations: {}
  zookeeper:
    image:
      repository: ibmcom/fci-zookeeper
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent
    # The external port for Zookeeper.
    # Set to an empty string to use a random port
    externalNodePort: "32181"
    externalService:
      annotations: {}

    dataPvcSpec:
      size: 10Gi

    logPvcSpec:
      size: 10Gi

ibm-sch:
  image:
    repository: opencontent-common-utils
    tag: 1.1.4
    pullPolicy: IfNotPresent

mongodb:
  enabled: True
  securityContext:
    enabled: false
  # If the customer has permanently disabled IPv6
  # (including for all kernel updates)
  # change this value to false so that mongodb will start.
  mongodbEnableIPv6: true

  pvcSpec:
    accessModes:
      - ReadWriteOnce
    size: 10Gi

  image:
    repository: ibmcom/fci-mongodb
    tag: <BUILD_TAG>
    pullPolicy: IfNotPresent

elasticsearch:
  client:
    numReplicas: 0
    image:
      repository: ibmcom/fci-logsearch
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent
    resources:
      limits:
        cpu: "1"
        memory: 2Gi
      requests:
        cpu: ".1"
        memory: 2Gi

  master:
    numReplicas: 3
    image:
      repository: ibmcom/fci-logsearch
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent
    pvcSpec:
      size: 10Gi
    resources:
      limits:
        cpu: "1"
        memory: 2Gi
      requests:
        cpu: ".1"
        memory: 2Gi

  data:
    numReplicas: 3
    image:
      repository: ibmcom/fci-logsearch
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent
    pvcSpec:
      size: 10Gi
    resources:
      limits:
        cpu: "1"
        memory: 8Gi
      requests:
        cpu: ".1"
        memory: 8Gi

  sideCarContainers:
     resources:
      limits:
        cpu: "1"
        memory: 4Gi
      requests:
        cpu: ".1"
        memory: 4Gi

ibm-odm-prod:

  serviceAccountName: ""

  service:
    enableTLS: true
    type: ClusterIP
    enableRoute: true
    hostname: ""
    ingress:
      enabled: false
      annotations: # {annotation1: "1",annotation2: "2"}

  decisionServerRuntime:
    enabled: true
    replicaCount: 0
    loggingRef:
    jvmOptionsRef:
    xuConfigRef:
    extendRoleMapping: false
    contextRoot:
    resources:
      requests:
        cpu: 500m
        memory: 512Mi
      limits:
        cpu: 2
        memory: 4096Mi
    image:
      repository: ibmcom/fci-odm-decisionserverruntime
      tag: "<BUILD_TAG>"
      pullPolicy: "IfNotPresent"

  decisionServerConsole:
    loggingRef:
    jvmOptionsRef:
    extendRoleMapping: false
    title:
    description:
    contextRoot:
    resources:
      requests:
        cpu: 500m
        memory: 512Mi
      limits:
        cpu: 2
        memory: 1024Mi
    image:
      repository: ibmcom/fci-odm-decisionserverconsole
      tag: "<BUILD_TAG>"
      pullPolicy: "IfNotPresent"

  decisionCenter:
    enabled: false
    persistenceLocale: en_US
    replicaCount: 0
    customlibPvc:
    loggingRef:
    jvmOptionsRef:
    extendRoleMapping: false
    contextRoot:
    resources:
      requests:
        cpu: 500m
        memory: 1500Mi
      limits:
        cpu: 2
        memory: 4096Mi
    image:
      repository: ibmcom/fci-odm-decisioncenter
      tag: "<BUILD_TAG>"
      pullPolicy: "IfNotPresent"

  decisionRunner:
    enabled: true
    replicaCount: 0
    loggingRef:
    jvmOptionsRef:
    extendRoleMapping: false
    contextRoot:
    resources:
      requests:
        cpu: 500m
        memory: 512Mi
      limits:
        cpu: 2
        memory: 4096Mi
    image:
      repository: ibmcom/fci-odm-decisionrunner
      tag: "<BUILD_TAG>"
      pullPolicy: "IfNotPresent"

  readinessProbe:
    initialDelaySeconds: 5
    periodSeconds: 5
    failureThreshold: 45
    timeoutSeconds: 5

  livenessProbe:
    initialDelaySeconds: 300
    periodSeconds: 10
    failureThreshold: 10
    timeoutSeconds: 5

  customization:
    authSecretRef: "fci-odm-user-registry"
    dedicatedNodeLabel:
    kubeVersion: "ODM on K8s"
    deployForProduction: true

  sideCarContainers:
    resources:
      requests:
        memory: "64Mi"
        cpu: "0.03"
      limits:
        memory: "512Mi"
        cpu: "0.1"
  dbClient:
    image:
      repository: ibmcom/fci-rms-odm-data-store
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent

    resources:
      requests:
        memory: "256Mi"
        cpu: "0.02"
      limits:
        memory: "512Mi"
        cpu: "0.5"
  oidc:
    enabled: false
    serverUrl:
    clientRef:
    provider:
    allowedDomains:

  networkPolicy:
    enabled: false
    apiVersion: networking.k8s.io/v1

wca:
  enabled: false

  pvcSpec:
    accessModes:
      - ReadWriteMany
    size: 30Gi

  image:
    repository: ibmcom/fci-wca
    tag: <BUILD_TAG>
    pullPolicy: "IfNotPresent"

cognos:
  enabled: false

  storageType:
    # The folowing values are for the CPD-provisioned Cognos Analytics with db2-as-a-service instance.
    # DATA_STORAGE_CLASS is used for COGNOSDB as well as other Cognos PVCs. Make sure it supports the access mode.
    # DATA_STORAGE_SIZE is for COGNOSDB.
    META_STORAGE_CLASS: null
    DATA_STORAGE_CLASS: null

  config:
    # MAX_HTTP_HEADER_SIZE: max size in bytes of HTTP headers that Cognos will allow in HTTP requests
    MAX_HTTP_HEADER_SIZE: "16000"
    DATA_STORAGE_ACCESS_MODE: ReadWriteMany
    DATA_STORAGE_SIZE: "10"
    META_STORAGE_ACCESS_MODE: ReadWriteMany
    META_STORAGE_SIZE: "10"
    DATABASE_NAME: COGNOSDB
    APPLHEAPSZ: "2048"
    APP_CTL_HEAP_SZ: "1024"
    CONNECTION_NAME: FCI_COGNOS
    DB2_CPU: "2"
    DB2_MEM: "6"
    DEBUG_PROVISIONING: "false"
    DEDICATED: "false"
    LOCKTIMEOUT: "30"
    LOCKTIMEOUT: "30"
    LOGFILSIZ: "25000"
    LOGPRIMARY: "10"
    LOGSECOND: "100"
    METADATA_DISPLAY_NAME: IBM FCI COGNOS DB2
    NAMESPACE: 'fci'
    PAGE_SIZE: "32768"
    PCKCACHESZ: AUTOMATIC
    DB2OLTP_VERSION: '11.5.5.0-cn1-x86_64'

logging:
  # set to False to disable logging infrastructure in a non-production environment
  enabled: False

  logstash:
    image:
      repository: ibmcom/fci-loganalysis
      tag: <BUILD_TAG>

  kibana:
    image:
      repository: ibmcom/fci-logvisualization
      tag: <BUILD_TAG>

security-audit:
  # set to False to disable auditing in a non-production environment. Also set AUDIT_ACTIVE: '0' under securityAuthConfig:
  enabled: True

  image:
    repository: ibmcom/fci-security-audit-app
    tag: <BUILD_TAG>
    pullPolicy: IfNotPresent

  config:
    FLYWAY_BASELINE_VERSION: '2019.02.28.13.59.00'
    AUDIT_STORE: "db2"
    AUDIT_FILE_LOCATION: "/var/log/fciAudit-%g.log"
    AUDIT_FILE_SIZE_IN_BYTES: "10000000"
    AUDIT_NUM_FILES: "2"
    CONTAINERNAME: "audit"
    ENABLE_AUDIT: "true"

  sidecar:
    image:
      repository: ibmcom/fci-audit-sidecar
      tag: '<BUILD_TAG>'
      pullPolicy: IfNotPresent
    resources:
      requests:
        memory: "256Mi"
        cpu: ".2"
      limits:
        memory: "512Mi"
        cpu: ".5"

  db2:
    image:
      repository: ibmcom/fci-security-audit-data-store
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent

security-auth:
  replicas: 2
  nodejs:
    resources: {}
    image:
      repository: ibmcom/fci-security-auth
      tag: '<BUILD_TAG>'
      pullPolicy: IfNotPresent
  redis:
    resources: {}
    image:
      repository: ibmcom/fci-memoryds
      tag: '<BUILD_TAG>'
      pullPolicy: IfNotPresent

  translationJob:
    image:
      repository: ibmcom/fci-translations
      tag: <BUILD_TAG>
      pullPolicy: "Always"
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 50m
        memory: 64Mi

  securityAuthConfig:
    # set to '0' to disable auditing in a non-production environment. Also set enabled: False under security-audit:
    AUDIT_ACTIVE: '1'

    # If integrating with Cloud Pak for Data authentication, this property specifies
    # whether to create internal users in the Cloud Pak for Data user registry.
    # Valid values are "True" and "False".
    CREATE_CP_INTERNAL_USERS: "True"

    # specify internal user registry (also known as basic user registry) from WebSphere Liberty
    # for each numbered user, specify the user's id, roles, tenants, and password
    # roles are comma separated
    # tenants are comma separated
    # valid roles include: admin, analyst, investigator, supervisor, data_scientist
    # password are specified in the secrets.yaml file of the install toolkit
    # to add additional users, add additional user properties numbered 17, 18, etc.
    USER_ID_1: fciadmin
    USER_ROLES_1: admin,supervisor,investigator,analyst,data_scientist
    USER_CPROLE_1: zen_fci_admin_role
    USER_TEAMS_1: SupervisorTeam,InvestigationTeam,TriageTeam
    USER_TENANTS_1: ibm.com
    USER_ID_2: fcianalyst
    USER_ROLES_2: analyst
    USER_CPROLE_2: zen_fci_analyst_role
    USER_TEAMS_2: TriageTeam
    USER_TENANTS_2: ibm.com
    USER_ID_3: fciinvestigator
    USER_ROLES_3: investigator
    USER_CPROLE_3: zen_fci_investigator_role
    USER_TEAMS_3: InvestigationTeam
    USER_TENANTS_3: ibm.com
    USER_ID_4: fcisupervisor
    USER_ROLES_4: supervisor
    USER_CPROLE_4: zen_fci_supervisor_role
    USER_TEAMS_4: SupervisorTeam,InvestigationTeam,TriageTeam
    USER_TENANTS_4: ibm.com
    USER_ID_5: fcivendor
    USER_ROLES_5: analyst,insurance
    USER_CPROLE_5: zen_fcii_vendor_role
    USER_TEAMS_5: VendorTeam
    USER_TENANTS_5: ibm.com
    USER_ID_6: fciconfidential
    USER_ROLES_6: confidential
    USER_TEAMS_6: confidential
    USER_TENANTS_6: ibm.com
    USER_ID_7: fciiadmin
    USER_ROLES_7: admin,insurance
    USER_CPROLE_7: zen_fcii_admin_role
    USER_TEAMS_7: SupervisorTeam,InsuranceTeam
    USER_TENANTS_7: ibm.com
    USER_ID_8: fciianalyst
    USER_ROLES_8: analyst,insurance
    USER_CPROLE_8: zen_fcii_analyst_role
    USER_TEAMS_8: TriageTeam,InsuranceTeam
    USER_TENANTS_8: ibm.com
    USER_ID_9: fciiinvestigator
    USER_ROLES_9: investigator,insurance
    USER_CPROLE_9: zen_fcii_investigator_role
    USER_TEAMS_9: InvestigationTeam,InsuranceTeam
    USER_TENANTS_9: ibm.com
    USER_ID_10: fciisupervisor
    USER_ROLES_10: supervisor,insurance
    USER_CPROLE_10: zen_fcii_supervisor_role
    USER_TEAMS_10: SupervisorTeam,InsuranceTeam
    USER_TENANTS_10: ibm.com
    USER_ID_11: tlsadmin
    USER_ROLES_11: admin,tls
    USER_TEAMS_11: SupervisorTeam
    USER_TENANTS_11: ibm.com
    USER_ID_12: fcaisupervisor
    USER_ROLES_12: alerts_insight,supervisor
    USER_TEAMS_12: TriageTeam
    USER_TENANTS_12: ibm.com
    USER_ID_13: fcaianalyst
    USER_ROLES_13: alerts_insight,analyst
    USER_TEAMS_13: TriageTeam
    USER_TENANTS_13: ibm.com
    USER_ID_14: fcaiinvestigator
    USER_ROLES_14: alerts_insight,investigator
    USER_TEAMS_14: TriageTeam
    USER_TENANTS_14: ibm.com
    USER_ID_15: fcaiadmin
    USER_ROLES_15: alerts_insight,admin
    USER_TEAMS_15: TriageTeam
    USER_TENANTS_15: ibm.com
    USER_ID_16: fcaiexecutive
    USER_ROLES_16: alerts_insight,executive
    USER_TEAMS_16: TriageTeam
    USER_TENANTS_16: ibm.com
    USER_ID_17: toolingadmin
    USER_ROLES_17: admin,data_scientist,tooling
    USER_TEAMS_17: SupervisorTeam
    USER_TENANTS_17: ibm.com
    USER_ID_18: graphadmin
    USER_ROLES_18: admin,supervisor,graph
    USER_CPROLE_18: zen_graph_admin_role
    USER_TEAMS_18: SupervisorTeam
    USER_TENANTS_18: ibm.com
    USER_ID_19: sisupervisor
    USER_ROLES_19: si,si-complaints,supervisor
    USER_TEAMS_19: SupervisorTeam,InvestigationTeam,TriageTeam
    USER_TENANTS_19: ibm.com
    USER_ID_20: sianalyst
    USER_ROLES_20: si,analyst
    USER_TEAMS_20: TriageTeam
    USER_TENANTS_20: ibm.com
    USER_ID_21: siinvestigator
    USER_ROLES_21: si,investigator
    USER_TEAMS_21: InvestigationTeam
    USER_TENANTS_21: ibm.com
    USER_ID_22: siadmin
    USER_ROLES_22: si,si-voice,admin
    USER_TEAMS_22: SupervisorTeam,InvestigationTeam,TriageTeam
    USER_TENANTS_22: ibm.com
    USER_ID_23: kycadmin
    USER_ROLES_23: admin,supervisor,investigator,analyst,data_scientist,kyc
    USER_TEAMS_23: SupervisorTeam,InvestigationTeam,TriageTeam
    USER_MAIL_23: kycadmin@ibm.com
    USER_DISPLAY_NAME_23: kycadmin
    USER_TENANTS_23: ibm.com
    USER_ID_24: fcilegal
    USER_ROLES_24: supervisor,insurance
    USER_CPROLE_24: zen_fcii_legal_role
    USER_TEAMS_24: LegalTeam
    USER_TENANTS_24: ibm.com

    # JWT_KEY_EXPIRY is the length of time before the security token expires
    # This controls the length of a user's session after logging in.
    # The value is expressed in a time span. e.g.:  "30m", "4h", "1d"
    JWT_KEY_EXPIRY: '4h'
    # DO NOT CHANGE THIS VALUE:
    JWT_ISSUER: 'fci.ibm.com'

    ### IBM Cloud AppID configuration ###
    ## You can find these configuration values by clicking View credentials
    ## in the 'Service credentials' tab of your AppID service's manage section.
    # The client ID for the App ID instance of service.
    APPID_CLIENT_ID: <clientId>
    # Tenant ID is specific to our App ID instance
    APPID_TENANTID: <tenantId>
    # The App ID oauth server URL which is used for redirecting user to IBM Cloud app ID based authentication.
    APPID_OAUTH_SERVER_URL: "https://us-south.appid.cloud.ibm.com/oauth/v4/<tenantId>"
    # The App ID profiles URL.
    APPID_PROFILES_URL: "https://us-south.appid.cloud.ibm.com"
    # The App ID API version.
    APPID_VERSION: "4"
    # End Point for the app ID service.
    APPID_SERVICE_ENDPOINT: "https://us-south.appid.cloud.ibm.com"
    # Default Call back URL needed for redirecting to security-auth service after AppID authentication.
    APPID_CALLBACK_URL: "/security-auth/ibm/cloud/appid/callback"
    # IBM Cloud IAM Token URL
    APPID_IAM_TOKEN_URL: "https://iam.cloud.ibm.com/identity/token"

    # BRUTE_THROTTLING is set to 'true' to enable throttling of requests from an IP address
    # when too many invalid logins have occurred from that IP address.
    # It is used to prevent brute force attacks.
    # Set to 'false' to disable this feature.
    BRUTE_THROTTLING: 'true'
    # BRUTE_FREE_RETRIES specifies the number of invalid logins before throttling begins
    BRUTE_FREE_RETRIES: '5'

    # MULTI_TENANCY_ENABLED enable multi-tenancy fields in JWT
    MULTI_TENANCY_ENABLED: 'true'
    # Default tenants to use if none provided by LDAP and MULTI_TENANCY_ENABLED is set to true
    DEFAULT_TENANT_IF_MISSING: 'ibm.com'
    CLOUD_PAK_TOKEN: '/var/run/sharedsecrets/token'

case-manager:
  # set to False to disable Case Manager
  # set to True otherwise
  enabled: True

  mq:
    # Load Balancer Node Ports - Port to expose in loadbalancer for Cloud Pak for Data installs
    # Set to an empty string to use a random port
    mqNodePort: "30683"
    mqSeriesNodePort: "30997"
    image:
      repository: ibmcom/fci-messaging
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent
    resources:
      requests:
        memory: "256Mi"
        cpu: ".5"
      limits:
        memory: "512Mi"
        cpu: "2"
    nodeSelector: {}
    tolerations: []
    affinity: {}
    externalService:
      annotations: {}
  liberty:
    image:
      repository: ibmcom/fci-solution
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent
    resources:
      requests:
        memory: "256Mi"
        cpu: "1"
      limits:
        memory: "9Gi"
        cpu: "4"
    nodeSelector: {}
    tolerations: []
    affinity: {}
    # runAsUser is the user id that containers requiring updates to file system permissions
    # run as if pvRequiresPermissionsFix is set to true
    runAsUser: 1000
  kerberosClient:
    image:
      repository: ibmcom/fci-hdp-client
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent

  libertyConfig:
    env_fci_batch_userid: 'fcibatch'
    COM_FCI_HBASE_SITE_FILE: "/home/wlpadmin/hbase-site.xml"
    COM_FCI_CORE_SITE_FILE: "/home/wlpadmin/core-site.xml"
    COM_FCI_KRB5_CONF_FILE: "/home/wlpadmin/krb5.conf"

  solutionPvcSpec:
    accessModes:
    - ReadWriteMany
    size: 1Gi

  # If upgrading from the 6.5 release, then change below to ReadWriteMany
  mqPvcSpec:
    accessModes:
    - ReadWriteOnce
    size: 1Gi

cedm:
  # set to False to disable CEDM
  # set to True otherwise
  enabled: True

  # set to True to mount the Analytics Runtime application volume
  requireARMount: False

  # Set this to False when running an upgrade
  runPopulateJob: True

  liberty:
    image:
      repository: ibmcom/fci-cedm-integration
      tag: <BUILD_TAG>
    resources:
      requests:
        memory: "256Mi"
        cpu: "1"
      limits:
        memory: "12Gi"
        cpu: "4"
    # runAsUser is the user id that containers requiring updates to file system permissions
    # run as if pvRequiresPermissionsFix is set to true
    runAsUser: 1000
  kerberosClient:
    image:
      repository: ibmcom/fci-hdp-client
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent

  db2:
    image:
      repository: ibmcom/fci-cedm-data-store-client
      tag: <BUILD_TAG>

  libertyPvcSpec:
    accessModes:
      - ReadWriteMany
    size: 30Gi

  cedmConfig:
    FLYWAY_BASELINE_VERSION: '01.01.01.00.119'

    #Kafka max message size in producer (Broker max message size - 1KB)
    KAFKA_MAX_REQUEST_SIZE: '19998988'

    # Publish batch size and frequency
    PARTY_PUBLISH_BATCH_SIZE: '250'
    PARTY_PUBLISH_PERIOD_SECONDS: '300'

    SCORING_NO_PARTITIONS: '8'

    # Time interval for running Entity Resolution (in seconds)
    RESOLUTION_TIME_INTERVAL: '1800'
    # Complete current Entity Resolution workflow jobs before starting
    # another one. Applies to interval processing only.
    RESOLUTION_COMPLETE_WORKFLOW: 'false'
    # Daily scheduled time for running Entity Resolution (HH:mm)
    #RESOLUTION_RUNTIME: '23:59'

    # Time interval for running the Watchlist matching process (in seconds)
    WATCHLIST_PROCESSING_INTERVAL: '300'
    # Daily scheduled time for running the Watchlist matching process (HH:mm)
    # WATCHLIST_PROCESSING_RUNTIME: '23:59'

    ## Party enrichment configuration
    # Entity Enrichment Endpoint(default=Entity Enrichment product exposed endpoint as specified below)
    ENRICHMENT_ENDPOINT: 'https://fci-ees-engine:443/v1/entities/auto-enrich'
    # Comma separated list of plans to be executed
    # A value of '' means party enrichment is not enabled
    ENRICHMENT_PLANS: ''
    # Party type of enrich (o=Organizations, i=Individuals, a=All  default=a)
    ENRICHMENT_PARTY_TYPE: 'a'
    # Party operation type to enrich (i=Insert, a=all (Insert and Update)  default=a)
    ENRICHMENT_OPERATION_TYPE: 'a'
    # Party process type using enrichment. A value of
    # i. 'false' allows only parties created via REST API to be enriched.
    # ii.'true' allows all parties(created via async - bulk load, or sync - REST API) to be enriched
    # default='true'
    ENRICHMENT_ALL_PARTIES: 'true'
    # Enrichment batch size and frequency
    ENRICHMENT_PUBLISH_BATCH_SIZE: '100'
    ENRICHMENT_PUBLISH_PERIOD_SECONDS: '300'

search:
  # set to False to disable Search
  # set to True otherwise
  enabled: True

  liberty:
    resources: {}
    image:
      repository: ibmcom/fci-search-liberty
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent

common-ui:
  nodejs:
    image:
      repository: ibmcom/fci-common-ui
      tag: <BUILD_TAG>
  nginx:
    image:
      repository: ibmcom/fci-common-ui-web
      tag: <BUILD_TAG>
  investigativeUI:
    image:
      repository: ibmcom/fci-investigative-ui
      tag: <BUILD_TAG>

    # set to 0 to disable the Investigative UI
    # otherwise set to 1
    replicas: 1

  iuiStaticConfig:
    image:
      repository: ibmcom/fci-investigative-ui-config
      tag: <BUILD_TAG>


  iuiConfigService:
    image:
      repository: ibmcom/fci-investigative-ui-config-service
      tag: <BUILD_TAG>

  iuiNarrativeService:
  # set to False to disable Narrative Service
  # set to True otherwise
    enabled: False

    image:
      repository: ibmcom/fci-narrative-service
      tag: <BUILD_TAG>

  sidecar:
    image:
      repository: ibmcom/fci-audit-sidecar
      tag: '<BUILD_TAG>'
      pullPolicy: IfNotPresent
    resources:
      requests:
        memory: "256Mi"
        cpu: ".2"
      limits:
        memory: "512Mi"
        cpu: ".5"

  nginxConfig:
    # INCLUDE_CONFIG is a comma-separated list of components to include
    # valid values are: case,cedm,odm,search,fcai,fcdd,sifs,graph,art
    INCLUDE_CONFIG: 'case,cedm,odm,search,graph'
    CPD_ROUTE_NAME: "fci-cpd"
    CONTAINERNAME: ibm-fci-nginx-container

  bkPvcSpec:
    accessModes:
      - ReadWriteMany
    size: 1Gi

  nginxPvcSpec:
    accessModes:
      - ReadWriteMany
    size: 512Mi

  config:
    APP_ROLE_ANALYST: 'home'
    APP_ROLE_INVESTIGATOR: 'home'
    APP_ROLE_SUPERVISOR: 'home'
    APP_ROLE_ADMIN: 'home'
    APP_ROLE_DATA_SCIENTIST: 'home'
    APP_ROLE_EXECUTIVE: 'home'
    APP_ROLE_QUALITY_ASSURANCE: 'home'
    APP_ROLE_READ_ONLY: 'home'
    APP_ROLE_CONFIDENTIAL: 'home'

    # Set to 'true' if SSO is enabled.  The login form will then be replaced with a link to login through the SSO provider.
    SSO_ENABLED: 'false'
    # if using SAML without App ID then change to: security-auth/api/v1.0/login/saml
    SSO_PATH: "security-auth/api/v1.0/login/appid"

cdn-proxy:
  cdnProxy:
    image:
      repository: ibmcom/fci-cdn-proxy
      tag: <BUILD_TAG>

  bkPvcSpec:
    accessModes:
      - ReadWriteMany
    size: 5Gi

dsf:
  replicas: 0
  image:
    pullPolicy: IfNotPresent
    repository: ibmcom/kyc-data-source-framework
    tag: <BUILD_TAG>
  dsfConfig:
    DNB_USERNAME: ''
    POPULATE_DEFINITIONS_ON_STARTUP: 'true'
    DOWJONES_USERNAME: ''
    DOWJONES_NAMESPACE: ''
    KYCKR_USERNAME: ''
    TRANSUNION_TLO_USERNAME: ''
    TRANSUNION_EBUREAU_USERNAME: ''
    TRANSUNION_IDVISION_USERNAME: ''

ees-engine:
  replicas: 0
  image:
    pullPolicy: IfNotPresent
    repository: ibmcom/fci-shell-detection
    tag: <BUILD_TAG>

graph-writer:
  # set to False to disable Graph Liberty server
  # set to True otherwise
  enabled: True
  # runAsUser is the user id that containers requiring updates to file system permissions
  # run as if pvRequiresPermissionsFix is set to true
  runAsUser: 1000

  liberty:
    image:
      repository: ibmcom/fci-graph-writer
      tag: <BUILD_TAG>

  libertyInitPv:
    image:
      repository: ibmcom/fci-graph-writer-init-pv
      tag: <BUILD_TAG>

  kerberosClient:
    image:
      repository: ibmcom/fci-hdp-client
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent

  gremlin:
    image:
      repository: ibmcom/fci-graph-gremlin-server
      tag: <BUILD_TAG>
    graphGremlinConfig:
      gremlinNodePort: "30184"
    numReplicas: 0
    externalService:
      annotations: {}

  libertyPvcSpec:
    accessModes:
      - ReadWriteMany
    size: 1Gi

  gremlinPvcSpec:
    accessModes:
      - ReadWriteMany
    size: 1Gi

analytics-runtime:
  # set to False to disable Analytics Runtime
  # set to True otherwise
  enabled: False

  arLiberty:
    image:
      pullPolicy: IfNotPresent
      repository: ibmcom/fci-analytics-runtime-liberty
      tag: <BUILD_TAG>
    resources:
      requests:
        memory: "256Mi"
        cpu: "1"
      limits:
        memory: "16Gi"
        cpu: "4"

  arCanvas:
    image:
      pullPolicy: IfNotPresent
      repository: ibmcom/fci-analytics-runtime-canvas
      tag: <BUILD_TAG>
    resources:
      requests:
        cpu: "0.5"
        memory: "256Mi"
      limits:
        cpu: "2"
        memory: "2Gi"

  storageType:
    ZEN_VOLUME_STORAGE_CLASS: null

  arLibertyConfig:
    CP4D_VERSION: '3.5.1'
    CP4D_URL: https://internal-nginx-svc:12443
    ART_CONFIG: ''
    FCII_SPARK_INSTANCE_NAME: 'fciisparkinstance'
    FCII_SPARK_VOLUME_NAME: 'fciisparkvolume'
    FCII_SPARK_VOLUME_SIZE: '20Gi'
    FCII_APP_VOLUME_SIZE: '40Gi'
    DATASTORE: 'HBASE'

  kerberosClient:
    image:
      repository: ibmcom/fci-hdp-client
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent

realtime-scoring:
  enabled: false
  liberty:
    image:
      repository: ibmcom/fci-realtime-scoring
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent
    resources:
      requests:
        memory: 256Mi
        cpu: "1"
      limits:
        memory: "8Gi"
        cpu: "4"

cadence:
  enabled: false

  server:
    image:
      repository: ibmcom/fci-cadence-server
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent
