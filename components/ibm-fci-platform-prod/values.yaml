global:

  persistence:
    # This field can always be left blank because when running the cpd
    # installer, the storage class is provided using the -c command line
    # option, which overrides this setting.
    storageClassName: ''

    # By default, installation assumes that there is a dynamic provisioner
    # that allocates persistent volues using the storageClassName provided
    # above.
    #
    # If a dynamic provisioner is not in use, change useDynamicProvisioning
    # to false and if appropriate, set createPv to true.
    useDynamicProvisioning: true

  # createPv should be left false in cases where the PVs are auto-provisioned
  # or precreated by an administrator.  However, it's possible to use an NFS
  # server, set global.nfsServer to the value, and then set createPv to true,
  # and when installing the chart, the PVs will be created automatically using
  # the NFS Server.  Note that the NFS server must support NFS v4.1 or later.
  createPv: false

  # In some cases, when persistent volumes are created, the filesystem is
  # in a state where the pods will not have permission to write files to the
  # filesystem because they do not have permission.  This is often (but not
  # always) the case with auto-provisioners.
  # Setting this to true will create an initContainer that will run with
  # root-level priviledges in order to mount the volume, fix the filesystem
  # permissions, and then terminate.  If the PVs that are allocated already
  # have appropriate permissions, this field may be set to false.
  pvRequiresPermissionsFix: true

  # runAsUser is the user id that containers requiring updates to file system permissions
  # run as if pvRequiresPermissionsFix is set to true
  runAsUser: 1000

  # Provide the subdomain of the host used to access the cluster for
  # HTTP/HTTPS traffic.  This often can be determined by running this command:
  # oc get route console -n openshift-console | grep -v 'HOST/PORT' | awk '{print $2}'
  # This provides the URL of the OpenShift console.  After removing the
  # hostname up to the first dot, the remaining value will be the subdomain
  # required here.  For example, if the output of the command is:
  # console-openshift-console.apps.fyrecluster.os.fyre.ibm.com
  # then enter `apps.fyrecluster.os.fyre.ibm.com` here.
  # Or type this command
  # oc get route console -n openshift-console | grep -v 'HOST/PORT' | awk '{print $2}' | cut -d'.' -f 2-
  # which should give you `apps.fyrecluster.os.fyre.ibm.com`
  # If unsure, verify with the administrator that configured the OpenShift
  # cluster.  There can be a hardware loadbalancer in front of the cluster
  # making it impossible to determine the correct URL automatically.
  httpsIngressSubdomain: localhost.localdomain

  # Provide the subdomain of the host used to access the cluster for TCP/IP
  # traffic.  This is much more difficult to determine automatically.  In
  # some cases, this is the same as the httpsIngressSubdomain.  In other
  # cases, this could be a different value.  For example, in IBM Cloud (ibm.com/cloud),
  # the correct value for this can be determined by running this command:
  # oc get cm -n kube-system ibm-cloud-cluster-ingress-info -o yaml | grep ' ingress-subdomain'
  # which may output a value like:
  # sl-roks-441333-2ff4384e3bbe791290bc084e4f9f6c9a-0000.tor01.containers.appdomain.cloud
  # If unsure, verify with the administrator that configured the OpenShift
  # cluster.  There can be a hardware loadbalancer in front of the cluster
  # making it impossible to determine the correct URL automatically.
  tcpIngressSubdomain: localhost.localdomain

  # At this time, DB2 instances must be set to 1
  db2Instances: 1

  # Change to CLOUD_PAK to use DB2 from Cloud Pak
  DB2_PROVISIONER: 'FCI'
  DB2_CONNECTION_NAME: "FCI_DB2"

  # generateInternalPasswords determines whether secrets for internal
  # services are randomly generated.  Set to false to use a pre-existing
  # secret instead of randomly generated values.  In this case, all
  # values must be precreated for all secrets.  See the knowledge center
  # for a list of all of the secrets and the keys that need to be created.
  generateInternalPasswords: true

  # externalSecretName is the name of the OpenShift secret object that
  # contains passwords to external services, such as LDAP.  This value
  # does not normally need to be changed.
  externalSecretName: fci-platform-external-secrets-env

  # hostAliases provides a way to manage /etc/hosts in all Kubernetes
  # pods that will be installed in the Kubernetes cluster.
  # This is useful in situations where DNS may not work properly,
  # or DNS entries are not used at all.  Most often, this is used
  # to add the Hadoop (HDP) servers into the /etc/hosts of each pod to
  # ensure that pods can resolve the HDP servers to the correct IP
  # addresses.
  #
  # In addition if you are using this because of DNS issues
  # you most likely would need to create /etc/hosts entries on the
  # nodes (hosts) that are running Kubernetes (the Kubernetes Master
  # and Kubernetes Worker nodes) to contain the hostnames and ip
  # addresses of the Hadoop servers (nodes).
  #
  # Below are examples with hostnames of hdp1, hdp2, hdp3
  # and the dns of the servers end in .yourcompany.com
  # If you have 6 HDP servers then follow the pattern below
  # and enter ip and hostnames for 6 servers
  #
  # Also uncomment all the lines below for this to take affect
  #
  # hostAliases:
  # - ip: "10.187.45.4"
  #   hostnames:
  #   - "hdp1.yourcompany.com"
  # - ip: "10.187.45.38"
  #   hostnames:
  #   - "hdp2.yourcompany.com"
  # - ip: "10.187.45.37"
  #   hostnames:
  #   - "hdp3.yourcompany.com"


  # The product name to brand the login page
  productName: "IBM Financial Crimes Insight"

  # Set to "true" to update the branding of the Cloud Pak for Data login page.
  # Set to "false" otherwise.
  updateProductName: "true"

  swidtag:
    # Indicate license used for this installation.
    # For production use, set file to: ibm.com_IBM_Cloud_Pak_for_Data_Financial_Crimes_Insight-6.5.1.swidtag
    # For non-production use, set file to: ibm.com_IBM_Cloud_Pak_for_Data_Financial_Crimes_Insight_for_Non-Production_Environment-6.5.1.swidtag
    # For FCI for Watson Private use, set file to: ibm.com_IBM_Financial_Crimes_Insight_with_Watson_Private-6.5.1.swidtag
    file: 'ibm.com_IBM_Cloud_Pak_for_Data_Financial_Crimes_Insight_for_Non-Production_Environment-6.5.1.swidtag'

  cryptoUtils:
    image:
      repository: ibmcom/fci-crypto-utils
      tag: <BUILD_TAG>
      pullPolicy: "IfNotPresent"

    resources:
      requests:
        memory: "16Mi"
        cpu: "0.01"
      limits:
        memory: "512Mi"
        cpu: "0.2"

    # Make a note of the fully qualified domain name of your OpenShift nodes (master and workers)
    # for your installation.
    #
    # You MUST do one of the following.  Failure to do so will result in an installation failure.
    #
    # 1) Enter the values for CERT_DN, CERT_SAN_DNS_SPEC, and CERT_SAN_IP_SPEC.
    #    This generates new, replacement SSL keystore artifacts (including a new
    #    self-signed certificate).  This technique is often used for 'proof of concept' installations.
    #
    # 2) Obtain SSL/TLS certificates from a trusted certificate authority (CA)
    #    and configure the FCI according the documentation section called Managing SSL/TLS universal keystores.
    #    This technique is recommended for production installations.

    config:
      # REGENERATE_CERTS - this property determines if new, replacement universal SSL
      #                   keystore artifacts are generated.
      #
      #                   Valid values are: true -or- false.
      #
      #                   If true is specified, then confirm the certificate information
      #                   specified in the associated properties are correct for your
      #                   installation.
      #
      #                   If false is specified, then certificates will only be generated if
      #                   they do not already exist.
      REGENERATE_CERTS: false

      # SECRET_NAME is the name of the OpenShift secret object that contains the FCI
      # keystore artifacts.  This typically does not need to be changed.
      SECRET_NAME: 'platform-secret-files'

      # USER_SECRET_NAME is the name of the pre-defined secret that contains a user's
      # private key and certificate chain in PEM format.
      # The private key must be 'key.pem'.
      # The certificate chain must be 'crt.pem'.
      # If this secret exists and the FCI keystore secret has not been created already,
      # then the FCI keystore artifacts will be created from the key/certificate in this
      # pre-existing secret.
      # Enclose the values with ''
      USER_SECRET_NAME: 'fci-user-secret-files'
      # CERT_DN - use this property to set the Distinguished Name for the SSL
      #           certificate. This does not have to match up or have similar values
      #           for the CN or DN values of your LDAP (if your install specifies an LDAP server).
      #           Typically the default value below is fine for most installations.
      #
      #           Example: 'CN=FCI_server,OU=FCI_platform,O=FCI_Development,C=US'
      #
      CERT_DN: 'CN=FCI_server,OU=FCI_platform,O=FCI_Development,C=US'
      #
      # CERT_SAN_DNS_SPEC - use this property to set the DNS names to be
      #                     included in the certificate SubjectAlternativeName
      #                     extension.
      #
      #                     Run the following command to get the httpsIngressSubdomain
      #  oc get route console -n openshift-console | grep -v 'HOST/PORT' | awk '{print $2}' | cut -d'.' -f 2-
      #
      #                     The format is a comma-separated list of DNS names.
      #                     Wildcard domain specifications are acceptable.
      #                     There MUST BE no spaces before or after the comma in this list!!
      #                     This must include openshfit cluster information and hdp servers information
      #
      #                     You MUST have at least two entries for the OpenShift cluster
      #                     1) the full DNS name of your httpsIngressSubdomain (for example fciroute.yourco.com)
      #                     2) *.full DNS name of your httpsIngressSubdomain (for example *.fciroute.yourco.com)
      #                     If your https and tcp ingress are different you would have 4 entries for OpenShift
      #
      #                     Then you must include either the dns names for each HDP server or *.subdomain that includes
      #                     all HDP servers.  Specifying each HDP server indivdually is more secure as it limits only
      #                     those individual HDP servers to communicate to the openshift cluster
      #
      #                     For OpenShift include a wildcard for the subdomain and the domain itself (see example2 below)
      #
      #                     The localhost,127.0.0.1 at the end of this CERT_SANS_DNS_SPEC entry is optional
      #
      #                     Example1: CERT_SANS_DNS_SPEC = '*.localdomain,localdomain,localhost,127.0.0.1'
      #
      #                     Example2: The servers or virtual machines (nodes) for your install have
      #                     httpsIngressSubdomain that matches your tcpIngressSubdomain for an OpenShift 4.3.x install
      #                     the httpsIngressSubdomain is: fciroute.yourco.com
      #                     The Hadoop nodes have dns names like ambari.yourhdp.com, hdp1.yourhdp.com, hdp2.yourhdp.com
      #                     this line would need to look like either (note if all of your hdp machines are under
      #                     .yourhdp.com, you can simply use *.yourhdp.com, NOTE: your HDP machines must be directly under
      #                     .yourhdp.com for *.yourhdp.com to work, the HDP host cannot be hdp1.lab.yourhdp.com for example):
      #
      #                     CERT_SANS_DNS_SPEC = '*.fciroute.yourco.com,fciroute.yourco.com,*.yourhdp.com,localhost,127.0.0.1'
      #                     or
      # CERT_SANS_DNS_SPEC = '*.fciroute.yourco.com,fciroute.yourco.com,ambari.yourhdp.com,hdp1.yourhdp.com,hdp1.yourhdp.com,hdp2.yourhdp.com,localhost,127.0.0.1'
      #
      #                     Example3, same as Example 2 but the tcp ingress is tcpfciroute.yourco.com
      #                     everything else is the same as Example2 (this could be an OpenShift 3.11.x install)
      #
      # CERT_SANS_DNS_SPEC = '*.fciroute.yourco.com,fciroute.yourco.com,*.tcpfciroute.yourco.com,tcpfciroute.yourco.com,*.yourhdp.com'
      #
      CERT_SAN_DNS_SPEC: '*.localdomain,localhost,127.0.0.1'
      #
      # CERT_SAN_IP_SPEC - use this property to set IP addresses to be
      #                    included in the certificate SubjectAlternativeName
      #                    extension.
      #
      #                    Typically this line is a comma-separated list of IP addresses.
      #
      #                    Example: CERT_SAN_IP_SPEC = '127.0.0.1'
      #                    Typically this line is left unchanged.
      CERT_SAN_IP_SPEC: '127.0.0.1'
      # Set to false to generate unique certificates (recommended for production use)
      USE_DEFAULT_CERTS: false

##########################################################################
###
### With a few exceptions around specific components, not many fields
### need to be modified below here.  An example of where a number
### of changes are required would be to enable an LDAP server in
### security-auth section or enabling/disabling individual components.
###
##########################################################################

  commonScripts:
    image:
      repository: ibmcom/fci-common-scripts
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent
    resources:
      limits:
        cpu: 200m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 64Mi

  nginxSslProxy:
    image:
      repository: ibmcom/fci-nginx-ssl-proxy
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent
    resources:
      limits:
        cpu: 100m
        memory: 64Mi
      requests:
        cpu: 100m
        memory: 64Mi

  # SMTP Authentication to use for email alerting. Refer alertmanagerConfig and elastalertConfig further down this file
  smtp_auth_username: "<string>"
  smtp_auth_password: "<secret>"

  # The type of identity server being used.
  # Options include 'msad', 'sds', 'open', or 'none' (Keep the ' around the value)
  # CLOUD_PAK -- Cloud Pak for Data authentication
  # msad -- Microsoft Active Directory
  # sds -- IBM Security Directory Server, formerly known as Tivoli Directory Server
  # open -- OpenLDAP
  # none -- no LDAP server, use basic registry instead
  # appid -- to enable authentication through IBM Cloud App ID service
  IDENTITY_SERVER_TYPE: 'none'

  LDAP_SERVER_HOST: '<hostname>'
  LDAP_SERVER_PORT: '636'

  # set to True if connecting to the LDAP server over SSL
  # set to False otherwise
  LDAP_SERVER_SSL: True

  # The user account to connect (bind) to the LDAP server
  # For IBM Security Directory Server this is often 'cn=bind'
  LDAP_SERVER_BINDDN: 'administrator'

  # The LDAP_SERVER_SEARCHBASE is often referred to as the Base DN
  LDAP_SERVER_SEARCHBASE: 'cn=users,dc=aml,dc=ibm,dc=com'
  LDAP_PROFILE_DISPLAYNAME: 'displayName'
  LDAP_PROFILE_EMAIL: 'userPrincipalName'
  LDAP_PROFILE_GROUPS: 'memberOf'
  LDAP_PROFILE_ID: 'sAMAccountName'
  LDAP_PROFILE_TENANTS: 'fciTenants'
  LDAP_SERVER_USERNAME_MAPPING: 'sAMAccountName'

  # These are the default user search filters:
  # for Microsoft Active Directory: objectcategory=user
  # for IBM Security Directory Server: objectclass=inetOrgPerson
  # for OpenLDAP: objectClass=inetOrgPerson
  # If using a custom LDAP user search filter, uncomment this property and enter the filter as the value.
  #LDAP_USER_FILTER_OVERRIDE: 'objectclass=ePerson'

  # Performs a nested group search.
  # Set to true only if the LDAP server does not support recursive server-side searches.
  LDAP_SERVER_RECURSIVE_SEARCH: 'false'

  auth:
    # Role definitions
    # Do not modify  the Role definitions below
    ROLE_ANALYST: 'analyst'
    ROLE_INVESTIGATOR: 'investigator'
    ROLE_SUPERVISOR: 'supervisor'
    ROLE_ADMIN: 'admin'
    ROLE_DATA_SCIENTIST: 'data_scientist'
    ROLE_EXECUTIVE: 'executive'
    ROLE_QUALITY_ASSURANCE: 'quality_assurance'
    ROLE_READ_ONLY: 'read_only'
    ROLE_CONFIDENTIAL: 'confidential'

    # UI configuration definitions
    # Do not modify the UI configurations below
    ROLE_SURVEILLANCE: 'si'
    ROLE_COMPLAINTS: 'si-complaints'
    ROLE_SI_VOICE: 'si-voice'
    ROLE_CASE_MANAGER: 'case_manager'
    ROLE_INSURANCE: 'insurance'
    ROLE_KYC: 'kyc'
    ROLE_GRAPH: 'graph'
    ROLE_ALERTS_INSIGHT: 'alerts_insight'
    ROLE_TOOLING: 'tooling'
    ROLE_TLS: 'tls'

    # Team definitions
    # NOTE: The team names must begin with TEAM_
    # These values can be changed but you must follow documentation
    # under the section called Changing Team Names (do a search for it in the
    # documentation). It is under topic Managing groups, roles, UI configurations, and teams
    TEAM_TRIAGE_TEAM: 'TriageTeam'
    TEAM_INVESTIGATION_TEAM: 'InvestigationTeam'
    TEAM_SUPERVISOR_TEAM: 'SupervisorTeam'
    TEAM_CONFIDENTIAL_TEAM: 'confidential'
    TEAM_VENDOR_TEAM: 'VendorTeam'
    TEAM_LEGAL_TEAM: 'LegalTeam'

  # Set the Additional Group ID settings that the containers should run as
  # This depends on the GID of the shared storage like NFS
  fsGroupConfig:
    supplementalGroups:
    - 0

  hdp:
    # If using a non-prod HDP topology do two things to configure the HDP_NAME_NODE
    # i) uncomment the line below and provide HDP_MASTER_NODE (just delete the #)
    #HDP_NAME_NODE: 'hdfs://<HDP_MASTER_NODE>:8020'
    # ii) comment out the other line HDP_NAME_NODE: 'hdfs://fcicluster'

    # If using the HDP production environment, leave the line below as is. For HDP dev env. & single server comment it out
    HDP_NAME_NODE: 'hdfs://fcicluster'

    # Set to the fully qualified domain name of the HDP master node.
    HDP_MASTER_NODE: 'HDP_MASTER_FQDN'

    # If using the HDP production environment, make sure to set to the fully qualified domain name of the HDP secondary master node.
    HDP_SECONDARY_MASTER_NODE: 'HDP_SECONDARY_MASTER_FQDN'

    # Set to the fully qualified domain name of the HDP Ambari server.
    HDP_AMBARI_NODE: 'AMBARI_FQDN'

  kerberos:
    # This value should not be changed.
    KERBEROS_REALM: 'FCI.IBM'

    KINIT_SECONDS: '3600'
    HADOOP_VERSION: '3.x'
    HDP_CLUSTER_NAME: 'fcicluster'
    KERBEROS_ENABLED: 'true'

    runAsUser: 1000
    resources:
      requests:
        memory: "64Mi"
        cpu: ".1"
      limits:
        memory: "512Mi"
        cpu: ".5"


  hbase:
    # For the hbase_zookeeper_quorum property, you must specify at least one zookeeper
    # server. For a 6 server hdp install (prod topology)  Zookeeper servers are: HDP Master Node.
    # HDP Gateway and HDP Secondary Master. For 3 server hdp install (dev topology) the Zookeeper servers are:
    # HDP Master and HDP Gateway. For single server hdp install the Zookeeper server is the HDP Hostname.
    # Do not inlcude the <> in the hostname.  For example
    # hbase_zookeeper_quorum: 'hdp-master.acme.com,hdp-gw.acme.com,hdp-2gw.acme.com'
    hbase_zookeeper_quorum: '<ZOOKEEPER_SERVER_1,ZOOKEEPER_SERVER_2,ZOOKEEPER_SERVER_N>'
    hbase_zookeeper_property_clientPort: '2181'

  commonUI:
    # For ROKS and Cloud Pak for Data releases, add full hostname (subdomain of the ingress subdomain) of fci-common-ui-nginx route for http traffic
    SSO_HOSTNAME: "<roks_subdomain>"
    # Prior releases to 6.5, keep this to 3000 for the security-auth service port.
    SSO_PORT: "443"

  # CP4D Application Volume name to be created for analytics runtime 
  FCII_APP_VOLUME_NAME: fciiappvolume

db2:
  hostIPC: true
  # Select antiAffinity as either hard or soft
  podAntiAffinity: hard

  nodeSelector: {}

  resources:
    requests:
      memory: "5Gi"
      cpu: "0.5"
    limits:
      memory: "32Gi"
      cpu: "8"

  # Load Balancer Node Ports - Ports to expose in load balancer for Cloud Pak for Data installs
  # Set to an empty string to use a random port
  sslNodePort: "30560" 
 
  image:
    repository: ibmcom/fci-data-store-server
    tag: <BUILD_TAG>
    pullPolicy: "IfNotPresent"

  instancePvcSpec:
    accessModes:
      - ReadWriteOnce
    storageClassName: null
    size: 1Gi

  dbPvcSpec:
    accessModes:
      - ReadWriteOnce
    storageClassName: null
    size: 100Gi

  scriptsPvcSpec:
    accessModes:
      - ReadWriteMany
    storageClassName: null
    size: 1Gi

  storageType:
    META_STORAGE_CLASS: null
    DATA_STORAGE_CLASS: null
    BACKUP_STORAGE_CLASS: null

  config:
    ENV_HEALTHCHECK_CONNECTION_TEST: 'false'
    com_fci_db2_precheck: 'true'
    NAMESPACE: 'fci'
    DB2OLTP_VERSION: '11.5.5.0-x86_64'
    ARCHITECTURE: 'x86_64'
    DB2_CPU: '2'
    DATABASE_NAME: 'FCIDB'
    METADATA_DEDICATED: 'false'
    METADATA_DISPLAY_NAME: 'IBM FCI DB2'
    DB2_MEM: '6'
    META_STORAGE_SIZE: '10'
    DATA_STORAGE_SIZE: '10'
    BACKUP_STORAGE_SIZE: '10'
    META_STORAGE_ACCESS_MODE: 'ReadWriteMany'
    DATA_STORAGE_ACCESS_MODE: 'ReadWriteMany'
    BACKUP_STORAGE_ACCESS_MODE: 'ReadWriteMany'
    DEBUG_PROVISIONING: "false"
    CONNECTION_NAME: "FCI_DB2"
    PAGE_SIZE: "32768"
    DEDICATED: 'false'
    PAGE_SIZE: "32768"
    LOGFILSIZ: "25000"
    LOGPRIMARY: "10"
    LOGSECOND:  "100"
    APPLHEAPSZ: "2048"
    APP_CTL_HEAP_SZ: "1024"
    PCKCACHESZ: "AUTOMATIC"
    LOCKTIMEOUT: "30"

global-name-mgmt:
  enabled: false

  image:
    repository: ibmcom/fci-global-name-mgmt
    tag: <BUILD_TAG>
    pullPolicy: "IfNotPresent"

  gnmPvcSpec:
    accessModes:
    - ReadWriteMany
    storageClassName: null
    size: 1Gi  

kafka:
  # Set to False to disable Kafka in a non-production environment
  enabled: True

  # A cluster of three Kafka and Zookeeper instances is created by default.
  # In a non-production environment, a cluster of one Kafka and Zookeeper instances can be used by setting replicas to 1.
  replicas: 3
  zookeeperReplicaCount: 3

  pvcSpec:
    storageClassName: null
    size: 10Gi

  image:
    repository: ibmcom/fci-kafka
    tag: "<BUILD_TAG>"
    pullPolicy: "IfNotPresent"

  ## Topic creation and configuration.
  ## The job will be run on a deployment only when the config has been changed.
  ## - If 'partitions' is specified we create the topic (with --if-not-exists.)
  ## - If 'partitions' is specified we 'alter' the number of partitions. This will
  ## silently and safely fail if the new setting isn't strictly larger than the old (i.e. a NOOP.) Do be aware of the
  ## implications for keyed topics (ref: https://docs.confluent.io/current/kafka/post-deployment.html#admin-operations)
  ## - If 'defaultConfig' is specified it's deleted from the topic configuration. If it isn't present,
  ## it will silently and safely fail.
  ## - If 'config' is specified it's added to the topic configuration.
  ##
  topics:
    - name: Party-General
      partitions: 1
    - name: Party-Match-Upload
      partitions: 1
    - name: Party-Resolved-Update
      partitions: 1
    - name: Party-Delete
      partitions: 1
    - name: FCI_IGA_AI_Data_Account
      partitions: 1
    - name: FCI_IGA_AI_Data_Transaction
      partitions: 1
    - name: FCDD_ML_CLASSIFIED
      partitions: 1
    - name: FCI_SEC_AuditRecords
      partitions: 1
    - name: sifs.email.in
      partitions: 10
      config: "retention.ms=86400000"
    - name: sifs.chat.in
      partitions: 10
    - name: sifs.alert.in
      partitions: 1
    - name: sifs.ecomm.in
      partitions: 10
    - name: sifs.voice.in
      partitions: 1
    - name: sifs.attach.in
      partitions: 10
      config: "retention.ms=86400000"

  ## Configuration Overrides. Specify any Kafka settings you would like set on the StatefulSet
  ## here in map format, as defined in the official docs.
  ## ref: https://kafka.apache.org/documentation/#brokerconfigs
  ## The end user does not need to edit any of these configurations
  configurationOverrides:
    "listener.security.protocol.map": "SSL:SSL,EXTERNAL:SSL"
    "inter.broker.listener.name": "SSL"
    "ssl.endpoint.identification.algorithm": ""

    # client authentication is requested, but a client without certs can still connect.
    # Change to "required" if client authentication is required.
    "ssl.client.auth": "requested"
    "confluent.support.metrics.enable": false
    "advertised.listeners": |-
     EXTERNAL://${MASTER_HOST}:${NODE_PORT}

  # First Listerner Port to hit the kafka externally.
  # Set to an empty string to use a random port
  nodeport:
    firstListenerPort: "31090"
  zookeeper:
    image:
      repository: ibmcom/fci-zookeeper
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent
    # The external port for Zookeeper.
    # Set to an empty string to use a random port
    externalNodePort: "32181"

    dataPvcSpec:
      storageClassName: null
      size: 10Gi

    logPvcSpec:
      storageClassName: null
      size: 10Gi

ibm-sch:
  image:
    repository: opencontent-common-utils
    tag: 1.1.4
    pullPolicy: IfNotPresent

mongodb:
  enabled: True
  securityContext:
    enabled: false
  # If the customer has permanently disabled IPv6
  # (including for all kernel updates)
  # change this value to false so that mongodb will start.
  mongodbEnableIPv6: true

  pvcSpec:
    accessModes:
      - ReadWriteOnce
    storageClassName: null
    size: 10Gi

  image:
    repository: ibmcom/fci-mongodb
    tag: <BUILD_TAG>
    pullPolicy: IfNotPresent

elasticsearch:
  enabled: True

  config:
    ES_ADMIN_DN: "CN=FCI_server,OU=FCI_platform,O=FCI_Development,C=US"

    # Set the minimum heap size (Xms) and maximum heap size (Xmx) to be equal to each other.
    ES_JAVA_OPTS: "-Xmx1g -Xms1g"

    # The Elasticsearch user id is fci_admin
    # The number of minutes to wait until Elasticsearch starts.
    # If Elasticsearch has not started within this amount of time, then the Elasticsearch
    # container will be restarted.
    # Increase this value on systems with a slow disk.
    WAIT_MINUTES: '3'

  image:
    repository: ibmcom/fci-logsearch
    tag: "<BUILD_TAG>"
    pullPolicy: "IfNotPresent"

  # If upgrading from the 6.5 release, then change below to ReadWriteMany
  pvcSpec:
    accessModes:
    - ReadWriteOnce
    storageClassName: null
    size: 10Gi

odm:
  # set to False to disable ODM
  # set to True otherwise
  enabled: False

  dbClient:
    image:
      repository: ibmcom/fci-rms-odm-data-store
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent

  odm:
    image:
      repository: ibmcom/fci-rms-odm
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent
    resources:
      requests:
        memory: "256Mi"
        cpu: "0.1"
      limits:
        memory: "8Gi"
        cpu: "3"

  config:
    FLYWAY_BASELINE_VERSION: '01.01.01.00.005'

wca:
  enabled: false

  pvcSpec:
    accessModes:
      - ReadWriteMany
    storageClassName: null
    size: 30Gi

  image:
    repository: ibmcom/fci-wca
    tag: <BUILD_TAG>
    pullPolicy: "IfNotPresent"

cognos:
  enabled: false

  image:
    repository: ibmcom/fci-cognos
    tag: <BUILD_TAG>
    pullPolicy: "IfNotPresent"

  pvcSpec:
    accessModes:
      - ReadWriteMany
    storageClassName: null
    size: 1Gi

  resources:
    requests:
      memory: "10Gi"
      cpu: "1"
    limits:
      memory: "16Gi"
      cpu: "4"

  config:
    # MAX_HTTP_HEADER_SIZE: max size in bytes of HTTP headers that Cognos will allow in HTTP requests
    MAX_HTTP_HEADER_SIZE: "16000"
    # The folowing values are for the CPD-provisioned Cognos Analytics with db2-as-a-service instance.
    # DATA_STORAGE_CLASS is used for COGNOSDB as well as other Cognos PVCs. Make sure it supports the access mode. 
    # DATA_STORAGE_SIZE is for COGNOSDB.
    DATA_STORAGE_CLASS: ""
    DATA_STORAGE_ACCESS_MODE: ReadWriteMany
    DATA_STORAGE_SIZE: "10"
    DATABASE_NAME: COGNOSDB
    APPLHEAPSZ: "2048"
    APP_CTL_HEAP_SZ: "1024"
    CONNECTION_NAME: FCI_COGNOS
    DB2_CPU: "2"
    DB2_MEM: "6"
    DEBUG_PROVISIONING: "false"
    DEDICATED: "false"
    LOCKTIMEOUT: "30"
    LOCKTIMEOUT: "30"
    LOGFILSIZ: "25000"
    LOGPRIMARY: "10"
    LOGSECOND: "100"
    METADATA_DISPLAY_NAME: IBM FCI COGNOS DB2
    NAMESPACE: 'fci'
    PAGE_SIZE: "32768"
    PCKCACHESZ: AUTOMATIC
    DB2OLTP_VERSION: '11.5.5.0-x86_64'

  proxy:
    resources: {}
    image:
      repository: ibmcom/fci-dashboards-proxy
      tag: <BUILD_TAG>
      pullPolicy: "IfNotPresent"

logging:
  # set to False to disable logging infrastructure in a non-production environment
  enabled: True

  logstash:
    image:
      repository: ibmcom/fci-loganalysis
      tag: <BUILD_TAG>

  kibana:
    image:
      repository: ibmcom/fci-logvisualization
      tag: <BUILD_TAG>

security-audit:
  # set to False to disable auditing in a non-production environment
  enabled: True

  image:
    repository: ibmcom/fci-security-audit-app
    tag: <BUILD_TAG>
    pullPolicy: IfNotPresent

  config:
    FLYWAY_BASELINE_VERSION: '2019.02.28.13.59.00'
    AUDIT_STORE: "CLOUD_PAK"
    AUDIT_FILE_LOCATION: "/var/log/fciAudit-%g.log"
    AUDIT_FILE_SIZE_IN_BYTES: "10000000"
    AUDIT_NUM_FILES: "2"
    CONTAINERNAME: "audit"
    ENABLE_AUDIT: "true"

  sidecar:
    image:
      repository: ibmcom/fci-audit-sidecar
      tag: '<BUILD_TAG>'
      pullPolicy: IfNotPresent
    resources:
      requests:
        memory: "256Mi"
        cpu: ".2"
      limits:
        memory: "512Mi"
        cpu: ".5"

  db2:
    image:
      repository: ibmcom/fci-security-audit-data-store
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent

security-auth:
  replicas: 2
  nodejs:
    resources: {}
    image:
      repository: ibmcom/fci-security-auth
      tag: '<BUILD_TAG>'
      pullPolicy: IfNotPresent
  redis:
    resources: {}
    image:
      repository: ibmcom/fci-memoryds
      tag: '<BUILD_TAG>'
      pullPolicy: IfNotPresent

  translationJob:
    image:
      repository: ibmcom/fci-translations
      tag: <BUILD_TAG>
      pullPolicy: "Always"
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 50m
        memory: 64Mi

  securityAuthConfig:
    # set to '0' to disable auditing in a non-production environment
    AUDIT_ACTIVE: '1'

    # Role groups
    GROUP_ANALYST: 'CN=analysts,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_INVESTIGATOR: 'CN=investigators,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_SUPERVISOR: 'CN=supervisors,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_ADMIN: 'CN=admins,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_DATA_SCIENTIST: 'CN=datascientists,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_EXECUTIVE: 'CN=executives,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_QUALITY_ASSURANCE: 'CN=qualityassurance,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_READ_ONLY: 'CN=readonly,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_CONFIDENTIAL: 'CN=confidential,CN=Users,DC=aml,DC=ibm,DC=com'

    # UI configuration groups
    GROUP_SURVEILLANCE: 'CN=surveillance,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_COMPLAINTS: 'CN=complaints,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_VOICE: 'CN=voice,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_CASE_MANAGER: 'CN=casemanager,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_INSURANCE: 'CN=insurance,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_KYC: 'CN=kyc,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_GRAPH: 'CN=graph,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_ALERTS_INSIGHT: 'CN=alerts_insight,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_TOOLING: 'CN=tooling,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_TLS: 'CN=tls,CN=Users,DC=aml,DC=ibm,DC=com'

    # Team groups
    GROUP_TRIAGE_TEAM: 'CN=TriageTeam,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_INVESTIGATION_TEAM: 'CN=InvestigationTeam,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_SUPERVISOR_TEAM: 'CN=SupervisorTeam,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_CONFIDENTIAL_TEAM: 'CN=confidential,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_VENDOR_TEAM: 'CN=VendorTeam,CN=Users,DC=aml,DC=ibm,DC=com'
    GROUP_LEGAL_TEAM: 'CN=LegalTeam,CN=Users,DC=aml,DC=ibm,DC=com'

    # If integrating with Cloud Pak for Data authentication, this property specifies
    # whether to create internal users in the Cloud Pak for Data user registry.
    # Valid values are "True" and "False".
    CREATE_CP_INTERNAL_USERS: "True"

    # specify internal user registry (also known as basic user registry) from WebSphere Liberty
    # for each numbered user, specify the user's id, roles, tenants, and password
    # roles are comma separated
    # tenants are comma separated
    # valid roles include: admin, analyst, investigator, supervisor, data_scientist
    # password are specified in the secrets.yaml file of the install toolkit
    # to add additional users, add additional user properties numbered 17, 18, etc.
    USER_ID_1: fciadmin
    USER_ROLES_1: admin,supervisor,investigator,analyst,data_scientist
    USER_CPROLE_1: FCI administrator
    USER_TEAMS_1: SupervisorTeam,InvestigationTeam,TriageTeam
    USER_TENANTS_1: ibm.com
    USER_ID_2: fcianalyst
    USER_ROLES_2: analyst
    USER_CPROLE_2: FCI analyst
    USER_TEAMS_2: TriageTeam
    USER_TENANTS_2: ibm.com
    USER_ID_3: fciinvestigator
    USER_ROLES_3: investigator
    USER_CPROLE_3: FCI investigator
    USER_TEAMS_3: InvestigationTeam
    USER_TENANTS_3: ibm.com
    USER_ID_4: fcisupervisor
    USER_ROLES_4: supervisor
    USER_CPROLE_4: FCI supervisor
    USER_TEAMS_4: SupervisorTeam,InvestigationTeam,TriageTeam
    USER_TENANTS_4: ibm.com
    USER_ID_5: fcivendor
    USER_ROLES_5: analyst,insurance
    USER_CPROLE_5: Claims Fraud vendor
    USER_TEAMS_5: VendorTeam
    USER_TENANTS_5: ibm.com
    USER_ID_6: fciconfidential
    USER_ROLES_6: confidential
    USER_TEAMS_6: confidential
    USER_TENANTS_6: ibm.com
    USER_ID_7: fciiadmin
    USER_ROLES_7: admin,insurance
    USER_CPROLE_7: Claims Fraud administrator
    USER_TEAMS_7: SupervisorTeam,InsuranceTeam
    USER_TENANTS_7: ibm.com
    USER_ID_8: fciianalyst
    USER_ROLES_8: analyst,insurance
    USER_CPROLE_8: Claims Fraud analyst
    USER_TEAMS_8: TriageTeam,InsuranceTeam
    USER_TENANTS_8: ibm.com
    USER_ID_9: fciiinvestigator
    USER_ROLES_9: investigator,insurance
    USER_CPROLE_9: Claims Fraud investigator
    USER_TEAMS_9: InvestigationTeam,InsuranceTeam
    USER_TENANTS_9: ibm.com
    USER_ID_10: fciisupervisor
    USER_ROLES_10: supervisor,insurance
    USER_CPROLE_10: Claims Fraud supervisor
    USER_TEAMS_10: SupervisorTeam,InsuranceTeam
    USER_TENANTS_10: ibm.com
    USER_ID_11: tlsadmin
    USER_ROLES_11: admin,tls
    USER_TEAMS_11: SupervisorTeam
    USER_TENANTS_11: ibm.com
    USER_ID_12: fcaisupervisor
    USER_ROLES_12: alerts_insight,supervisor
    USER_TEAMS_12: TriageTeam
    USER_TENANTS_12: ibm.com
    USER_ID_13: fcaianalyst
    USER_ROLES_13: alerts_insight,analyst
    USER_TEAMS_13: TriageTeam
    USER_TENANTS_13: ibm.com
    USER_ID_14: fcaiinvestigator
    USER_ROLES_14: alerts_insight,investigator
    USER_TEAMS_14: TriageTeam
    USER_TENANTS_14: ibm.com
    USER_ID_15: fcaiadmin
    USER_ROLES_15: alerts_insight,admin
    USER_TEAMS_15: TriageTeam
    USER_TENANTS_15: ibm.com
    USER_ID_16: fcaiexecutive
    USER_ROLES_16: alerts_insight,executive
    USER_TEAMS_16: TriageTeam
    USER_TENANTS_16: ibm.com
    USER_ID_17: toolingadmin
    USER_ROLES_17: admin,data_scientist,tooling
    USER_CPROLE_17: Conduct Surveillance tooling administrator
    USER_TEAMS_17: SupervisorTeam
    USER_TENANTS_17: ibm.com
    USER_ID_18: graphadmin
    USER_ROLES_18: admin,supervisor,graph
    USER_CPROLE_18: Graph analytics admin
    USER_TEAMS_18: SupervisorTeam
    USER_TENANTS_18: ibm.com
    USER_ID_19: sisupervisor
    USER_ROLES_19: si,si-complaints,supervisor
    USER_CPROLE_19: Conduct Surveillance supervisor
    USER_TEAMS_19: SupervisorTeam,InvestigationTeam,TriageTeam
    USER_TENANTS_19: ibm.com
    USER_ID_20: sianalyst
    USER_ROLES_20: si,analyst
    USER_CPROLE_20: Conduct Surveillance analyst
    USER_TEAMS_20: TriageTeam
    USER_TENANTS_20: ibm.com
    USER_ID_21: siinvestigator
    USER_ROLES_21: si,investigator
    USER_CPROLE_21: Conduct Surveillance investigator
    USER_TEAMS_21: InvestigationTeam
    USER_TENANTS_21: ibm.com
    USER_ID_22: siadmin
    USER_ROLES_22: si,si-voice,admin
    USER_CPROLE_22: Conduct Surveillance administrator
    USER_TEAMS_22: SupervisorTeam,InvestigationTeam,TriageTeam
    USER_TENANTS_22: ibm.com
    USER_ID_23: kycadmin
    USER_ROLES_23: admin,supervisor,investigator,analyst,data_scientist,kyc
    USER_TEAMS_23: SupervisorTeam,InvestigationTeam,TriageTeam
    USER_MAIL_23: kycadmin@ibm.com
    USER_DISPLAY_NAME_23: kycadmin
    USER_TENANTS_23: ibm.com
    USER_ID_24: fcilegal
    USER_ROLES_24: supervisor,insurance
    USER_CPROLE_24: Claims Fraud legal
    USER_TEAMS_24: LegalTeam
    USER_TENANTS_24: ibm.com

    # JWT_KEY_EXPIRY is the length of time before the security token expires
    # This controls the length of a user's session after logging in.
    # The value is expressed in a time span. e.g.:  "30m", "4h", "1d"
    JWT_KEY_EXPIRY: '4h'
    # DO NOT CHANGE THIS VALUE:
    JWT_ISSUER: 'fci.ibm.com'

    #SAML configuration
    SAML_DISABLEREQUESTEDAUTHNCONTEXT: "true"
    # identity provider entrypoint
    # This is typically the URL provided by the identity provider for a user to log in
    SAML_ENTRY_POINT: 'https://<hostname>/adfs/ls'
    # name identifier format to request from identity provider
    SAML_IDENTIFIERFORMAT: 'urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified'
    # issuer string to supply to identity provider
    SAML_ISSUER: 'https://<hostname>/adfs/services/trust'
    # the SAML profile property that maps to a user's display name
    SAML_PROFILE_DISPLAYNAMEPROP: 'http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name'
    # the SAML profile property that maps to a user's email address
    SAML_PROFILE_EMAILPROP: 'http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress'
    # the SAML profile property that maps to a user's groups
    SAML_PROFILE_GROUPSPROP: 'http://schemas.xmlsoap.org/claims/Group'
    # the SAML profile property that maps to a user's id
    SAML_PROFILE_NAMEIDPROP: 'nameID'
    # SAML_ACCEPTED_CLOCK_SKEW is the time in milliseconds of skew
    # that is acceptable between client and server
    # when checking OnBefore and NotOnOrAfter assertion condition validity timestamps.
    # Setting to -1 will disable checking these conditions entirely.
    SAML_ACCEPTED_CLOCK_SKEW: '5000'

    ### IBM Cloud AppID configuration ###
    ## You can find these configuration values by clicking View credentials
    ## in the 'Service credentials' tab of your AppID service's manage section.
    # The client ID for the App ID instance of service.
    APPID_CLIENT_ID: <clientId>
    # Tenant ID is specific to our App ID instance
    APPID_TENANTID: <tenantId>
    # The App ID oauth server URL which is used for redirecting user to IBM Cloud app ID based authentication.
    APPID_OAUTH_SERVER_URL: "https://us-south.appid.cloud.ibm.com/oauth/v4/<tenantId>"
    # The App ID profiles URL.
    APPID_PROFILES_URL: "https://us-south.appid.cloud.ibm.com"
    # The App ID API version.
    APPID_VERSION: "4"
    # End Point for the app ID service.
    APPID_SERVICE_ENDPOINT: "https://us-south.appid.cloud.ibm.com"
    # Default Call back URL needed for redirecting to security-auth service after AppID authentication.
    APPID_CALLBACK_URL: "/security-auth/ibm/cloud/appid/callback"
    # IBM Cloud IAM Token URL
    APPID_IAM_TOKEN_URL: "https://iam.cloud.ibm.com/identity/token"

    # BRUTE_THROTTLING is set to 'true' to enable throttling of requests from an IP address
    # when too many invalid logins have occurred from that IP address.
    # It is used to prevent brute force attacks.
    # Set to 'false' to disable this feature.
    BRUTE_THROTTLING: 'true'
    # BRUTE_FREE_RETRIES specifies the number of invalid logins before throttling begins
    BRUTE_FREE_RETRIES: '5'

    # MULTI_TENANCY_ENABLED enable multi-tenancy fields in JWT
    MULTI_TENANCY_ENABLED: 'true'
    # Default tenants to use if none provided by LDAP and MULTI_TENANCY_ENABLED is set to true
    DEFAULT_TENANT_IF_MISSING: 'ibm.com'
    CLOUD_PAK_TOKEN: '/var/run/sharedsecrets/token'

case-manager:
  # set to False to disable Case Manager
  # set to True otherwise
  enabled: True

  mq:
    # Load Balancer Node Ports - Port to expose in loadbalancer for Cloud Pak for Data installs
    # Set to an empty string to use a random port
    mqNodePort: "30683"
    mqSeriesNodePort: "30997"
    image:
      repository: ibmcom/fci-messaging
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent
    resources:
      requests:
        memory: "256Mi"
        cpu: ".5"
      limits:
        memory: "512Mi"
        cpu: "2"
    nodeSelector: {}
    tolerations: []
    affinity: {}
  liberty:
    image:
      repository: ibmcom/fci-solution
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent
    resources:
      requests:
        memory: "256Mi"
        cpu: "1"
      limits:
        memory: "9Gi"
        cpu: "4"
    nodeSelector: {}
    tolerations: []
    affinity: {}
    # runAsUser is the user id that containers requiring updates to file system permissions
    # run as if pvRequiresPermissionsFix is set to true
    runAsUser: 1000
  kerberosClient:
    image:
      repository: ibmcom/fci-hdp-client
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent

  libertyConfig:
    env_fci_batch_userid: 'fcibatch'
    COM_FCI_HBASE_SITE_FILE: "/home/wlpadmin/hbase-site.xml"
    COM_FCI_CORE_SITE_FILE: "/home/wlpadmin/core-site.xml"
    COM_FCI_KRB5_CONF_FILE: "/home/wlpadmin/krb5.conf"

  solutionPvcSpec:
    accessModes:
    - ReadWriteMany
    storageClassName: null
    size: 1Gi

  # If upgrading from the 6.5 release, then change below to ReadWriteMany
  mqPvcSpec:
    accessModes:
    - ReadWriteOnce
    storageClassName: null
    size: 1Gi

cedm:
  # set to False to disable CEDM
  # set to True otherwise
  enabled: True

  # set to True to mount the Analytics Runtime application volume
  requireARMount: False

  liberty:
    image:
      repository: ibmcom/fci-cedm-integration
      tag: <BUILD_TAG>
    resources:
      requests:
        memory: "256Mi"
        cpu: "1"
      limits:
        memory: "12Gi"
        cpu: "4"
    # runAsUser is the user id that containers requiring updates to file system permissions
    # run as if pvRequiresPermissionsFix is set to true
    runAsUser: 1000
  kerberosClient:
    image:
      repository: ibmcom/fci-hdp-client
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent

  db2:
    image:
      repository: ibmcom/fci-cedm-data-store-client
      tag: <BUILD_TAG>

  libertyPvcSpec:
    accessModes:
      - ReadWriteMany
    storageClassName: null
    size: 30Gi

  cedmConfig:
    FLYWAY_BASELINE_VERSION: '01.01.01.00.119'

    #Kafka max message size in producer (Broker max message size - 1KB)
    KAFKA_MAX_REQUEST_SIZE: '19998988'

    # Publish batch size and frequency
    PARTY_PUBLISH_BATCH_SIZE: '250'
    PARTY_PUBLISH_PERIOD_SECONDS: '300'

    # HDP VERSION 2.6 or 3.1.0
    HDP_VERSION: '3.1.0'

    # Big Match Oozie Workflow Config
    # The number of worker tasks, threads, Java heap size are automatically adjusted during  
    # installation by the IBM data platform install script that takes into account HDP cluster resources.
    OOZIE_MAX_JVM_HEAP_SIZE: '4712'
    OOZIE_NUM_OF_WORKER_TASKS: '4'
    OOZIE_NUM_OF_THREADS_FOR_EACH_WORKER: '1'
    OOZIE_PORT: '11443'
    OOZIE_JOBTRACKER_PORT: '8050'
    OOZIE_SECURE_CONNECTION: 'true'
    # The Fully Qualified DNS Name (FQDN) of the Oozie server.  The Oozie server is the
    # Hadoop Master for both production topology (6 servers), dev topology (3 servers) and
    # single HDP server install.  Do not include the <> in the hostname
    OOZIE_SERVER: '<hostname>'
    OOZIE_USER_NAME: 'bigmatch'

    #  Scoring Oozie workflow config Spark parameters
    # By default, yarn decides the HDP container requirements. The Scoring Driver & Executor Memory & 
    # Number of Executors are automatically adjusted during installation by the IBM data platform install 
    # script that takes into account HDP cluster resources.
    SCORING_DRIVER_MEMORY: '2048m'
    SCORING_EXECUTOR_MEMORY: '2048m'
    SCORING_NO_OF_EXECUTORS: '8'
    SCORING_NO_PARTITIONS: '8'

    # Time interval for running Entity Resolution (in seconds)
    RESOLUTION_TIME_INTERVAL: '1800'
    # Complete current Entity Resolution workflow jobs before starting
    # another one. Applies to interval processing only.
    RESOLUTION_COMPLETE_WORKFLOW: 'false'
    # Daily scheduled time for running Entity Resolution (HH:mm)
    #RESOLUTION_RUNTIME: '23:59'

    # Time interval for running the Watchlist matching process (in seconds)
    WATCHLIST_PROCESSING_INTERVAL: '300'
    # Daily scheduled time for running the Watchlist matching process (HH:mm)
    # WATCHLIST_PROCESSING_RUNTIME: '23:59'

    ## Party enrichment configuration
    # Entity Enrichment Endpoint(default=Entity Enrichment product exposed endpoint as specified below)
    ENRICHMENT_ENDPOINT: 'https://fci-ees-engine:443/v1/entities/auto-enrich'
    # Comma separated list of plans to be executed
    # A value of '' means party enrichment is not enabled
    ENRICHMENT_PLANS: ''
    # Party type of enrich (o=Organizations, i=Individuals, a=All  default=a)
    ENRICHMENT_PARTY_TYPE: 'a'
    # Party operation type to enrich (i=Insert, a=all (Insert and Update)  default=a)
    ENRICHMENT_OPERATION_TYPE: 'a'
    # Party process type using enrichment. A value of
    # i. 'false' allows only parties created via REST API to be enriched.
    # ii.'true' allows all parties(created via async - bulk load, or sync - REST API) to be enriched
    # default='true'
    ENRICHMENT_ALL_PARTIES: 'true'
    # Enrichment batch size and frequency
    ENRICHMENT_PUBLISH_BATCH_SIZE: '100'
    ENRICHMENT_PUBLISH_PERIOD_SECONDS: '300'


    # HBase REST Configuration
    # The value is required for
    # a) DERS
    # b) watchlist processing

    # fully qualified domain name of the Hadoop  (HDP) master node, for example HBASE_REST_SERVER: 'master.acme.com'
    HBASE_REST_SERVER: '<hostname>'
    HBASE_REST_PORT: '9081'
    HBASE_REST_SECURE_CONNECTION: 'true'

search:
  # set to False to disable Search
  # set to True otherwise
  enabled: True

  liberty:
    resources: {}
    image:
      repository: ibmcom/fci-search-liberty
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent

common-ui:
  nodejs:
    image:
      repository: ibmcom/fci-common-ui
      tag: <BUILD_TAG>
  nginx:
    image:
      repository: ibmcom/fci-common-ui-web
      tag: <BUILD_TAG>
  investigativeUI:
    image:
      repository: ibmcom/fci-investigative-ui
      tag: <BUILD_TAG>

    # set to 0 to disable the Investigative UI
    # otherwise set to 1
    replicas: 1

  iuiStaticConfig:
    image:
      repository: ibmcom/fci-investigative-ui-config
      tag: <BUILD_TAG>


  iuiConfigService:
    image:
      repository: ibmcom/fci-investigative-ui-config-service
      tag: <BUILD_TAG>

  iuiNarrativeService:
  # set to False to disable Narrative Service
  # set to True otherwise
    enabled: False

    image:
      repository: ibmcom/fci-narrative-service
      tag: <BUILD_TAG>

  nginxConfig:
    # INCLUDE_CONFIG is a comma-separated list of components to include
    # valid values are: case,cedm,odm,search,fcai,fcdd,sifs,graph,art
    INCLUDE_CONFIG: 'case,cedm,odm,search,graph'


  bkPvcSpec:
    accessModes:
      - ReadWriteMany
    storageClassName: null
    size: 1Gi

  nginxPvcSpec:
    accessModes:
      - ReadWriteMany
    storageClassName: null
    size: 512Mi

  config:
    APP_ROLE_ANALYST: 'home'
    APP_ROLE_INVESTIGATOR: 'home'
    APP_ROLE_SUPERVISOR: 'home'
    APP_ROLE_ADMIN: 'home'
    APP_ROLE_DATA_SCIENTIST: 'home'
    APP_ROLE_EXECUTIVE: 'home'
    APP_ROLE_QUALITY_ASSURANCE: 'home'
    APP_ROLE_READ_ONLY: 'home'
    APP_ROLE_CONFIDENTIAL: 'home'

    # Set to 'true' if SSO is enabled.  The login form will then be replaced with a link to login through the SSO provider.
    SSO_ENABLED: 'false'
    # if using SAML without App ID then change to: security-auth/api/v1.0/login/saml
    SSO_PATH: "security-auth/api/v1.0/login/appid"

cdn-proxy:
  cdnProxy:
    image:
      repository: ibmcom/fci-cdn-proxy
      tag: <BUILD_TAG>

  bkPvcSpec:
    accessModes:
      - ReadWriteMany
    storageClassName: null
    size: 5Gi

dsf:
  replicas: 0
  image:
    pullPolicy: IfNotPresent
    repository: ibmcom/kyc-data-source-framework
    tag: <BUILD_TAG>
  dsfConfig:
    DNB_USERNAME: ''
    POPULATE_DEFINITIONS_ON_STARTUP: 'true'
    DOWJONES_USERNAME: ''
    DOWJONES_NAMESPACE: ''
    KYCKR_USERNAME: ''
    TRANSUNION_TLO_USERNAME: ''
    TRANSUNION_EBUREAU_USERNAME: ''
    TRANSUNION_IDVISION_USERNAME: ''

ees-engine:
  replicas: 0
  image:
    pullPolicy: IfNotPresent
    repository: ibmcom/fci-shell-detection
    tag: <BUILD_TAG>

graph-writer:
  # set to False to disable Graph Liberty server
  # set to True otherwise
  enabled: True
  # runAsUser is the user id that containers requiring updates to file system permissions
  # run as if pvRequiresPermissionsFix is set to true
  runAsUser: 1000

  liberty:
    image:
      repository: ibmcom/fci-graph-writer
      tag: <BUILD_TAG>

  libertyInitPv:
    image:
      repository: ibmcom/fci-graph-writer-init-pv
      tag: <BUILD_TAG>

  kerberosClient:
    image:
      repository: ibmcom/fci-hdp-client
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent

  gremlin:
    image:
      repository: ibmcom/fci-graph-gremlin-server
      tag: <BUILD_TAG>
    graphGremlinConfig:
      gremlinNodePort: "30184"


  libertyPvcSpec:
    accessModes:
      - ReadWriteMany
    storageClassName: null
    size: 1Gi

  gremlinPvcSpec:
    accessModes:
      - ReadWriteMany
    storageClassName: null
    size: 1Gi

analytics-runtime:
  # set to False to disable Analytics Runtime
  # set to True otherwise
  enabled: False

  arLiberty:
    image:
      pullPolicy: IfNotPresent
      repository: ibmcom/fci-analytics-runtime-liberty
      tag: <BUILD_TAG>
    resources:
      requests:
        memory: "256Mi"
        cpu: "1"
      limits:
        memory: "8Gi"
        cpu: "4"

  arCanvas:
    image:
      pullPolicy: IfNotPresent
      repository: ibmcom/fci-analytics-runtime-canvas
      tag: <BUILD_TAG>
    resources:
      requests:
        cpu: "0.5"
        memory: "256Mi"
      limits:
        cpu: "2"
        memory: "2Gi"

  arLibertyConfig:
    CP4D_URL: https://internal-nginx-svc:12443
    FCII_SPARK_INSTANCE_NAME: 'fciisparkinstance'
    FCII_SPARK_VOLUME_NAME: 'fciisparkvolume'
    FCII_SPARK_VOLUME_SIZE: '20Gi'
    FCII_APP_VOLUME_SIZE: '20Gi'

  kerberosClient:
    image:
      repository: ibmcom/fci-hdp-client
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent

realtime-scoring:
  enabled: false
  liberty:
    image:
      repository: ibmcom/fci-realtime-scoring
      tag: <BUILD_TAG>
      pullPolicy: IfNotPresent
    resources:
      requests:
        memory: 256Mi
        cpu: "1"
      limits:
        memory: "8Gi"
        cpu: "4"  
