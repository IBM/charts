global:
  postgres:
    auth:
      # global.postgres.auth.authSecretName - Name of the (manually created) secret that holds the password (key: `password`) for postgres superuser specified in global.`postgres.auth.user`. If empty the secrets is autogenerated with randomly generated password.
      authSecretName: ""
    
    # global.postgres.sslSecretName - Name of the manually (created) secret that holds the certificate (key `tls.crt`) and if not `global.postgres.create` is true then also private key for the certificate (key `tls.key`)
    # If the global.postgres.create is `false` (i.e., provide your own postgres, you describe provided postgres instance, if the valu is true, you provides cert/key pair to be used by postgres installed by the this chart.
    sslSecretName: ""
  
  image:
    repository: ""

backup:
  image:
    repository: "{{ tpl .Values.global.image.repository . }}"
    name: "opencontent-postgres-stolon"
    tag: "2.0.1"
    pullPolicy: "IfNotPresent"

  # To disable the cronjob, set suspend to true 
  suspend: false

  # Here you can change the schedule of the backups
  # For example, 0 23 * * * equates to 11pm everyday
  # see https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/
  schedule: "0 23 * * *"  
  
  # Specify how many jobs to keep and dumps stored in PVC (if backup.persistence.enabled is true) to keep
  history:
    jobs:
      success: 30 
      failed: 10 
    files:
      #Sunday=0, Monday=1, Tuesday=2 etc.
      weeklyBackupDay: 0
      #The number of backups to keep taken on weeklyBackupDay
      weekly: 4
      #The number of backups to keep that were taken on all the other days of the week
      daily: 6

  persistence:
    # if enabled is set to false, the backup will be stored in the job log. If set to true, the backup dumps will be stored in the PVC.
    enabled: true
    useDynamicProvisioning: "{{ tpl ( .Values.global.persistence.useDynamicProvisioning | toString ) . }}"

  dataPVC:
    name: "{{ .Release.Name }}-store-backup"
    storageClassName: "{{ tpl ( .Values.global.storageClassName                   | toString ) . }}"
    
    # Specify the name of the Existing Claim to be used by your application
    # empty string means don't use an existClaim
    existingClaimName: ""
    
    # if your not using dynamic provisioning, you can use selectors to 
    # refine the binding process. You cannot specify a selector if your using dynamic provisioning!
    selector:
      label: ""
      value: ""
    
    accessMode: ReadWriteOnce
    size: 1Gi

creds:
  image:
    repository: "{{ tpl .Values.global.image.repository . }}"
    name:       "conan-tools"
    tag:        "20200421-1841"
    pullPolicy: "IfNotPresent"
  resources:
    requests:
      cpu:    10m
      memory: 256Mi
    limits:
      cpu:    50m
      memory: 256Mi

arch:
  amd64: "2 - No prefrence"

config:

  global:
    license: true

  # config.nameOverride - Do not even think about changing this value
  nameOverride: store-postgres
  
  clusterName: "{{ .Release.Name }}"
  auth:
    pgSuperuserName: "{{ .Values.global.postgres.auth.user }}"
    authSecretName: '{{ include "assistant.postgres.secret_name" . }}'

  tls:
    enabled: true
    tlsSecretName: '{{ include "assistant.postgres.secret_name" . }}'


  persistence:
    # enabled - Data stre stored persistenly. Not to loose the data if pod is deleted/restarted
    enabled: true
    
    # storageClassName - set to null to use the default storageClassName
    # Note that glusterfs is not recommended (poor performance, stability???) of postgres 
    storageClassName:        "{{ tpl ( .Values.global.storageClassName                   | toString ) . }}"

    # config.persistence.useDynamicProvisioning - if enabled the  config.persistence.storageClassName volumes will be dynamicaly created (if the storage class can be created automatically).
    #  If disabled either dataPVC.selector.label should be specify and then the PVC will bound to precreated PV based on labels or dataPVC.storageClassName should be empty and then cluster admin has to bound the PVC to existing PV manually
    useDynamicProvisioning: "{{ tpl ( .Values.global.persistence.useDynamicProvisioning | toString ) . }}"
    
    #size - size of PVC
    #size: 10Gi
  dataPVC:
    name: pg
  
  keep: "{{ .Values.global.keepDatastores }}"

  creds:
    affinity: '{{ include "assistant.ibm-postgres.affinity.nodeAffinity"    . }}'
  createCluster:
    affinity: '{{ include "assistant.ibm-postgres.affinity.nodeAffinity"    . }}'
  test:
    affinity: '{{ include "assistant.ibm-postgres.affinity.nodeAffinity"    . }}'

  proxy:
    serviceType: ""
    resources:
      requests:
        cpu: "10m"
      limits:
        cpu: 4
    
    affinity: '{{ include "assistant.ibm-postgres.affinity.nodeAffinity"    . }}'
    
    # Enabling pod disruption budget
    podDisruptionBudget:
      maxUnavailable: 1
    
    topologySpreadConstraints:
      enabled:           "{{ tpl ( .Values.global.topologySpreadConstraints.enabled           | toString ) . }}"
      maxSkew:           "{{ tpl ( .Values.global.topologySpreadConstraints.maxSkew           | toString ) . }}"
      topologyKey:       "{{ tpl ( .Values.global.topologySpreadConstraints.topologyKey       | toString ) . }}"
      whenUnsatisfiable: "{{ tpl ( .Values.global.topologySpreadConstraints.whenUnsatisfiable | toString ) . }}"
  
  sentinel:
    resources:
      requests:
        cpu: "40m"
      limits:
        cpu: 4
    
    affinity: '{{ include "assistant.ibm-postgres.affinity.nodeAffinity"    . }}'
    
    # Enabling pod disruption budget
    podDisruptionBudget:
      maxUnavailable: 1
    
    topologySpreadConstraints:
      enabled:           "{{ tpl ( .Values.global.topologySpreadConstraints.enabled           | toString ) . }}"
      maxSkew:           "{{ tpl ( .Values.global.topologySpreadConstraints.maxSkew           | toString ) . }}"
      topologyKey:       "{{ tpl ( .Values.global.topologySpreadConstraints.topologyKey       | toString ) . }}"
      whenUnsatisfiable: "{{ tpl ( .Values.global.topologySpreadConstraints.whenUnsatisfiable | toString ) . }}"
  
  keeper:
    resources:
      requests:
        cpu: "50m"
      limits:
        cpu: 4
    
    affinity: '{{ include "assistant.ibm-postgres.affinity.nodeAffinity"    . }}'
    
    # Enabling pod disruption budget
    podDisruptionBudget:
      maxUnavailable: 1
    
    topologySpreadConstraints:
      enabled:           "{{ tpl ( .Values.global.topologySpreadConstraints.enabled           | toString ) . }}"
      maxSkew:           "{{ tpl ( .Values.global.topologySpreadConstraints.maxSkew           | toString ) . }}"
      topologyKey:       "{{ tpl ( .Values.global.topologySpreadConstraints.topologyKey       | toString ) . }}"
      whenUnsatisfiable: "{{ tpl ( .Values.global.topologySpreadConstraints.whenUnsatisfiable | toString ) . }}"

  metering:
    productName:              "IBM Watson Assistant for IBM Cloud Pak for Data"
    productID:                "ICP4D-addon-fa92c14a5cd74c31aab1616889cbe97a-assistant"
    productVersion:           "1.4.2"
    cloudpakName:             "IBM Cloud Pak for Data"
    cloudpakId:               "eb9998dcc5d24e3eb5b6fb488f750fe2"
    cloudpakVersion:          "3.0.0"
    productChargedContainers: "All"
    productMetric:            "VIRTUAL_PROCESSOR_CORE"

  antiAffinity:
    policy: '{{- if or (eq .Values.global.podAntiAffinity "Enable") (and (eq .Values.global.deploymentType "Production") (ne .Values.global.podAntiAffinity "Disable")) -}} hard {{- else -}} soft {{- end -}}'

  affinity: 

# The suffix of all the cluster DNS names like service_name.service_namespace.svc.cluster.local
#clusterDomain: "cluster.local"
clusterDomain: "{{ tpl .Values.global.clusterDomain . }}"
