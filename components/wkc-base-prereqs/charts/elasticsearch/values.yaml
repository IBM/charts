global:
  chownPV: false
  dockerRegistryPrefix: ""
  persistence:
    useDynamicProvisioning: false 
    storageClassName:
  serviceabilityLabelName: wkc
  serviceabilityLabelValue: wkc
  installCerts: false
  ibmProduct: ""

clusterName: "elasticsearch"
nodeGroup: "master"

# The service that non master groups will try to connect to when joining the cluster
# This should be set to clusterName + "-" + nodeGroup for your master group
masterService: ""

# Elasticsearch roles that will be applied to this nodeGroup
# These will be set as environment variables. E.g. node.master=true
roles:
  master: "true"
  ingest: "true"
  data: "true"

replicas: 3


# Allows you to add any config files in /usr/share/elasticsearch/config/
# such as elasticsearch.yml and log4j2.properties
esConfig: 
  elasticsearch.yml: |
    
    path.repo: ["/usr/share/elasticsearch/backups"]
    cluster.routing.allocation.disk.watermark.low: "2gb"
    cluster.routing.allocation.disk.watermark.high: "1gb"
    cluster.routing.allocation.disk.watermark.flood_stage: "500mb"
#    key:
#      nestedkey: value
#  log4j2.properties: |
#    key = value

# Extra environment variables to append to this nodeGroup
# This will be appended to the current 'env:' key. You can use any of the kubernetes env
# syntax here
extraEnvs:


username: elastic

# A list of secrets and their paths to mount inside the pod
# This is useful for mounting certificates for security and for mounting
# the X-Pack license
secretMounts: []

securityContext:
  runAsUser: 

# if chown is true, an init-container with sudo is launched to
# change the owner of the persistence volume mount folder to the user defined in the
# security context  
initContainer:
  chown: false
  initImage:
    repository: wkc-init-container
    tag: 1.0.50
    pullPolicy: IfNotPresent
  resources: 
    limits:
      cpu: 100m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi
  securityContext:
    runAsUser: 
    
image: 
  repository: elasticsearch
  tag: "7.5.2-45"
imagePullPolicy: "IfNotPresent"

podAnnotations:
  productName: "IBM Common Core Services for IBM Cloud Pak for Data"
  productID: "shared infrastructure"
  productVersion: "3.0"

esJavaOpts: "-Xmx1g -Xms1g"

resources:
  requests:
    cpu: "100m"
    memory: "1Gi"
  limits:
    cpu: "1000m"
    memory: "2Gi"

existingSecret: 

# job for generating secrets.. only runs is existingSecret=false
secretGeneration:
  serviceAccountName:
  image:
    repository: wkc-init-container
    tag: 1.0.59
    imagePullSecrets: ""
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi  
      
networkHost: "0.0.0.0"

# network publish host, set to 127.0.0.1 for supporting haproxy TLS. else comment out.
networkPublishHost: "127.0.0.1"

backups:
  enabled: true
  persistence:
    accessModes: 
    - ReadWriteMany
    size: 30Gi
    storageClass: "wdp-elasticsearch-backups"
    
persistence:
  enabled: false
  accessModes: 
    - ReadWriteOnce
  size: 30Gi
  storageClass: "wdp-elasticsearch"
  overrideStorageClass: false
  annotations: {}

extraVolumes: []
  # - name: extras
  #   emptyDir: {}

extraVolumeMounts: []
  # - name: extras
  #   mountPath: /usr/share/extras
  #   readOnly: true

# This is the PriorityClass settings as defined in
# https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
priorityClassName: ""

# By default this will make sure two pods don't end up on the same node
# Changing this to a region would allow you to spread pods across regions
antiAffinityTopologyKey: "kubernetes.io/hostname"

# Hard means that by default pods will only be scheduled if there are enough nodes for them
# and that they will never end up on the same node. Setting this to soft will do this "best effort"
antiAffinity: "hard"

# The default is to deploy all pods serially. By setting this to parallel all pods are started at
# the same time when bootstrapping the cluster
podManagementPolicy: "Parallel"

protocol: https
httpPort: 19200
transportPort: 19300



service:
  type: ClusterIP
  nodePort:
  annotations: {}

updateStrategy: RollingUpdate

# This is the max unavailable setting for the pod disruption budget
# The default value of 1 will make sure that kubernetes won't allow more than 1
# of your pods to be unavailable during maintenance
maxUnavailable: 1

 # GroupID for the elasticsearch user. The official elastic docker images always have the id of 1000
fsGroup: 1000


serviceAccount:
  name: 

# How long to wait for elasticsearch to stop gracefully
terminationGracePeriod: 30

setSysctls: false
sysctlVmMaxMapCount: 262144
setSysctlsUsingJob: false
sysCtlsJobContainer:
  initImage:
    repository: wkc-init-container
    tag: 1.0.59
    pullPolicy: IfNotPresent
  resources: 
    limits:
      cpu: 100m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi
  securityContext:
    runAsUser: 
  serviceAccount: 
    name: 
    
readinessProbe:
  failureThreshold: 3
  initialDelaySeconds: 45
  periodSeconds: 10
  successThreshold: 3
  timeoutSeconds: 5

# https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-health.html#request-params wait_for_status
clusterHealthCheckParams: "wait_for_status=green&timeout=1s"

## Use an alternate scheduler.
## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
##
schedulerName: ""

imagePullSecrets: 
nodeSelector: {}
tolerations: []

# Enabling this will publically expose your Elasticsearch instance.
# Only enable this if you have security enabled on your cluster
ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  path: /
  hosts:
    - chart-example.local
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

nameOverride: "wdp-elasticsearch"
fullnameOverride: "wdp-elasticsearch"


# haproxy - reverse proxy TLS/auth sidecar
useSSLProxy: true

# haproxy port
proxyHttpPort: 9200
proxyTransportPort: 9300

haproxy:
  image:
    repository: haproxy
    tag: 1.9-17
    pullPolicy: IfNotPresent
  securityContext:
    runAsUser: 1000 
  resources:
    requests:
      memory: 256Mi
      cpu: 200m
    limits:
      memory: 1Gi
      cpu: 1



## Kubernetes Cluster Domain
clusterDomain: cluster.local

## Add arch value
archx86_64: amd64
archppc64le: ppc64le

nodeLabel: wkc-privileged-node
