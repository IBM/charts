---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ template "uname" . }}
  labels:
    heritage: {{ .Release.Service | quote }}
    release: {{ .Release.Name | quote }}
    chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
    app: "{{ template "uname" . }}"
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/managed-by: {{ .Release.Service }} 
    app.kubernetes.io/name: {{ .Chart.Name }}
    helm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version | replace "+" "_" }}
spec:
  serviceName: {{ template "uname" . }}-headless
  selector:
    matchLabels:
      app: "{{ template "uname" . }}"
      chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
  replicas: {{ default .Values.replicas }}
  podManagementPolicy: {{ .Values.podManagementPolicy }}
  updateStrategy:
    type: {{ .Values.updateStrategy }}
  {{- if .Values.persistence.enabled }}
  volumeClaimTemplates:
  - metadata:
      name: {{ template "uname" . }}
    {{- with .Values.persistence.annotations  }}
      annotations:
    {{ toYaml . | indent 4 }}
    {{- end }}
    spec:
      accessModes:
      {{- range .Values.persistence.accessModes }}
        - {{ . | quote }}
      {{- end }}
      resources:
        requests:
          storage: {{ .Values.persistence.size | quote }}
      {{- if .Values.persistence.storageClass }}
      {{- if (eq "-" .Values.persistence.storageClass) }}
      storageClassName: ""
      {{- else }}
      {{- if and .Values.global.persistence.useDynamicProvisioning (not .Values.persistence.overrideStorageClass) }}
      storageClassName: {{ default nil .Values.global.persistence.storageClassName | quote }}
      {{- else }}
      storageClassName: "{{ .Values.persistence.storageClass }}"
      {{- end }}  
      {{- end }}
      {{- end }}
  {{- end }}
  template:
    metadata:
      name: "{{ template "uname" . }}"
      labels:
        heritage: {{ .Release.Service | quote }}
        release: {{ .Release.Name | quote }}
        chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
        app: "{{ template "uname" . }}"
        icpdsupport/app: "apis"
        {{ .Values.global.serviceabilityLabelName }}: {{ .Values.global.serviceabilityLabelValue }}
        app.kubernetes.io/instance: {{ .Release.Name }}
        app.kubernetes.io/managed-by: {{ .Release.Service }} 
        app.kubernetes.io/name: {{ .Chart.Name }}
        helm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version | replace "+" "_" }}
      annotations:
        {{- range $key, $value := .Values.podAnnotations }}
        {{ $key }}: {{ $value | quote }}
        {{- end }}
        {{/* This forces a restart if the configmap has changed */}}
        {{- if .Values.esConfig }}
        configchecksum: {{ include (print .Template.BasePath "/configmap.yaml") . | sha256sum | trunc 63 }}
        {{- end }}
    spec:
      {{- if .Values.schedulerName }}
      schedulerName: "{{ .Values.schedulerName }}"
      {{- end }}
      hostNetwork: false
      hostPID: false
      hostIPC: false     
      {{- with .Values.tolerations }}
      tolerations:
{{ toYaml . | indent 6 }}
      {{- end }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
{{ toYaml . | indent 8 }}
      {{- end }}
      {{- if .Values.priorityClassName }}
      priorityClassName: {{ .Values.priorityClassName }}
      {{- end }}
      affinity:
      {{- if eq .Values.antiAffinity "hard" }}
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - "{{ template "uname" .}}"
            topologyKey: {{ .Values.antiAffinityTopologyKey }}
      {{- else if eq .Values.antiAffinity "soft" }}
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: {{ .Values.antiAffinityTopologyKey }}
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - "{{ template "uname" . }}"
      {{- end }}
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 2
            preference:
              matchExpressions:
              - key: wkc-privileged
                operator: In
                values:
                - {{ .Values.nodeLabel }}
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - {{ .Values.archx86_64 }}
                - {{ .Values.archppc64le }}
              
      terminationGracePeriodSeconds: {{ .Values.terminationGracePeriod }}
      volumes:
      {{- if eq .Values.protocol "https" }}
        - name: {{ template "uname" . }}-cert
          secret:
            secretName: {{ template "uname" . }}-cert
      {{- end }}
        {{- range .Values.secretMounts }}
        - name: {{ .name }}
          secret:
            secretName: {{ .name }}
        {{- end }}
        {{- if .Values.esConfig }}
        - name: esconfig
          configMap:
            name: {{ template "uname" . }}-config
        {{- end }}
 {{- if .Values.setSysctlsUsingJob }}        
        - name: elastic-search-sysctl-config
          configMap:
            name: elastic-search-sysctl-config
            defaultMode: 0555
{{- end }}            
  {{- if not .Values.persistence.enabled }}
        - name: "{{ template "uname" . }}"
          emptyDir: {}
  {{- end }}
        - name: backups-pv
      {{- if .Values.persistence.enabled }}        
          persistentVolumeClaim:
            claimName: {{ template "uname" . }}-backups
      {{- else }}
          emptyDir: {}    
      {{- end }}          
        - name: haproxy-config-volume
          configMap:
            name: {{ template "uname" . }}-haproxy
        - name: shared-socket
          emptyDir: {}      
      {{- if .Values.extraVolumes }}
{{ tpl .Values.extraVolumes . | indent 6 }}
      {{- end }}
      {{- if .Values.imagePullSecrets }}
      imagePullSecrets:
        - name: {{ .Values.imagePullSecrets | quote }}
      {{- end }}     
      serviceAccountName: {{ default "default" .Values.serviceAccount.name }}
      securityContext:
        runAsNonRoot: true
      {{- if .Values.fsGroup }}        
        fsGroup: {{ .Values.fsGroup }}
      {{- end }}  
      {{- if .Values.securityContext.runAsUser }}
        runAsUser: {{ .Values.securityContext.runAsUser }}
      {{- end }}    
      {{- if .Values.setSysctls }}        
        sysctls:
        - name: vm.max_map_count
          value: "{{ .Values.sysctlVmMaxMapCount }}"
      {{- end }}       
      initContainers:
{{- if .Values.setSysctlsUsingJob }}       
        - name: init-sysctls
          image: {{ if .Values.global.dockerRegistryPrefix }}{{ trimSuffix "/" .Values.global.dockerRegistryPrefix }}/{{ end }}{{ .Values.initContainer.initImage.repository}}:{{ .Values.initContainer.initImage.tag }}
          imagePullPolicy: {{ .Values.initContainer.initImage.pullPolicy }}
          command: ["/bin/sh", "/job/runJob.sh"]
          securityContext:
            privileged: false
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            capabilities:
              drop:
              - ALL
          resources:
{{ toYaml .Values.initContainer.resources | indent 13 }}
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          volumeMounts:
          - name: elastic-search-sysctl-config
            mountPath: /job
{{- end }}                
{{- if or .Values.initContainer.chown .Values.global.chownPV }}            
        - name: init-chown-pv
          image: {{ if .Values.global.dockerRegistryPrefix }}{{ trimSuffix "/" .Values.global.dockerRegistryPrefix }}/{{ end }}{{ .Values.initContainer.initImage.repository}}:{{ .Values.initContainer.initImage.tag }}
          imagePullPolicy: {{ .Values.initContainer.initImage.pullPolicy }}
          command: ['sh']
          args:
          - "-c"
          - |
            set -ex;
{{- if .Values.securityContext.runAsUser }}            
            sudo chown {{ .Values.securityContext.runAsUser }}:{{ .Values.securityContext.runAsUser }} /usr/share/elasticsearch/data;
            sudo chmod 755 /usr/share/elasticsearch/data;
            sudo chown {{ .Values.securityContext.runAsUser }}:{{ .Values.securityContext.runAsUser }} /usr/share/elasticsearch/backups;
            sudo chmod 755 /usr/share/elasticsearch/backups;
{{- end }}          
          securityContext:
            privileged: false
            runAsNonRoot: true
            capabilities:
              add:
              - SETUID
              - SETGID
              drop:
              - ALL            
          {{- if .Values.initContainer.securityContext.runAsUser }}
            runAsUser: {{ .Values.initContainer.securityContext.runAsUser }}
          {{- end }}  
          resources:
{{ toYaml .Values.initContainer.resources | indent 13 }}
          volumeMounts:
            - name: "{{ template "uname" . }}"
              mountPath: /usr/share/elasticsearch/data
            - name: backups-pv
              mountPath: /usr/share/elasticsearch/backups
              subPath: backups  
{{- end }}                
      containers:
      - name: "{{ template "name" . }}"
        image: {{ if .Values.global.dockerRegistryPrefix }}{{ trimSuffix "/" .Values.global.dockerRegistryPrefix }}/{{ end }}{{ .Values.image.repository }}:{{ .Values.image.tag }}
        imagePullPolicy: "{{ .Values.imagePullPolicy }}"   
        {{- if .Values.useSSLProxy }}
        command: ['sh']
        args:
        - "-c"
        - |
          set -e;
          
          publishPort="970${HOSTNAME:21}"
          echo "publish port: ${publishPort}"
          
          env transport.publish_port=${publishPort} /usr/local/bin/docker-entrypoint.sh;      
        {{- end }}               
        readinessProbe:
{{ toYaml .Values.readinessProbe | indent 10 }}
          exec:
            command:
              - sh
              - -c
              - |
                #!/usr/bin/env bash -e
                # If the node is starting up wait for the cluster to be ready (request params: '{{ .Values.clusterHealthCheckParams }}' )
                # Once it has started only check that the node itself is responding
                START_FILE=/tmp/.es_start_file

                http () {
                    local path="${1}"
                    
                    curl -XGET -s -k --fail http://127.0.0.1:{{ .Values.httpPort }}${path}
                }

                if [ -f "${START_FILE}" ]; then
                    echo 'Elasticsearch is already running, lets check the node is healthy'
                    http "/"
                else
                    echo 'Waiting for elasticsearch cluster to become cluster to be ready (request params: "{{ .Values.clusterHealthCheckParams }}" )'
                    if http "/_cluster/health?{{ .Values.clusterHealthCheckParams }}" ; then
                        touch ${START_FILE}
                        exit 0
                    else
                        echo 'Cluster is not yet ready (request params: "{{ .Values.clusterHealthCheckParams }}" )'
                        exit 1
                    fi
                fi
        ports:
        - name: http
          containerPort: {{ .Values.httpPort }}
        - name: transport
          containerPort: {{ .Values.transportPort }}
        resources:
{{ toYaml .Values.resources | indent 10 }}
        env:
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          {{- if eq .Values.roles.master "true" }}
          - name: cluster.initial_master_nodes
          {{- if .Values.useSSLProxy }}
            value: "{{ template "haproxyEndpoints" .Values }}"
          {{- else }}
            value: "{{ template "endpoints" .Values }}"
          {{- end }}
          {{- end }}
          - name: discovery.seed_hosts
          {{- if .Values.useSSLProxy }}
            value: "{{ template "haproxyEndpoints" .Values }}"
          {{- else }}
            value: "elasticsearch-master-headless"
          {{- end }}
          - name: cluster.name
            value: "{{ .Values.clusterName }}"
          - name: network.host
            value: "{{ .Values.networkHost }}"
          - name: network.publish_host
            value: "{{ .Values.networkPublishHost }}"
          - name: ES_JAVA_OPTS
            value: "{{ .Values.esJavaOpts }}"
          - name: transport.tcp.port
            value: "{{ .Values.transportPort }}"
          - name: http.port
            value: "{{ .Values.httpPort }}"
          
          {{- range $role, $enabled := .Values.roles }}
          - name: node.{{ $role }}
            value: "{{ $enabled }}"
          {{- end }}          
        
{{- if .Values.extraEnvs }}
{{ toYaml .Values.extraEnvs | indent 10 }}
{{- end }}
        securityContext:
          privileged: false
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false
          runAsNonRoot: true
          capabilities:
            drop:
            - ALL
        volumeMounts:
          - name: "{{ template "uname" . }}"
            mountPath: /usr/share/elasticsearch/data
          - name: backups-pv
            mountPath: /usr/share/elasticsearch/backups
            subPath: backups            
          {{- if eq .Values.protocol "https" }}
          - name: {{ template "uname" . }}-cert         
            mountPath: /usr/share/elasticsearch/config/certs
          {{- end }}
          {{- range .Values.secretMounts }}
          - name: {{ .name }}
            mountPath: {{ .path }}
            {{- if .subPath }}
            subPath: {{ .subPath }}
            {{- end }}
          {{- end }}
          {{- range $path, $config := .Values.esConfig }}
          - name: esconfig
            mountPath: /usr/share/elasticsearch/config/{{ $path }}
            subPath: {{ $path }}
          {{- end -}}
        {{- if .Values.extraVolumeMounts }}
{{ tpl .Values.extraVolumeMounts . | indent 10 }}
        {{- end }}
      {{- if eq .Values.roles.master "true" }}
      # This sidecar will prevent slow master re-election
      # https://github.com/elastic/helm-charts/issues/63
      - name: elasticsearch-master-graceful-termination-handler
        image: {{ if .Values.global.dockerRegistryPrefix }}{{ trimSuffix "/" .Values.global.dockerRegistryPrefix }}/{{ end }}{{ .Values.image.repository }}:{{ .Values.image.tag }}
        imagePullPolicy: "{{ .Values.imagePullPolicy }}"
        command:
        - "sh"
        - -c
        - |
          #!/usr/bin/env bash
          set -eo pipefail

          http () {
              local path="${1}"
              if [ -n "${ELASTIC_USERNAME}" ] && [ -n "${ELASTIC_PASSWORD}" ]; then
                BASIC_AUTH="-u ${ELASTIC_USERNAME}:${ELASTIC_PASSWORD}"
              else
                BASIC_AUTH=''
              fi
              curl -XGET -s -k --fail ${BASIC_AUTH} {{ .Values.protocol }}://{{ template "masterService" . }}:{{ if .Values.useSSLProxy }}{{ .Values.proxyHttpPort }}{{ else }}{{ .Values.httpPort }}{{ end }}${path}
          }

          cleanup () {
            while true ; do
              local master="$(http "/_cat/master?h=node" || echo "")"
              if [[ $master == "{{ template "masterService" . }}"* && $master != "${NODE_NAME}" ]]; then
                echo "This node is not master."
                break
              fi
              echo "This node is still master, waiting gracefully for it to step down"
              sleep 1
            done

            exit 0
          }

          trap cleanup SIGTERM

          sleep infinity &
          wait $!
        securityContext:
          privileged: false
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false
          runAsNonRoot: true
          capabilities:
            drop:
            - ALL
        resources:
{{ toYaml .Values.initContainer.resources | indent 10 }}
        env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          # auth is provided by the haproxy sidecar...      
          {{- if .Values.useSSLProxy }}      
          - name: ELASTIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: {{ template "uname" . }}-secret
                key: password
          - name: ELASTIC_USERNAME
            valueFrom:
              secretKeyRef:
                name: {{ template "uname" . }}-secret
                key: username
          {{- end }}
        {{- if .Values.extraEnvs }}
{{ toYaml .Values.extraEnvs | indent 10 }}
        {{- end }}
      {{- end }}
      
      # This sidecar will enable https proxy tls termination
      {{- if .Values.useSSLProxy }}
      - name: haproxy
        image: {{ if .Values.global.dockerRegistryPrefix }}{{ trimSuffix "/" .Values.global.dockerRegistryPrefix }}/{{ end }}{{ .Values.haproxy.image.repository }}:{{ .Values.haproxy.image.tag }}
        imagePullPolicy: {{ .Values.haproxy.image.pullPolicy }}        
        ports:
        - name: https
          containerPort: {{ .Values.proxyHttpPort }}
        - name: transport
          containerPort: {{ .Values.proxyTransportPort }}
        securityContext:
          privileged: false
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false
          runAsNonRoot: true
          {{- if .Values.haproxy.securityContext.runAsUser }}
          runAsUser: {{ .Values.haproxy.securityContext.runAsUser }}
          {{- end }}
          capabilities:
            drop:
            - ALL
        volumeMounts:
        - name: haproxy-config-volume
          mountPath: /usr/local/etc/haproxy
        - name: shared-socket
          mountPath: /run/haproxy
        - name: {{ template "uname" . }}-cert 
          mountPath: /etc/ssl/certs
        resources:
{{ toYaml .Values.haproxy.resources | indent 10 }}
        env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: ELASTIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: {{ template "uname" . }}-secret
                key: password
          - name: ELASTIC_USERNAME
            valueFrom:
              secretKeyRef:
                name: {{ template "uname" . }}-secret
                key: username
      {{- end }}
