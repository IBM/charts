apiVersion: apps/v1
kind: StatefulSet
metadata:
  # Unique key of the Deployment instance
  name: {{ .Values.servicename }}-tenant-zk
  labels:
    app.kubernetes.io/name: {{ template "eventstore.fullname" . }}
    helm.sh/chart: "{{ .Chart.Name }}"
    release: "{{ .Release.Name }}"
    app.kubernetes.io/instance: "{{ .Release.Name }}"
    app.kubernetes.io/managed-by: "{{ .Release.Service }}"
    component: eventstore
    {{- include "eventstore.podLabels" . | indent 4 }}
spec:
  # 1 Pods should exist at all times.
  replicas: {{ .Values.deployment.controlNodes }}
  serviceName: {{ .Values.servicename }}-tenant-zk-svc
  podManagementPolicy: "Parallel"
  selector:
     matchLabels:
       app.kubernetes.io/name: {{ .Values.servicename }}-tenant-zk
  template:
    metadata:
      labels:
        # Apply this label to pods and default
        # the Deployment label selector to this value
        app.kubernetes.io/name: {{ .Values.servicename }}-tenant-zk
        helm.sh/chart: "{{ .Chart.Name }}"
        name: {{ template "eventstore.name" . }}
        release: "{{ .Release.Name }}"
        app.kubernetes.io/instance: "{{ .Release.Name }}"
        app.kubernetes.io/managed-by: "{{ .Release.Service }}"
        component: eventstore
        enabled: "true"
        {{- include "eventstore.podLabels" . | indent 8 }}
      annotations:
        {{- include "eventstore.annotations" . | indent 8 }}
    spec:
      affinity:
        {{- include "eventstore.nodeAffinityControl" . | indent 6 }}
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "app.kubernetes.io/name"
                    operator: In
                    values: 
                    - {{ .Values.servicename }}-tenant-zk
              topologyKey: "kubernetes.io/hostname"
      {{- include "eventstore.tolerations" . | indent 6 }}
      {{- include "eventstore.security" . | indent 6 }}
      serviceAccountName: {{ default "default" .Values.serviceAccountName }}
      {{- include "eventstore.podSecurityContext" . | indent 6 }}
      # ZK needs to wait for the sqllib-shared job because it sets the zk connection string
      initContainers:
      {{- include "eventstore.wait-sqllib-shared" . | indent 6 }}
      containers:
      - name: eventstore-tenant-zookeeper
        {{- if .Values.eventstoreZookeeper.image.tag }}
        image: {{ .Values.eventstoreZookeeper.image.repository }}:{{ .Values.eventstoreZookeeper.image.tag }}
        {{- else }}
        image: {{ .Values.eventstoreZookeeper.image.repository }}:{{ .Values.image.universalTag }}
        {{- end }}
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        {{- include "eventstore.securityContext" . | indent 8 }}
        ports:
        - containerPort: 2181
          name: client
        - containerPort: 2888
          name: server
        - containerPort: 3888
          name: leader-election
        livenessProbe:
          exec:
            command:
              - bash
              - -c
              - "/opt/zookeeper/bin/zkOk.sh"
          initialDelaySeconds: 15
          periodSeconds: 20
          timeoutSeconds: {{ .Values.eventstoreZookeeper.livenessProbeTimeout }}
        readinessProbe:
          exec:
            command:
              - bash
              - -c
              - "/opt/zookeeper/bin/zkOk.sh --readiness"
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: {{ .Values.eventstoreZookeeper.readinessProbeTimeout }}
        env:
        - name: RUNTIME_CONTEXT
          value: {{ .Values.runtime }}
        # EventStore allows user to define flexible number of zookeeper replicas at install time. The defined replica count
        # is recorded in this env var of the statefulset definition to facilitate scale up/down.
        - name: MEMBER_COUNT
          value: "{{ .Values.deployment.controlNodes }}"
        volumeMounts:
        - mountPath: /var/zookeeper/logs
          name: zookeeper-logs
        - mountPath: /var/zookeeper/data
          name: zookeeper-data
        - mountPath: /var/zookeeper/datalog
          name: zookeeper-datalog
        - mountPath: /opt/zookeeper/conf/external_conf
          name: zookeeper-config-volume
        command: ['bash']
        args: ['-c','/home/db2inst1/base_entrypoint.sh /opt/zookeeper/bin/zkEntryPoint.sh']
      # Restart policy for all containers within the pod
      # One of Always, OnFailure, Never. Default to Always.
      # From env var ${ZOOKEEPER_RESTART_POLICY}
      restartPolicy: Always
      volumes:
      - name: zookeeper-logs
        # HostPath represents a pre-existing file or directory on the host machine that is directly exposed to the container.
        # This is generally used for system agents or other privileged things that are allowed to see the host machine.
        # Most containers will NOT need this.
        hostPath:
          path: {{ tpl .Values.disk.zookeeperPath . }}/{{ .Values.servicename }}/zookeeper/server.0/log
      - name: zookeeper-data
        hostPath:
          path: {{ tpl .Values.disk.zookeeperPath . }}/{{ .Values.servicename }}/zookeeper/server.0/data
      - name: zookeeper-datalog
        hostPath:
          path: {{ tpl .Values.disk.zookeeperPath . }}/{{ .Values.servicename }}/zookeeper/server.0/datalog
      - name: zookeeper-config-volume
        configMap:
          name: "{{ .Values.servicename }}-config-files"
          items:
          - key: zoo-cfg
            path: zoo.cfg
