apiVersion: v1
data:
  log4j.properties: |
    log4j.rootLogger=INFO,all,STDOUT
    log4j.logger.cluster=INFO
    log4j.logger.security=INFO,securityAppender,STDOUT
    log4j.additivity.security=false
    log4j.logger.org.apache.kafka=ERROR,all
    log4j.logger.org.apache.spark=ERROR,all
    log4j.logger.org.apache.parquet=ERROR,all
    log4j.logger.org.apache.hadoop=ERROR,all
    log4j.logger.com.ibm.analytics.ngp.repository_v3=WARN,all
    log4j.logger.com.ibm.analytics.ngp.util=WARN,all

    log4j.appender.all=org.apache.log4j.RollingFileAppender
    log4j.appender.all.File=/opt/ibm/wml-online-scoring/runtimes/spark-2.1/logs/wml-online-scoring-sparkRuntime.log
    log4j.appender.all.DatePattern=.yyyy-MM-dd
    log4j.appender.all.MaxFileSize=5MB
    log4j.appender.all.MaxBackupIndex=5
    log4j.appender.all.layout=org.apache.log4j.PatternLayout
    log4j.appender.all.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss,SSS}|%t|%-5p|%l|%X{Request-ID}|%m%n
    log4j.logger.com.ibm.ml.scoring.online.ScoringServer=TRACE
    log4j.logger.akka=TRACE

    log4j.appender.STDOUT=org.apache.log4j.ConsoleAppender
    log4j.appender.STDOUT.layout=org.apache.log4j.PatternLayout
    log4j.appender.STDOUT.Target=System.out
    log4j.appender.STDOUT.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss,SSS}|%t|%-5p|%l|%X{Request-ID}|%m%n

    log4j.appender.securityAppender=org.apache.log4j.DailyRollingFileAppender
    log4j.appender.securityAppender.File=/opt/ibm/wml-online-scoring/runtimes/spark-2.1/logs/security-online-scoring-sparkRuntime.log
    log4j.appender.securityAppender.layout=org.apache.log4j.PatternLayout
    log4j.appender.securityAppender.MaxFileSize=5MB
    log4j.appender.securityAppender.MaxBackupIndex=2
    log4j.appender.securityAppender.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss,SSS}|%t|%-5p|%l|%X{Request-ID}|%m%n

    #log4j.appender.cluster-events-appender=org.apache.log4j.DailyRollingFileAppender
    #log4j.appender.cluster-events-appender.File=/temp/scoring-cluster-events.log
    #log4j.appender.cluster-events-appender.DatePattern=.yyyy-MM-dd
    #log4j.appender.cluster-events-appender.layout=org.apache.log4j.PatternLayout
    #log4j.appender.cluster-events-appender.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss,SSS}|%-5p|%l|%X{Request-ID}|%m%n
  scoringService.conf: |+
    akka {
      loggers = ["akka.event.slf4j.Slf4jLogger"]
      loglevel = "WARNING"

      log-dead-letters = 10
      log-dead-letters-during-shutdown = on
      # log-config-on-start = on
      http.server.max-connections = 6144
      http.server.request-timeout = 120 s
      http.server.idle-timeout = 121 s
      http.server.parsing.max-content-length = 100m

      actor {
        default-dispatcher {
          default-executor {
            fallback: "fork-join-executor"
          }
          executor = "fork-join-executor"
          fork-join-executor {
            parallelism-min = 4
            throughput = 1
          }
        }
      }

      online-scoring-runtime-spark {
        type = "Dispatcher"
        executor = "thread-pool-executor"
        thread-pool-executor {
          core-pool-size-min = 12
          core-pool-size-max = 12
          max-pool-size-min = 24
          max-pool-size-max = 128
          task-queue-size = 24
        }
        throughput = 1
      }

      http.host-connection-pool {
        # The maximum number of open requests accepted into the pool across all
        # materializations of any of its client flows.
        # Protects against (accidentally) overloading a single pool with too many client flow materializations.
        # Note that with N concurrent materializations the max number of open request in the pool
        # will never exceed N * max-connections * pipelining-limit.
        # Must be a power of 2 and > 0!
        max-open-requests = 256
        max-connections = 64
      }

      //  http.service.interface = "localhost"
      http.service.port = 16500

      ssl {
        port = 16500
        keystore {
          path = "/opt/ibm/wml-online-scoring/runtimes/spark-2.1/keys/private/keystore.jks"
          type = "JKS"
        }
        truststore {
          # working directory for testing expect to be 'ml-online-scoring' dir
          path = "/opt/ibm/wml-online-scoring/runtimes/spark-2.1/keys/private/trust-store.jks"
          type = "JKS"
          trustStrategy.trustSelfSignedStrategy = true
        }
      }
    }

    grpc {
      service.interface = "wml-scoring-rt-utils"
      service.port = 16501
      service.agent_port = 16502
    }

    service {

      # Config for Validate Token connection
      validateTokenService {
        #public key may be provided via file or url
        ml.pubkey.path = "/opt/ibm/wml-online-scoring/runtimes/spark-2.1/keys/mltoken.pub"
        icp.pubkey.path = "/user-home/_global_/config/jwt/public.pem"
      }

      wmlService {
        url = "https://internal-nginx-svc:12443"
        timeout = 60
      }

      features {
        list = ["SPACES"]
      }

      space_access {
         post = ["Admin", "Editor", "Viewer"]
      }

      space_cache_refresh {
        interval = 10
      }

      ml-event-client {
        topicSpace = "misc"
        messageProcessParallelism = 20
        limitPlanEventsTopicName = "wml.api.instances"
      }

      ml-kafka-client.credentials.path = "/opt/ibm/wml-kafka/kafka-credentials-es.json"

      iam {
        url = "dummy"
        pdp {
          serviceName = "pm-20-devops"
          apiKey = ""
        }
      }

    //  wml_instances {
    //    url = "https://wml-fvt.ml.test.cloud.ibm.com"
    //  }

      resiliency {
        delay = 500  // In millisecodns
        retries = 3
      }
    }

    common {
      enable-ssl = true
      memory {
        threshold = 80.0
      }
      cacerts {
        cert = "/opt/ibm/wml-online-scoring/runtimes/spark-2.1/cacerts/server.crt"
        key = "/opt/ibm/wml-online-scoring/runtimes/spark-2.1/cacerts/server.key"
      }
    }
    // make a list of all your custom transformers
    // the list contains the fully-qualified class names of the
    // OpNode implementations for wml transformers
    com.ibm.analitics.wml.mleap.ops = [
      "com.ibm.analytics.wml.features.mleap.ops.mleap.ADPOp",
      "com.ibm.analytics.wml.features.mleap.ops.mleap.DecoderHolderOp"
    ]

    // override the default MLeap registry to
    // include the builtin ops as well as
    // the custom transformers we have defined
    ml.combust.mleap.registry.default.ops += "com.ibm.analitics.wml.mleap.ops"


    com.ibm.analitics.wml.mleap.spark.ops = [
      "com.ibm.analytics.wml.features.mleap.ops.spark.ADPOp",
      "com.ibm.analytics.wml.features.mleap.ops.spark.DecoderHolderOp"
    ]

    // override the default Spark registry to
    // include the Spark base ops (compatible with 2.0, 2.1, 2.2 and 2.3)
    // as well as the default Spark ops
    // and finally our custom transformer ops
    ml.combust.mleap.spark.registry.v20.ops += "com.ibm.analitics.wml.mleap.spark.ops"
    ml.combust.mleap.spark.registry.v21.ops += "com.ibm.analitics.wml.mleap.spark.ops"
    ml.combust.mleap.spark.registry.v22.ops += "com.ibm.analitics.wml.mleap.spark.ops"
    ml.combust.mleap.spark.registry.v23.ops += "com.ibm.analitics.wml.mleap.spark.ops"


kind: ConfigMap
metadata:
  name: wmlscoringmllib
  labels:
    app: wml-scoring
    chart: "{{ .Chart.Name }}"
    release: "{{ .Release.Name }}"
    heritage: "{{ .Release.Service }}"
    app.kubernetes.io/managed-by: {{.Release.Service | quote }}
    app.kubernetes.io/instance: {{.Release.Name | quote }}
    app.kubernetes.io/name: "{{ .Release.Name }}"
    helm.sh/chart: "{{.Chart.Name}}-{{.Chart.Version}}"

