###############################################################################
#
# Licensed Materials - Property of IBM
#
# 5737-H33
#
# (C) Copyright IBM Corp. 2019  All Rights Reserved.
#
# US Government Users Restricted Rights - Use, duplication or
# disclosure restricted by GSA ADP Schedule Contract with IBM Corp.
#
###############################################################################
# Runs a job to create or patch the release configmap on upgrade
###############################################################################
{{ if eq .Values.license "accept" -}}
{{- include "sch.config.init" (list . "sch.chart.config.values") -}}
{{- include "sch.config.init" (list . "securitycontext.sch.chart.config.values") | trim -}}
# Name of the job
{{ $namePrefix := .sch.chart.components.essential.releasePatcherJob.name -}}
{{ $name := include "sch.names.fullCompName" (list . $namePrefix ) -}}
# Component is essential
{{ $compName := .sch.chart.components.essential.compName -}}
{{ $labels := include "sch.metadata.labels.standard" (list . $compName) -}}
# Name of the new temporary service account
{{ $serviceAccount := .sch.chart.components.essential.releaseServiceAccount.name -}}
{{ $serviceAccountName := include "sch.names.fullCompName" (list . $serviceAccount) -}}
# Name of configmap to create
{{ $releaseConfigMap := .sch.chart.components.essential.releaseConfigMap.name -}}
{{ $releaseConfigMapName := include "sch.names.fullCompName" (list . $releaseConfigMap) -}}
# Name of configmap to create
{{ $proxyConfigMap := .sch.chart.components.proxy.configMap.name -}}
{{ $proxyConfigMapName := include "sch.names.fullCompName" (list . $proxyConfigMap) -}}
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ $name | quote }}
  labels:
{{ $labels | indent 4 }}
  annotations:
    # This needs to run before we upgrade, as kafka sts will read values from the configmap created
    "helm.sh/hook": pre-upgrade
    # We don't delete this if it fails, as the logs may be useful to
    # diagnose a failed install
    "helm.sh/hook-delete-policy": hook-succeeded, before-hook-creation
    # This needs to happen after relevant rolebindings have been set to give permission
    # for job to create the configmap
    "helm.sh/hook-weight": "3"
spec:
  backoffLimit: 0
  template:
    metadata:
      name: {{ $name | quote }}
      labels:
{{ $labels | indent 8 }}
        job: {{ $name | quote }}
    spec:
      serviceAccountName: {{ $serviceAccountName | quote }}
      hostNetwork: false
      hostPID: false
      hostIPC: false
      securityContext:
{{- include "sch.security.securityContext" (list . .sch.chart.securitycontexts.pod) | indent 8 }}
      affinity:
{{ include "ibm-eventstreams.customNodeaffinity" (list .) | indent 8 }}
      restartPolicy: Never
      containers:
        - name: {{ $name | quote }}
          image: {{ include "eventstreams.image" (list . "eventstreams-kubectl" .Values.global.image.imageTags.kubectlTag) | quote }}
          imagePullPolicy: {{ .Values.global.image.pullPolicy }}
          env:
            {{- if eq (empty .Values.proxy.externalEndpoint) false}}
              - name: EXTERNAL_ENDPOINT
                value: {{ .Values.proxy.externalEndpoint | quote }}
            {{- end }}
            {{- include "license.accept.env.ref" . | indent 12 }}

          securityContext:
{{- include "sch.security.securityContext" (list . .sch.chart.securitycontexts.containerReadOnlyFilesystem) | indent 12 }}
          command:
            - sh
            - -c
            - 'set -e;

KEYS=();

echo "Adding Kafka versions";
KEYS+=( --from-literal=inter.broker.protocol.version=2.0 \
  --from-literal=log.message.format.version=2.0);
PATCH_KEYS=(\"inter.broker.protocol.version\":\"2.0\",\"log.message.format.version\":\"2.0\",);

echo "Getting CLUSTER_ADDRESS from kube-public";
CLUSTER_ADDRESS=$(kubectl get cm ibmcloud-cluster-info -o jsonpath="{.data.cluster_address}" -n kube-public);
KEYS+=( --from-literal=CLUSTER_ADDRESS="${CLUSTER_ADDRESS}" );
PATCH_KEYS=(${PATCH_KEYS}\"CLUSTER_ADDRESS\":\"${CLUSTER_ADDRESS}\",);

{{- if .Values.proxy.upgradeToRoutes }}
echo "Checking for OpenShift v4 endpoint";
if [[ $CLUSTER_ADDRESS =~ ^icp-console.(apps.*) ]]; then
  KEYS+=( --from-literal=OPENSHIFT_DOMAIN="${BASH_REMATCH[1]}" );
  PATCH_KEYS=(${PATCH_KEYS}\"OPENSHIFT_DOMAIN\":\"${BASH_REMATCH[1]}\",);
fi
{{- end }}

echo "Getting CLUSTER_NAME from kube-public";
CLUSTER_NAME=$(kubectl get cm ibmcloud-cluster-info -o jsonpath="{.data.cluster_name}" -n kube-public);
KEYS+=( --from-literal=CLUSTER_NAME=$CLUSTER_NAME );
PATCH_KEYS=(${PATCH_KEYS}\"CLUSTER_NAME\":\"${CLUSTER_NAME}\",);

echo "Getting CLUSTER_ROUTER_HTTPS_PORT from kube-public";
CLUSTER_ROUTER_HTTPS_PORT=$(kubectl get cm ibmcloud-cluster-info -o jsonpath="{.data.cluster_router_https_port}" -n kube-public);
KEYS+=( --from-literal=CLUSTER_ROUTER_HTTPS_PORT=$CLUSTER_ROUTER_HTTPS_PORT );
PATCH_KEYS=(${PATCH_KEYS}\"CLUSTER_ROUTER_HTTPS_PORT\":\"${CLUSTER_ROUTER_HTTPS_PORT}\",);

echo "Getting PROXY_ADDRESS from kube-public";
PROXY_ADDRESS=$(kubectl get cm ibmcloud-cluster-info -o jsonpath="{.data.proxy_address}" -n kube-public | tr "[:upper:]" "[:lower:])");
KEYS+=( --from-literal=PROXY_ADDRESS=$PROXY_ADDRESS );
PATCH_KEYS=(${PATCH_KEYS}\"PROXY_ADDRESS\":\"${PROXY_ADDRESS}\",);

echo "Getting PROXY_INGRESS_HTTPS_PORT from kube-public";
PROXY_INGRESS_HTTPS_PORT=$(kubectl get cm ibmcloud-cluster-info -o jsonpath="{.data.proxy_ingress_https_port}" -n kube-public);
KEYS+=( --from-literal=PROXY_INGRESS_HTTPS_PORT=$PROXY_INGRESS_HTTPS_PORT );
PATCH_KEYS=(${PATCH_KEYS}\"PROXY_INGRESS_HTTPS_PORT\":\"${PROXY_INGRESS_HTTPS_PORT}\",);

echo "Setting external Endpoint";
if [[ -z $EXTERNAL_ENDPOINT ]]; then
  echo "No external endpoint specifed, getting existing endpoint";
  EXTERNAL_ENDPOINT=$(kubectl get cm $releaseConfigMapName -o jsonpath="{.data.EXTERNAL_ENDPOINT}");
  if [[ -z $EXTERNAL_ENDPOINT ]]; then
    echo "No existing external endpoint found, using proxy address";
    EXTERNAL_ENDPOINT=$PROXY_ADDRESS;
  fi;
fi;
KEYS+=( --from-literal=EXTERNAL_ENDPOINT=$EXTERNAL_ENDPOINT );
PATCH_KEYS=(${PATCH_KEYS}\"EXTERNAL_ENDPOINT\":\"${EXTERNAL_ENDPOINT}\",);


echo "Getting CLUSTER_CA_DOMAIN from kube-public";
CLUSTER_CA_DOMAIN=$(kubectl get cm ibmcloud-cluster-info -n kube-public -o jsonpath="{.data.cluster_ca_domain}");
KEYS+=( --from-literal=CLUSTER_CA_DOMAIN=$CLUSTER_CA_DOMAIN );
PATCH_KEYS=(${PATCH_KEYS}\"CLUSTER_CA_DOMAIN\":\"${CLUSTER_CA_DOMAIN}\");

# remove the last comma - if any - from PATCH_KEYS
PATCH_KEYS=${PATCH_KEYS%,}

CONFIGMAP_PATCH="{\"data\":{${PATCH_KEYS}}}";

CM_CHECKER=$(kubectl get configmap {{ $releaseConfigMapName }} --ignore-not-found -n {{ .Release.Namespace }});

if [[ -z "${CM_CHECKER}" ]]; then
  echo "Config map not found";
  echo "Creating Config Map {{ $releaseConfigMapName }} with keys/values : ${KEYS[@]}";
  echo "Running command : kubectl create configmap {{ $releaseConfigMapName }} -n {{ .Release.Namespace }} ${KEYS[@]}";
  kubectl create configmap {{ $releaseConfigMapName }} -n {{ .Release.Namespace }} ${KEYS[@]} ;
  echo "Labelling Config Map";
  echo "Running command : kubectl label cm {{ $releaseConfigMapName }} -n {{ .Release.Namespace }}
    app=ibm-es chart={{ .Chart.Name }} release={{ .Release.Name }}
    component={{ $compName }} heritage=Tiller";
  kubectl label cm {{ $releaseConfigMapName }} -n {{ .Release.Namespace }}
    app=ibm-es chart={{ .Chart.Name }} release={{ .Release.Name }}
    component={{ $compName }} heritage=Tiller;
else
  echo "Config map already present patching keys";
  kubectl patch configmap {{ $releaseConfigMapName }} -n {{ .Release.Namespace }} -p ${CONFIGMAP_PATCH};
fi'
{{ end -}}
