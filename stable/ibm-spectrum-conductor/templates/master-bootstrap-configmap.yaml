apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-conductor-master-bootstrap
  labels:
    heritage: {{.Release.Service | quote }}
    release: {{.Release.Name | quote }}
    chart: "{{.Chart.Name}}"
    app: {{ template "master-fullname" . }}
data:
  license.dat: |-
      ego_base   3.7   30/06/2019   ()   ()   ()   41ce08b06662163633c9d5b3f55c3c25566f9d24
      conductor_spark   2.3.0   30/06/2019   ()   ()   ()   a000facfb049c21dac9ddb3b26e5dc779aedb38d
      {{- if .Values.dli.enabled }}
      conductor_deep_learning   1.2.0   30/06/2019   ()   ()   ()   03b2f273d8e9e45f9ac8f023fa84c95f730df57e
      {{- end }}
  {{- if eq .Values.cluster.proxyOption "IngressProxy" }}
  startDnsmasq.sh: |-
    dnsmasq -k &
    while [ true ]
    do
        sleep 10
        killall dnsmasq
        dnsmasq -k &
    done

  startnginx.sh: |-
    while [ true ]
    do
      if [ -f /var/share/tls.key ];then
           nginx -c /nginx/nginx.conf
      else
          sleep 10
      fi
    done

  body_filter.lua: |-
    local data = ngx.ctx.buf
    local var  = ngx.var
    local replacementStr = '/'..var[1]..'-'..var[2]..'-'..var[3]..'/'
    if data then
      ngx.ctx.buf = data .. ngx.arg[1]
      ngx.arg[1] = ngx.ctx.buf
    else
      ngx.ctx.buf = ngx.arg[1]
    end
    if ngx.arg[2] then
      ngx.ctx.buf = nil
      local str = ngx.arg[1]
      local pattern = [[(https?)://({{.Release.Name}}-[a-z0-9-]+-[a-z0-9]+-[a-z0-9]+):(\d+)]]
      local newstr, n, err = ngx.re.gsub(str, pattern, 'https://{{ template "master-fullname" . }}:{{ template "proxyHttpsPort" . }}/$1-$2-$3', 'i')
      newstr, n, err = ngx.re.gsub(newstr,'src="/' ,'src="'..replacementStr)
      newstr, n, err = ngx.re.gsub(newstr,'href="/' ,'href="'..replacementStr)
      newstr, n, err = ngx.re.gsub(newstr,'href="app', 'href="'..replacementStr..'app')
      newstr, n, err = ngx.re.gsub(newstr,'action="', 'action="'..replacementStr)
      newstr, n, err = ngx.re.gsub(newstr,'"/api/v1/applications/"', '"'..replacementStr..'api/v1/applications/"')
      newstr, n, err = ngx.re.gsub(newstr,'uiroot}}/history','uiroot}}/'..replacementStr..'/history')
      ngx.arg[1] = newstr
    else
      ngx.arg[1] = nil
    end
  nginx.conf: |-
    daemon off;
    pid /run/nginx.pid;
    worker_rlimit_nofile 7168;
    events {
        multi_accept        on;
        worker_connections  16384;
        use                 epoll;
    }
    http {
        sendfile            on;
        tcp_nopush          on;
        tcp_nodelay         on;
        log_subrequest      on;
        reset_timedout_connection on;
        keepalive_timeout  75s;
        keepalive_requests 100;
        client_header_buffer_size       1k;
        client_header_timeout           60s;
        large_client_header_buffers     4 8k;
        client_body_buffer_size         8k;
        client_body_timeout             60s;
        http2_max_field_size            4k;
        http2_max_header_size           16k;
        types_hash_max_size             2048;
        server_names_hash_max_size      1024;
        server_names_hash_bucket_size   32;
        map_hash_bucket_size            64;
        proxy_headers_hash_max_size     512;
        proxy_headers_hash_bucket_size  64;
        variables_hash_bucket_size      64;
        variables_hash_max_size         2048;
        underscores_in_headers          off;
        ignore_invalid_headers          on;
        resolver 127.0.0.1 valid=30s;
        include /opt/ibm/router/nginx/conf/mime.types;
        default_type text/html;
        gzip off;
        gzip_comp_level 5;
        gzip_http_version 1.1;
        gzip_min_length 256;
        gzip_types application/atom+xml application/javascript application/x-javascript application/json application/rss+xml application/vnd.ms-fontobject application/x-font-ttf application/x-web-app-manifest+json application/xhtml+xml application/xml font/opentype image/svg+xml image/x-icon text/css text/plain text/x-component;
        gzip_proxied any;
        # Custom headers for response
        server_tokens on;
        # disable warnings
        uninitialized_variable_warn off;
        map $request_uri $loggable {
            default 1;
        }
        access_log off;
        error_log  error.log notice;
        # Retain the default nginx handling of requests without a "Connection" header
        map $http_upgrade $connection_upgrade {
            default          upgrade;
            ''               close;
        }
        # trust http_x_forwarded_proto headers correctly indicate ssl offloading
        map $http_x_forwarded_proto $pass_access_scheme {
            default          $http_x_forwarded_proto;
            ''               $scheme;
        }
        map $http_x_forwarded_port $pass_server_port {
           default           $http_x_forwarded_port;
           ''                $server_port;
        }
        map $pass_server_port $pass_port {
            443              443;
            default          $pass_server_port;
        }
        # Map a response error watching the header Content-Type
        map $http_accept $httpAccept {
            default          html;
            application/json json;
            application/xml  xml;
            text/plain       text;
        }
        map $httpAccept $httpReturnType {
            default          text/html;
            json             application/json;
            xml              application/xml;
            text             text/plain;
        }
        # Obtain best http host
        map $http_host $this_host {
            default          $http_host;
            ''               $host;
        }
        map $http_x_forwarded_host $best_http_host {
            default          $http_x_forwarded_host;
            ''               $this_host;
        }
        server_name_in_redirect off;
        port_in_redirect        off;
        ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
        # turn on session caching to drastically improve performance
        ssl_session_cache builtin:1000 shared:SSL:10m;
        ssl_session_timeout 10m;
        # allow configuring ssl session tickets
        ssl_session_tickets on;
        # slightly reduce the time-to-first-byte
        ssl_buffer_size 4k;
        proxy_ssl_session_reuse on;
        server {
            server_name {{ template "master-fullname" . }};
            listen 80;
            listen [::]:80;

            listen {{ template "proxyHttpsPort" . }}  ssl http2;
            listen [::]:{{ template "proxyHttpsPort" . }}  ssl http2;

            ssl_certificate     /var/share/tls.crt;
            ssl_certificate_key /var/share/tls.key;

            location ~* ^/https?-{{ .Release.Name }}-[a-z0-9-]+-\d+(/.*)?$ {
              rewrite ^/(https?)-({{ .Release.Name }}-[a-z0-9-]+)-(\d+)$ / break;
              rewrite ^/(https?)-({{ .Release.Name }}-[a-z0-9-]+)-(\d+)(/.*)?$ $4 break;
              proxy_pass $1://$2:$3;
              proxy_set_header Accept-Encoding "";
              body_filter_by_lua_file /nginx/body_filter.lua;
              proxy_redirect ~^(https?)://({{ .Release.Name }}-[a-z0-9-]+):(\d+)(/.*)$ https://{{ template "master-fullname" . }}:{{ template "proxyHttpsPort" . }}/$1-$2-$3/$4;
            }

            fastcgi_param       HTTP_X_Code             503;
            fastcgi_param       HTTP_X_Format           $http_accept;
            fastcgi_param       HTTP_X_Original_URI     $request_uri;
        }
    }
  {{- end }}
  generate_ssl.sh: |-
    topdir=/opt/ibm/spectrumcomputing
    exec 1<>$topdir/generatessl.log
    exec 2>&1
    set -x
    CLUSTERADMIN=root
    domain=`dnsdomainname`
    if [ "$domain" = "" ]; then
        domain=`hostname -f`
    else
        domain="*.$domain"
    fi
    . $topdir/jre/profile.jre
    dnsname=`hostname -f`
    cd $topdir/security
    rm -f tier2and3ServerKeyStore.jks tier*
    egojre=$topdir/jre
    egokeytool=`find $egojre -name keytool`
    $egokeytool -genkeypair -noprompt -alias tier2alias -dname "CN=$domain,O=IBM,C=CA" -keystore tier2and3ServerKeyStore.jks -storepass SparkPassword -keypass tier2passwd -keyalg rsa -validity 1095 -keysize 2048 -sigalg SHA256withRSA -ext "san=dns:$dnsname"
    $egokeytool -certreq -alias tier2alias -file tier2alias.csr -storepass SparkPassword -keypass tier2passwd -keystore tier2and3ServerKeyStore.jks -ext "san=dns:$dnsname"
    $egokeytool -gencert -infile tier2alias.csr -outfile tier2aliascertcasigned.pem -alias caalias -keystore caKeyStore.jks -storepass Liberty -validity 1095 -ext "san=dns:$dnsname"
    $egokeytool -importcert -noprompt -alias caalias -file cacert.pem -keystore tier2and3ServerKeyStore.jks -storepass SparkPassword
    $egokeytool -import -noprompt -alias tier2alias -file tier2aliascertcasigned.pem -storepass SparkPassword -keypass tier2passwd -keystore tier2and3ServerKeyStore.jks
    $egokeytool -genkeypair -noprompt -alias tier3alias -dname "CN=$domain,O=IBM,C=CA" -keystore tier2and3ServerKeyStore.jks -storepass SparkPassword -keypass tier3passwd -keyalg rsa -validity 1095 -keysize 2048 -sigalg SHA256withRSA -ext "san=dns:$dnsname"
    $egokeytool -certreq -alias tier3alias -file tier3alias.csr -storepass SparkPassword -keypass tier3passwd  -keystore tier2and3ServerKeyStore.jks -ext "san=dns:$dnsname"
    $egokeytool -gencert -infile tier3alias.csr -outfile tier3aliascertcasigned.pem -alias caalias -keystore caKeyStore.jks -storepass Liberty -validity 1095 -ext "san=dns:$dnsname"
    $egokeytool -import -noprompt -alias tier3alias -file tier3aliascertcasigned.pem -storepass SparkPassword -keypass tier3passwd -keystore tier2and3ServerKeyStore.jks
    #Convert Tier 3 keys in Java Keystore to OpenSSL PKCS12 format
    $egokeytool -importkeystore -srckeystore tier2and3ServerKeyStore.jks -srcalias tier3alias -srcstoretype jks -srcstorepass SparkPassword -srckeypass tier3passwd -destkeystore tier3KeyStore.p12 -deststoretype pkcs12 -deststorepass tier3passwd -destkeypass tier3passwd -noprompt
    openssl pkcs12 -in tier3KeyStore.p12 -passin pass:tier3passwd -nocerts -out tier3opensslprivate.key -passout pass:tier3passwd
    openssl pkcs12 -in tier3KeyStore.p12 -passin pass:tier3passwd -clcerts -nokeys -out tier3opensslpublic.pem
    chmod 444 $topdir/security/tier2and3ServerKeyStore.jks
    chmod 444 $topdir/security/tier3KeyStore.p12
    chown -Rh $CLUSTERADMIN:$CLUSTERADMIN $topdir/security
    openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /var/share/tls.key -out /var/share/tls.crt -subj "/CN=$dnsname"
    #=========
    cd $topdir/wlp/usr/shared/resources/security
    rm -f servercertcasigned.pem serverKeyStore.jks srvcertreq.csr serverTrustStore.jks
    $egokeytool -genkeypair -noprompt -alias srvalias -dname "CN=$domain,O=IBM,C=CA" -keystore serverKeyStore.jks -storepass Liberty -keypass Liberty -keyalg rsa -validity 1095 -keysize 2048 -sigalg SHA256withRSA -ext "san=dns:$dnsname"
    $egokeytool -certreq -alias srvalias -file srvcertreq.csr -storepass Liberty -keystore serverKeyStore.jks -ext "san=dns:$dnsname"
    $egokeytool -gencert -infile srvcertreq.csr -outfile servercertcasigned.pem -alias caalias -keystore caKeyStore.jks -storepass Liberty -validity 1095 -ext "san=dns:$dnsname"
    $egokeytool -importcert -noprompt -alias caalias -file cacert.pem -keystore serverKeyStore.jks -storepass Liberty
    $egokeytool -import -noprompt -alias srvalias -file servercertcasigned.pem -storepass Liberty -keystore serverKeyStore.jks
    $egokeytool -importcert -noprompt -alias srvalias -file cacert.pem -keystore serverTrustStore.jks -storepass Liberty
    #==========
    {{- if .Values.dli.enabled }}
    # create certificate and key to enable https for monitor and optimizer Flask server
    CWS_TOP=$topdir
    #DLMAO_HOME=/opt/ibm/spectrumcomputing/dli/dlmao
    KEYSTR=$CWS_TOP/wlp/usr/shared/resources/security/serverKeyStore.jks
    if [ -z "$KEYSTRPASS" ]; then
        KEYSTRPASS=Liberty
    fi
    #DESTKEYSTR_DIR=$DLMAO_HOME/conf
    DESTKEYSTR_DIR=/opt/ibm/spectrumcomputing/dli/conf/dlinsights
    DESTKEYSTR=$DESTKEYSTR_DIR/serverKeyStore.p12
    DESTCRT=$DESTKEYSTR_DIR/srv.crt
    DESTKEY=$DESTKEYSTR_DIR/srv.key
    keytool -importkeystore -srckeystore $KEYSTR -srcstorepass $KEYSTRPASS -destkeystore $DESTKEYSTR \
    -deststoretype PKCS12 -srckeypass $KEYSTRPASS -destkeypass $KEYSTRPASS -deststorepass $KEYSTRPASS \
    -noprompt -srcalias srvalias -storepass $KEYSTRPASS
    openssl pkcs12 -in $DESTKEYSTR -nokeys -out $DESTCRT -password pass:$KEYSTRPASS
    openssl pkcs12 -in $DESTKEYSTR -nodes -nocerts -out $DESTKEY -password pass:$KEYSTRPASS
    #Important: secure the key
    rm -f $DESTKEYSTR
    chmod 400 $DESTCRT $DESTKEY
    chown $CLUSTERADMIN:$CLUSTERADMIN $DESTCRT $DESTKEY
    {{- end }}
  enableLdapLogon: |-
    topdir=/opt/ibm/spectrumcomputing
    sed -i '/EGO_SEC_PLUGIN/d' $topdir/kernel/conf/ego.conf
    echo "EGO_SEC_PLUGIN=sec_ego_pam_default" >> $topdir/kernel/conf/ego.conf
    if [ "x$LDAP_SERVER_IP" = "x" -o "x$BASE_DN" = "x" ]; then
      exit 0
    fi
    disabled=`grep @Ldap_server /etc/nslcd.conf`
    if [ "x$disabled" != "x" ]; then
      sed -i "s/@Ldap_server/$LDAP_SERVER_IP/g" /etc/nslcd.conf
      sed -i "s/@Ldap_server/$LDAP_SERVER_IP/g" /etc/ldap.conf
      sed -i "s/@Base_DN/$BASE_DN/g" /etc/nslcd.conf
      sed -i "s/@Base_DN/$BASE_DN/g" /etc/ldap.conf
      auth-client-config -t nss -p lac_ldap
      echo "session required pam_mkhomedir.so skel=/etc/skel umask=0022" >> /etc/pam.d/common-session
      update-rc.d nslcd enable
      cp /etc/pam.d/common-password /etc/pam.d/common-password.bak
      sed -i 's/use_authtok//' /etc/pam.d/common-password
      /etc/init.d/nscd restart
    fi
  setupHelmCLI.sh: |-
        echo "setup helm cli."
{{- if eq (include "securedHelm" .) "true" }}
        os_arch=`arch`
        bin_arch=linux-amd64
        if [ "$os_arch" != "x86_64" ]; then
            bin_arch=linux-ppc64le
        fi
        #master_ip=`curl -s http://127.0.0.1:8001/api/v1/nodes?labelSelector=master%3Dtrue|grep "kubernetes.io/hostname"|awk -F" " {'print $2'}|sed -e "s/\"//g"|sed -e "s/,//g"`
        master_ip=`curl -s http://127.0.0.1:8001/api/v1/nodes?labelSelector=master%3Dtrue|grep "address"|grep -v "addresses"|head -n1|awk -F" " {'print $2'}|sed -e "s/\"//g"|sed -e "s/,//g"`
        icp_cluster=`echo {{ .Values.sig.registry }}|awk -F"." {'print $1'}`
        if [ ! -f /usr/bin/helm ]; then
          {{- if eq (include "global.icpVersion" .) "2.x" }}
            if [ -f /tmp/helmdir/cli/$bin_arch/helm ]; then
                cp /tmp/helmdir/cli/$bin_arch/helm /usr/bin/helm
            else
                cp /tmp/helmdir/helm /usr/bin/helm
            fi
          {{- else }}
            curl -kLo /tmp/helm-${bin_arch}.tar.gz https://${master_ip}:8443/api/cli/helm-${bin_arch}.tar.gz
            tar -xvf /tmp/helm-${bin_arch}.tar.gz -C /tmp/
            mv /tmp/${bin_arch}/helm /usr/bin/helm
          {{- end }}
        fi
    {{- if eq .Values.helm.credentialType "Secret" }}
        export HELM_HOME={{ template "helmHome" . }}
    {{- else }}
        if [ -z "$ADMIN_USERNAME" -a -f /root/.helm/.credential ]; then
            decodedCred=`cat /root/.helm/.credential |base64 -d`
            export ADMIN_USERNAME=`echo $decodedCred|awk -F':' '{ print $1 }'`
            export ADMIN_PASSWORD=`echo $decodedCred|awk -F':' '{ print $2 }'`
        fi
        {{- if and (eq (.Capabilities.KubeVersion.Major|int) 1) (lt (.Capabilities.KubeVersion.Minor|int) 11)}}
        plugin_installed=`bx plugin list|grep icp`
        if [ "x$plugin_installed" = "x" ]; then
            icp_cli=icp-${bin_arch}
            wget -q https://${master_ip}:8443/api/cli/${icp_cli} --no-check-certificate -P /tmp/
            bx plugin install /tmp/${icp_cli} -f
        fi
        bx pr login -a https://${master_ip}:8443 --skip-ssl-validation -u $ADMIN_USERNAME -p $ADMIN_PASSWORD -c id-${icp_cluster}-account
        {{- else }}
        cloud_ctl=cloudctl-${bin_arch}
        if [ ! -f "/usr/local/bin/cloudctl" ]; then
            curl -kLo /tmp/$cloud_ctl https://${master_ip}:8443/api/cli/$cloud_ctl
            chmod a+x /tmp/$cloud_ctl
            mv /tmp/$cloud_ctl /usr/local/bin/cloudctl
        fi
        export HELM_HOME=/root/.helm
        cloudctl login -a https://${master_ip}:8443 --skip-ssl-validation -u $ADMIN_USERNAME -p $ADMIN_PASSWORD -c id-${icp_cluster}-account -n default
        {{- end }}
        if [ $? -ne 0 ]; then
            echo "helm cli setup failed. you may not be able to create any Spark Instance Groups."
        fi
    {{- end }}
{{- end }}
  startPrequisiteServices: |-
    export HELM_HOST={{ template "helmHost" . }}
    echo "initializing helm."
    {{- if and (eq (include "securedHelm" .) "true") (ne .Values.helm.credentialType "Secret") }}
    mkdir -p /root/.helm
    echo "$ADMIN_USERNAME:$ADMIN_PASSWORD"|base64 > /root/.helm/.credential
    {{- end }}
    source /var/tmp/cfc/setupHelmCLI.sh
    helm init --client-only

    etcdservice=$(curl http://127.0.0.1:8001/api/v1/namespaces/{{ template "etcdServiceNamespace" . }}/services/{{template "etcdService" . }} | grep  '"kind"'|cut -d '"' -f4)
    if [ x"$etcdservice" != "xService" ]; then
      mkdir -p /tmp/conductor-etcd/templates
      cp /var/tmp/prerequisite/etcd-Chart /tmp/conductor-etcd/Chart.yaml
      cp /var/tmp/prerequisite/etcd-template /tmp/conductor-etcd/templates/etcd
      helm install /tmp/conductor-etcd/ --name {{template "etcdService" . }} {{ template "helmFlag" . }}
      if [ $? -ne 0 ]; then
         echo "failed to create prerequisite etcd service automatically."
      fi
      # cws-sig-Cleaner
      mkdir -p /tmp/conductor-imagecleaner/templates
      cp /var/tmp/prerequisite/cws-imagecleaner-Chart /tmp/conductor-imagecleaner/Chart.yaml
      cp /var/tmp/prerequisite/cws-imagecleaner-template /tmp/conductor-imagecleaner/templates/cws-imagecleaner
      helm install /tmp/conductor-imagecleaner/ --name {{template "imageCleaner" . }} {{ template "helmFlag" . }}
      if [ $? -ne 0 ]; then
         echo "failed to create prerequisite cwsimagecleaner daemonset automatically."
      fi
    fi

{{- if eq .Values.cluster.proxyOption "HttpProxy" }}
    # cws proxy
    proxyservice=$(curl http://127.0.0.1:8001/api/v1/namespaces/default/services/{{template "cwsProxyService" . }} | grep  '"kind"'|cut -d '"' -f4)
    if [ x"$proxyservice" != "xService" ]; then
      mkdir -p /tmp/conductor-proxy/templates
      cp /var/tmp/prerequisite/cws-proxy-Chart /tmp/conductor-proxy/Chart.yaml
      cp /var/tmp/prerequisite/cws-proxy-template /tmp/conductor-proxy/templates/cws-proxy
      helm install /tmp/conductor-proxy/ --name {{template "cwsProxyService" . }} {{ template "helmFlag" . }}
      if [ $? -ne 0 ]; then
         echo "failed to create prerequisite CwS proxy service automatically."
      fi
    fi
{{- end }}

  startMaster.sh: |-
    topdir=/opt/ibm/spectrumcomputing
    exec 1<>$topdir/startmaster.log
    exec 2>&1
    set +x
    cd /bin
    rm -rf sh
    ln -s bash sh
    TZ=`date +%z`
    echo "GMT$TZ" >/etc/timezone
    /bin/cp -f /tmp/kubedir/kubectl /usr/local/bin/

    # workaroud to delete the zero byte Nvidia binaries for PAIE
    {{- if eq (include "isPAIE" .) "yes" }}
    rm -rf /usr/bin/nvidia*
    {{- end }}

    echo "export PATH=$PATH" >> /etc/profile
    sh /var/tmp/cfc/startPrequisiteServices
    helm list {{ template "helmFlag" . }}
    if [ $? -ne 0 ]; then
        echo "helm commandline failed, conductor master will not start."
        exit 1
    fi
    sh /var/tmp/cfc/enableLdapLogon
    sh /var/tmp/cfc/generate_ssl.sh

    # workaroud to help get ego linux version
    binarytypename=`cat $topdir/kernel/conf/profile.ego |grep "BINARY_TYPE = \"fail\""|awk '{print $3}'|awk -F$ '{print $2 }'`
    egoversion=`ls $topdir/|grep 3`
    binarytype=`ls $topdir/$egoversion |grep linux`
    sed -i "/BINARY_TYPE = \"fail\"/i\\$binarytypename=$binarytype" $topdir/kernel/conf/profile.ego
    MASTER_HOST=`hostname -f`
    if [ ! -f /var/shareDir/profile.platform ]; then
          echo "Set up a new Cluster..."
          sed -i "s/iCluster_docker/{{ .Release.Name }}/g" $topdir/eservice/esc/conf/services/elk_elasticsearch.xml
          sed -i "s/iCluster_docker/{{ .Release.Name }}/g" $topdir/eservice/esc/conf/services/elk_elasticsearch_master.xml
          sed -i "s/iCluster_docker/{{ .Release.Name }}/g" $topdir/eservice/esc/conf/services/elk_manager.xml
          sed -i "s/iCluster_docker/{{ .Release.Name }}/g" $topdir/eservice/esc/conf/services/elk_elasticsearch_data.xml
          sed -i "s/@DERBY_DB_HOST@/$MASTER_HOST/g" $topdir/eservice/esc/conf/services/derby_service.xml
          sed -i "s/@DERBY_DB_HOST@/$MASTER_HOST/g" $topdir/perf/conf/datasource.xml
          sed -i '/ASC_HOME/a\      <ego:EnvironmentVariable name="K8S_NAMESPACE">{{.Release.Namespace}}</ego:EnvironmentVariable>'  $topdir/eservice/esc/conf/services/ascd_service.xml
          sed -i "s/iCluster_docker/{{ .Release.Name }}/g" $topdir/kernel/conf/ego.shared
          mv $topdir/kernel/conf/ego.cluster.iCluster_docker $topdir/kernel/conf/ego.cluster.{{ .Release.Name }}
          #update Spark version Yaml files
          escape()
          {
            repl='s/\//\\\//g'
            rep2='s/\*/\\\*/g'
            ret1=`echo $1 | sed $repl`
            ret=`echo $ret1 | sed $rep2`
          }
          escape "http://127.0.0.1:8001"; MasterURL4SS=$ret
          {{- if eq .Values.cluster.proxyOption "IngressProxy" }}
          sed -i "s/8443/{{ template "guiPort" . }}/g" $topdir/gui/conf/server_gui.xml
          sed -i "s/8543/{{ template "egoRestPort" . }}/g" $topdir/kernel/conf/server_rest.xml
          sed -i "s/8643/{{ template "ascdPort" . }}/g" $topdir/ascd/conf/server_ascd.xml
          {{- if .Values.dli.enabled }}
          sed -i "s/5000/{{ template "dliMonitorPort" . }}/g" $topdir/dli/conf/dlinsights/profile.dlinsights
          sed -i "s/5001/{{ template "dliOptimizerPort" . }}/g" $topdir/dli/conf/dlinsights/profile.dlinsights
          sed -i "s/5000/{{ template "dliMonitorPort" . }}/g" $topdir/dli/conf/dlpd/dlpd.conf
          sed -i "s/5001/{{ template "dliOptimizerPort" . }}/g" $topdir/dli/conf/dlpd/dlpd.conf
          {{- end }}
          {{- end }}
          spark_pkg_dir=$topdir/conductorspark/conf/packages
          spark_versions=( "Spark1.6.1" "Spark2.1.1" "Spark2.2.0" "Spark2.3.0" "Spark2.3.1" )
          for spark_version in "${spark_versions[@]}"
          do
              spark_k8s_yml=$spark_pkg_dir/${spark_version}-Conductor{{ template "global.conductorVersion" . }}/${spark_version}.k8s.yaml
              sparkyaml=$spark_pkg_dir/${spark_version}-Conductor{{ template "global.conductorVersion" . }}/${spark_version}.yaml
              if [ -f $spark_k8s_yml ]; then
                  mv $spark_k8s_yml $sparkyaml
                  sed -i "/maxrestarts/d" $sparkyaml
                  sed -i "s/@RELNAME/{{ .Release.Name }}/" $sparkyaml
                  sed -i "s#@K8SMASTERURL#$MasterURL4SS#" $sparkyaml
                  sed -i "s/@SSALCUNITGPU/{{.Values.sig.gpu}}/" $sparkyaml
                  sed -i "s/@SSALCUNIT/{{ template "getmaxslots" .}}/" $sparkyaml
                  sed -i "s/@SSALCMAXPERCYCLE/{{.Values.sig.ssAllocationUnit}}/" $sparkyaml
                  sed -i "s/@SSALCMAX/{{ .Values.sig.maxReplicas }}/" $sparkyaml
                  sed -i "s/@SSALCINTVAL/{{ .Values.sig.ssAllocationInterval }}/" $sparkyaml
                  {{- if eq .Values.cluster.proxyOption "IngressProxy" }}
                  sed -i "s/@SPARKPREFIX/\"https:\/\/{{template "master-fullname" .}}:{{ template "proxyHttpsPort" . }}\/\", { get_param: web_url_protocol }, \"-\"/g" $sparkyaml
                  sed -i "s/@PORTLINK/-/g" $sparkyaml
                  sed -i "s/@PROTOCOLLINK/-/g" $sparkyaml
                  sed -i "s/@INGRESSPREFIX/\"https:\/\/{{template "master-fullname" .}}:{{ template "proxyHttpsPort" . }}\/\",/g" $sparkyaml
                  {{- else }}
                  sed -i "s/@SPARKPREFIX/\"spark:\/\/\"/g" $sparkyaml
                  sed -i "s/@PORTLINK/:/g" $sparkyaml
                  sed -i "s/@PROTOCOLLINK/:\/\//g" $sparkyaml
                  sed -i "s/@INGRESSPREFIX//g" $sparkyaml
                  {{- end }}
              fi
          done

          . $topdir/profile.platform
          echo "$(hostname -i|awk '{print $1}')  {{ template "master-fullname" . }}.{{ template "master-fullname" . }}.{{.Release.Namespace}}.svc.cluster.local" >> /etc/hosts
          while [ true ]
          do
            egoconfig join {{ template "master-fullname" . }}.{{ template "master-fullname" . }}.{{.Release.Namespace}}.svc.cluster.local -f
            if [ $? -eq 0 ]; then
              break
            fi
          done
          egoconfig setentitlement /var/tmp/cfc/license.dat
          egoconfig mghost /var/shareDir -f
          cp $topdir/profile.platform /var/shareDir/profile.platform
          echo "EGO_DYNAMIC_HOST_TIMEOUT=10m" >> /var/shareDir/kernel/conf/ego.conf
          echo "EGO_DYNAMIC_HOST_WAIT_TIME=1" >> /var/shareDir/kernel/conf/ego.conf
          echo "EGO_RESOURCE_UPDATE_INTERVAL=1" >> /var/shareDir/kernel/conf/ego.conf
          echo "EGO_ENABLE_RG_UPDATE_MEMBERSHIP=Y" >> /var/shareDir/kernel/conf/ego.conf
          echo "EGO_RG_UPDATE_MEMBERSHIP_INTERVAL=10" >> /var/shareDir/kernel/conf/ego.conf
          echo "EGO_ENABLE_BORROW_ONLY_CONSUMER=Y" >> /var/shareDir/kernel/conf/ego.conf
          sed -i "s/ASC_AUTO_DEPLOY_ON_NEW_HOST=ON/ASC_AUTO_DEPLOY_ON_NEW_HOST=OFF/" /var/shareDir/ascd/conf/ascd.conf
          echo -e "\nCONDUCTOR_K8S_ENABLED=ON" >> /var/shareDir/ascd/conf/ascd.conf
          echo "CONDUCTOR_K8S_NAMESPACE={{.Release.Namespace}}" >> /var/shareDir/ascd/conf/ascd.conf
          echo "CONDUCTOR_K8S_CPUREQ={{.Values.sig.cpu}}" >> /var/shareDir/ascd/conf/ascd.conf
          echo "CONDUCTOR_K8S_MEMREQ={{.Values.sig.memory}}" >> /var/shareDir/ascd/conf/ascd.conf
          echo "CONDUCTOR_K8S_GPUREQ={{.Values.sig.gpu}}" >> /var/shareDir/ascd/conf/ascd.conf
          echo "CONDUCTOR_K8S_CPUMAX={{.Values.sig.cpu}}" >> /var/shareDir/ascd/conf/ascd.conf
          echo "CONDUCTOR_K8S_MEMMAX={{.Values.sig.memory}}" >> /var/shareDir/ascd/conf/ascd.conf
          echo "CONDUCTOR_K8S_GPUMAX={{.Values.sig.gpu}}" >> /var/shareDir/ascd/conf/ascd.conf
          {{- if .Values.cluster.ascdDebugPort }}
          sed -i '/^      <!-- Uncomment to enable Java debug port for ascd. -->$/{$!{N;s/^      <!-- Uncomment to enable Java debug port for ascd. -->\n      <!--$//;ty;P;D;:y}}' /var/shareDir/eservice/esc/conf/services/ascd_service.xml
          sed -i '/^      <ego:EnvironmentVariable name="ASC_DEBUG_PORT">8000<\/ego:EnvironmentVariable>$/{$!{N;s/^      <ego:EnvironmentVariable name="ASC_DEBUG_PORT">8000<\/ego:EnvironmentVariable>\n      -->$/      <ego:EnvironmentVariable name="ASC_DEBUG_PORT">{{.Values.cluster.ascdDebugPort}}<\/ego:EnvironmentVariable>/;ty;P;D;:y}}' /var/shareDir/eservice/esc/conf/services/ascd_service.xml
          {{- end }}
          sed -i "s/ELK_DATA_LOCATION=\/opt\/ibm\/spectrumcomputing\/integration\/elk\/hosts/ELK_DATA_LOCATION=\/var\/shareDir\/integration\/elk\/hosts/" /var/shareDir/integration/elk/conf/elk.conf

          . $topdir/profile.platform;
    else
          echo "Recover a master contiainer..."
          sharedir=/var/shareDir
          profiles=`cat $topdir/profile.platform|awk '{print $2}'`
          for profile in $profiles
          do
            p=${profile##$topdir}
            sharedprofile=$sharedir$p
            if [ -f $sharedprofile ]; then
              cp -f $sharedprofile $profile
            fi
          done
          echo "$(hostname -i|awk '{print $1}')  {{ template "master-fullname" . }}.{{ template "master-fullname" . }}.{{.Release.Namespace}}.svc.cluster.local" >> /etc/hosts
          . $topdir/profile.platform
    fi

    {{- if .Values.dli.enabled }}
    dliversion=`ls $topdir/wlp/usr/servers/gui/apps/dli/`
    {{- if eq .Values.cluster.proxyOption "IngressProxy" }}
    sed -i "s/9243/{{ template "dlRestPort" . }}/g" $topdir/wlp/usr/servers/dlrest/$dliversion/server_dlrest.xml
    sed -i "s/9243/{{ template "dlRestPort" . }}/g" $topdir/dli/$dliversion/dlpd/liberty/dlrest/$dliversion/server_dlrest.xml
    sed -i "s#//.*:9243#//{{template "master-fullname" .}}:{{ template "dlRestPort" . }}#g" $topdir/wlp/usr/servers/gui/apps/dli/$dliversion/dlgui/apidocs/restful/dlpd/index.html
    {{- else }}
    sed -i "s#//.*:#//${MASTER_HOST}:#g" $topdir/wlp/usr/servers/gui/apps/dli/$dliversion/dlgui/apidocs/restful/dlpd/index.html
    {{- end }}
    DLI_SHARED_FS=/mygpfs
    IMAGE_HOSTNAME=900d7bfbe521
    MASTER_HOST=`hostname -f`
    HOSTIP=`getent ahostsv4 $MASTER_HOST | grep -Fw "$MASTER_HOST" | awk '{print $1}'`
    HOSTNET=`ip addr show | grep inet | grep -Fw "$HOSTIP" | awk '{print $2}'`
    FABRIC_COMMIPNETWORK=${FABRIC_COMMIPNETWORK:-$HOSTNET}
    cp $topdir/dli/conf/dlpd/dlpd.conf $topdir/dli/conf/dlpd/dlpd.conf_bak
    sed -i "s#@FABRIC_COMMIPNETWORK@#${FABRIC_COMMIPNETWORK}#g" $topdir/dli/conf/dlpd/dlpd.conf
    sed -i "s#${IMAGE_HOSTNAME}#${MASTER_HOST}#g" $topdir/dli/conf/dlpd/dlpd.conf
    rm -rf $DLI_SHARED_FS/*
    cp -r ${DLI_SHARED_FS}_bak/* $DLI_SHARED_FS
    {{- end }}

    sed -i '/docker_active/a\   sigName String 10 () ()' $EGO_CONFDIR/ego.shared
    sed -i '/docker_active/a\sigName [default]' $EGO_CONFDIR/ego.cluster.{{ .Release.Name }}

    egosh ego start

    {{- if or .Values.dli.enabled (gt (.Values.sig.gpu|int) 0) }}
    AscdConfFile="$EGO_CONFDIR/../../ascd/conf/ascd.conf"
    GPUModified=$(cat $AscdConfFile | grep "ASCD_GPU_ENABLED" | grep "ON")
    if [ "$GPUModified" = "" ]; then
        retries=0
        egoUser=Admin
        egoPass=Admin
        egosh user logon -u $egoUser -x $egoPass
        while [ $? -ne 0 -a $retries -le 30 ]
        do
            sleep 10
            retries=$((retries+1))
            egosh user logon -u $egoUser -x $egoPass
        done
        egosh service stop all
        sleep 10
        retries=0
        result=`$topdir/conductorspark/{{ template "global.conductorVersion" . }}/etc/gpuconfig.sh enable --quiet -u $egoUser -x $egoPass`
        while [ $? -ne 0 -a $retries -le 60 ]
        do
            echo "$result"|grep "already configured this feature"
            if [ $? -eq 0 ]; then
                break
            fi
            sleep 10
            retries=$((retries+1))
            result=`$topdir/conductorspark/{{ template "global.conductorVersion" . }}/etc/gpuconfig.sh enable --quiet -u $egoUser -x $egoPass`
        done
        if [ $retries -ge 60 ]; then
            echo "Failed to enable GPU monitoring feature in EGO."
            exit -1
        fi
    fi
    {{- end }}
    #record self IP and name to ETCD
    curl -s -X PUT {{ template "etcdHostDir" .}}/$(hostname) -d value="$(hostname -i|awk '{print $1}')" -d ttl=20
    curl -s -X PUT {{ template "etcdHostDir" .}}/$(hostname).$(hostname).{{.Release.Namespace}}.svc.cluster.local -d value="$(hostname -i|awk '{print $1}')" -d ttl=20
    mkdir -p /var/shareDir/sig-template/templates
    cp /var/tmp/cfc/sig-template_Chart.yaml /var/shareDir/sig-template/Chart.yaml
    cp /var/tmp/cfc/sig-template_templates_cws-slave-deployment.yaml  /var/shareDir/sig-template/templates/cws-slave-deployment.yaml
    mkdir -p /var/shareDir/livy-template/templates
    cp /var/tmp/cfc/livyChart.yaml /var/shareDir/livy-template/Chart.yaml
    cp /var/tmp/cfc/livyTemplate.yaml /var/shareDir/livy-template/templates/livy-deployment.yaml
    sh /var/tmp/cfc/appendEtcHostfromShare.sh
  appendEtcHostfromShare.sh: |-
    #set -x
    cp /etc/hosts /hosts.original
    while [ true ]
    do
        if [ -f /hosts.tmp ]; then
            rm -f /hosts.tmp
        fi
        cat /hosts.original >> /hosts.tmp
        curl -s -X PUT {{ template "etcdHostDir" .}}/$(hostname) -d value="$(hostname -i|awk '{print $1}')" -d ttl=20
        curl -s -X PUT {{ template "etcdHostDir" .}}/$(hostname).$(hostname).{{.Release.Namespace}}.svc.cluster.local -d value="$(hostname -i|awk '{print $1}')" -d ttl=20
        curl -s  {{ template "etcdHostDir" .}} | python -c 'import json,sys;obj=json.load(sys.stdin); print "\n".join([":".join([x["value"], x["key"]]) for x in obj["node"]["nodes"]])' | sed "s/\/cwsnodemap\///" > /hostlist.yaml
        tac /hostlist.yaml | while read -r line
        do
          ip=$(echo $line | cut -f 1 -d':' | xargs)
          host=$(echo $line | cut -f 2 -d':' | xargs)
          if [ ! -z "$host" ]; then
            ifit=$(grep "$ip  ${host}$" /hosts.tmp)
            if [ -z "$ifit" ]; then
               echo "$ip  $host"  >> /hosts.tmp
            fi
          fi
        done
        cat /hosts.tmp > /etc/hosts
        sleep 10
    done

  updateParameters.py: |-
        import json,sys,os
        if len(sys.argv) <= 2:
          print "Path of parameters json file and target file must be specified"
          sys.exit(1)
        with open(sys.argv[1], "r") as f:
          obj=json.load(f)
          for k,v in obj.items():
             if k == "cpu":
                os.system('sed -i "s/@CPUREQUEST/%s/g" %s' % (v, sys.argv[2]))
             elif k == "gpu":
                os.system('sed -i "s/@GPUREQUEST/%s/g" %s' % (v, sys.argv[2]))
             elif k == "memory":
                os.system('sed -i "s/@MEMREQUEST/%s/g" %s' % (v, sys.argv[2]))
             else:
                print "ignore value of " + k

  appendVolumes.py: |-
        import json,sys,os
        if len(sys.argv) <= 2:
          print "Path of volumes json file and target file must be specified"
          sys.exit(1)
        with open(sys.argv[1], "r") as f:
          obj=json.load(f)
          if "volumes" in obj:
            os.system('echo volumes: >> %s' % (sys.argv[2]))
            for volume in obj["volumes"]:
              os.system('echo "  - hostpath: %s" >> %s' % (volume["hostpath"], sys.argv[2]))
              os.system('echo "    type: %s" >> %s' % (volume["type"], sys.argv[2]))
              os.system('echo "    containerpath: %s" >> %s' % (volume["containerpath"], sys.argv[2]))
          if "dataimages" in obj:
            os.system('echo dataimages: >> %s' % (sys.argv[2]))
            for img in obj["dataimages"]:
              os.system('echo "  - imagename: %s" >> %s' % (img["imagename"], sys.argv[2]))
              os.system('echo "    volumes:" >> %s' % (sys.argv[2]))
              pathmap = {}
              for volume in img["volumes"]:
                containerpath = volume["containerpath"]
                if containerpath not in pathmap:
                  pathmap[containerpath] = []
                pathmap[containerpath].append({"type": volume["type"], "hostpath": volume["hostpath"]})
              for k, v in pathmap.items():
                os.system('echo "      - containerpath: %s" >> %s' % (k, sys.argv[2]))
                os.system('echo "        hostpaths:" >> %s' % (sys.argv[2]))
                for hp in v:
                  os.system('echo "          - hostpath: %s" >> %s' % (hp["hostpath"], sys.argv[2]))
                  os.system('echo "            type: %s" >> %s' % (hp["type"], sys.argv[2]))

  createSIGContainer.sh: |-
        # redirect output to a log file.
        topdir=/opt/ibm/spectrumcomputing
        exec 1<>$topdir/conductorspark/logs/createSIGContainer.log
        exec 2>&1
        set -x
        sig_name={{ .Release.Name }}-$1
        sig_rg_name=$1
        executor=$2
        sig_ns=$3
        paras=$4
        volumes=$5
        if [ "x$sig_rg_name" = "x" ]; then
          echo "Missed to specify a SIG name, exit."
          exit -1
        fi
        if [ "x$executor" = "x" ]; then
          echo "Missed to specify an executor name, exit."
          exit -1
        fi
        if [ "x$paras" = "x" ]; then
          echo "Missed to specify file path of parameters for the SIG, exit."
          exit -1
        fi
        if [ "x$volumes" = "x" ]; then
          echo "Missed to specify file path of volumes for the SIG, exit."
          exit -1
        fi
        #setup context
        export HELM_HOST={{ template "helmHost" . }}
        source /var/tmp/cfc/setupHelmCLI.sh
        cd /var/shareDir/
        . $topdir/profile.platform
        #install the SIG helm chart
        if [ -d "./$sig_name" ]; then
          echo "./$sig_name has been created. Remove it firstly."
          helm delete --purge $sig_name {{ template "helmFlag" . }} || exit -1
          rm -rf ./$sig_name
          egosh resourcegroup delete "$sig_rg_name"_services
          egosh resourcegroup delete "$sig_rg_name"_workloads
          egosh resourcegroup delete "$sig_rg_name"_gpu
          curl -X PUT {{ template "etcdInstanceDeletion" .}}/{{.Values.sig.registry}}@@{{template "imageNamespace" . }}@@$sig_name -d value="latest"
        fi
        #create the SIG deployment
        cp -R ./sig-template ./$sig_name
        #sed -i "s/@SIGNAME/$sig_name/g" ./$sig_name/values.yaml
        sed -i "s/@SIGNAME/$sig_name/g" ./$sig_name/Chart.yaml
        sed -i "s/@SIGNAME/$sig_name/g" ./$sig_name/templates/cws-slave-deployment.yaml
        sed -i "s/@EXECUTOR/$executor/g" ./$sig_name/templates/cws-slave-deployment.yaml
        sed -i "s/@SIG_NAME/$1/g" ./$sig_name/templates/cws-slave-deployment.yaml
        sed -i "s/@]/}/g" ./$sig_name/templates/cws-slave-deployment.yaml
        sed -i "s/@\[/{/g" ./$sig_name/templates/cws-slave-deployment.yaml
        sed -i "s/@new_namespace/.Values.new_namespace/g" ./$sig_name/templates/cws-slave-deployment.yaml
        sed -i "s/@storage_class_name/.Values.storage_class_name/g" ./$sig_name/templates/cws-slave-deployment.yaml
        {{- if .Values.dli.enabled }}
        sed -i "s/@need_create_pvc/.Values.need_create_pvc/g" ./$sig_name/templates/cws-slave-deployment.yaml
        {{- end }}

        #input data volumes
        python /var/tmp/cfc/updateParameters.py $paras /var/shareDir/$sig_name/templates/cws-slave-deployment.yaml
        python /var/tmp/cfc/appendVolumes.py $volumes /var/shareDir/$sig_name/values.yaml

        storage_class_name=
        storageClassNameKeyVal=`cat $paras| python -mjson.tool|grep storage_class_name`
        if [ "x$storageClassNameKeyVal" != "x" ]; then
           storage_class_name=`echo $storageClassNameKeyVal|awk -F'"' '{print $4}'`
           echo "storage_class_name: $storage_class_name" >> ./$sig_name/values.yaml
        fi

        namespace=$sig_ns
        if [ x"$sig_ns" = "x" ]; then
          namespace={{.Release.Namespace}}
        fi
        sed -i "s/@namespace/$namespace/g" ./$sig_name/templates/cws-slave-deployment.yaml

        if [ $namespace != {{.Release.Namespace}} ]; then
          echo "new_namespace: true" >> ./$sig_name/values.yaml
          {{- if .Values.dli.enabled }}
          pvc4DLI=`curl -s  http://127.0.0.1:8001/api/v1/namespaces/$namespace/persistentvolumeclaims/{{ .Release.Name }}-dl|grep "NotFound"`
          if [ -z "$pvc4DLI" ]; then
              echo "need_create_pvc: false" >> ./$sig_name/values.yaml
          else
              echo "need_create_pvc: true" >> ./$sig_name/values.yaml
          fi
          {{- end }}
        else
          echo "new_namespace: false" >> ./$sig_name/values.yaml
          {{- if .Values.dli.enabled }}
          echo "need_create_pvc: false" >> ./$sig_name/values.yaml
          {{- end }}
        fi

        helm lint $sig_name|| exit -1
        helm install --name $sig_name $sig_name --namespace $namespace {{ template "helmFlag" . }}|| exit -1
        #create resoure group for the SIG
        sleep 1
        retries=0
        egosh user logon -u Admin -x Admin
        #create resource group for services
        egosh resourcegroup add "$sig_rg_name"_services -t Dynamic -R "select(sigName=='$sig_rg_name')" -s $(({{ template "getserviceslots" .}}))
        while [ $? -ne 0 -a $retries -le 60 ]
        do
           sleep 10
           retries=$((retries+1))
           egosh resourcegroup add "$sig_rg_name"_services -t Dynamic -R "select(sigName=='$sig_rg_name')" -s $(({{ template "getserviceslots" .}}))
        done
        if [ $retries -ge 60 ]; then
           echo "Failed to create resource group <'$sig_rg_name'_services> in EGO."
           exit -1
        fi
        #create resource group for workloads
        egosh resourcegroup add "$sig_rg_name"_workloads -t Dynamic -R "select(sigName=='$sig_rg_name')" -s $(({{ template "getmaxslots" .}}))
        while [ $? -ne 0 -a $retries -le 60 ]
        do
           sleep 10
           retries=$((retries+1))
           egosh resourcegroup add "$sig_rg_name"_workloads -t Dynamic -R "select(sigName=='$sig_rg_name')" -s $(({{ template "getmaxslots" .}}))
        done
        if [ $retries -ge 60 ]; then
           echo "Failed to create resource group <'$sig_rg_name'_workloads> in EGO."
           exit -1
        fi
        # Todo: change the following .Values.sig.gpu to the one passed from ASCD parameter
        {{- if gt (.Values.sig.gpu|int) 0 }}
        egosh resourcegroup add "$sig_rg_name"_gpu -t Dynamic -R "select(sigName=='$sig_rg_name')" -e ngpus
        {{- end }}
        exist=0
        for consumer in `egosh consumer list|awk '{print $1}'`
        do
            if [ "$executor" = "$consumer"  ]; then
               exist=1
               break
            fi
        done
        if [ $exist -eq 1 ]; then
            {{- if gt (.Values.sig.gpu|int) 0 }}
            egosh consumer addrg /$executor -g "$sig_rg_name"_gpu,"$sig_rg_name"_workloads,"$sig_rg_name"_services
            {{- else }}
            egosh consumer addrg /$executor -g "$sig_rg_name"_workloads,"$sig_rg_name"_services
            {{- end }}
        else
            {{- if gt (.Values.sig.gpu|int) 0 }}
            egosh consumer add /$executor -e $executor -a Admin -g "$sig_rg_name"_gpu,"$sig_rg_name"_workloads,"$sig_rg_name"_services
            {{- else }}
            egosh consumer add /$executor -e $executor -a Admin -g "$sig_rg_name"_workloads,"$sig_rg_name"_services
            {{- end }}
        fi
        egosh user assignrole -u $executor -p /$executor -r "Consumer Admin"
        exit 0
  deleteSIGContainer.sh: |-
        # redirect output to a log file.
        topdir=/opt/ibm/spectrumcomputing
        exec 1<>$topdir/conductorspark/logs/deleteSIGContainer.log
        exec 2>&1
        set -x
        sig_name={{ .Release.Name }}-$1
        sig_rg_name=$1
        executor=$2
        if [ "x$sig_rg_name" = "x" ]; then
          echo "Missed to specify a SIG name, exit."
          exit -1
        fi
        if [ "x$executor" = "x" ]; then
          echo "Missed to specify an executor name, exit."
          exit -1
        fi
        #setup context
        export HELM_HOST={{ template "helmHost" . }}
        source /var/tmp/cfc/setupHelmCLI.sh
        cd /var/shareDir/
        . $topdir/profile.platform
        #uninstall the SIG helm chart
        if [ -d "./$sig_name" ]; then
          helm status $sig_name {{ template "helmFlag" . }}
          if [ $? -eq 0 ]; then
            helm delete --purge $sig_name {{ template "helmFlag" . }}|| exit -1
          fi
          rm -rf ./$sig_name

          helm status $sig_name-livy {{ template "helmFlag" . }}
          if [ $? -eq 0 ]; then
            helm delete --purge $sig_name-livy {{ template "helmFlag" . }}| exit -1
          fi

          egosh user logon -u Admin -x Admin
          exist=0
          for consumer in `egosh consumer list|awk '{print $1}'`
          do
            if [ "$executor" = "$consumer"  ]; then
               exist=1
               break
            fi
          done
          if [ $exist -eq 1 ]; then
            egosh consumer removerg /$executor -g "$sig_rg_name"_gpu,"$sig_rg_name"_workloads
            inuse=`egosh consumer view $executor|grep ResourceGroupName`
            if [ x"$inuse" = 'x' ]; then
                egosh user unassignrole -u $executor -p /$executor -r "Consumer User"
                egosh consumer delete /$executor
            fi
          fi

          egosh resourcegroup delete "$sig_rg_name"_services
          egosh resourcegroup delete "$sig_rg_name"_workloads
          egosh resourcegroup delete "$sig_rg_name"_gpu

          curl -X PUT {{ template "etcdInstanceDeletion" .}}/{{.Values.sig.registry}}@@{{template "imageNamespace" . }}@@$sig_name -d value="latest"
        fi
        exit 0

  livyChart.yaml: |-
        name: @SIGNAME-livy
        version: {{.Chart.Version}}
        description: Livy server for IBM Spectrum Conductor Spark Instance Group
        home: https://www.ibm.com/support/knowledgecenter/SSZU2E/product_welcome_conductorspark.html
        icon: https://www.ibm.com/developerworks/community/groups/service/html/image?communityUuid=281605c9-7369-46dc-ad03-70d9ad377480&lastMod=1464891144123&showDefaultForNoPermissions=true
        keywords:
        - Conductor
        - Spark Instance Group
        appVersion: 2.3.0
        maintainers:
        - name: IBM Spectrum Conductor Team

  livyTemplate.yaml: |-
        apiVersion: v1
        kind: Service
        metadata:
          name: @SIGNAME-livy
          labels:
            heritage: {{.Release.Service | quote }}
            release: {{.Release.Name | quote }}
            chart: "{{.Chart.Name}}"
            app: @SIGNAME-livy
        spec:
          ports:
            - name: livy-server
              port: 8998
              targetPort: 8998
              protocol: TCP
          type: NodePort
          selector:
              heritage: {{.Release.Service | quote }}
              release: {{.Release.Name | quote }}
              chart: "{{.Chart.Name}}"
              app: @SIGNAME-livy
        ---
        apiVersion: extensions/v1beta1
        kind: Deployment
        metadata:
          name: @SIGNAME-livy
          labels:
            heritage: {{.Release.Service | quote }}
            release: {{.Release.Name | quote }}
            chart: "{{.Chart.Name}}"
            app: @SIGNAME-livy
        spec:
          replicas: 1
          strategy:
            type: RollingUpdate
          selector:
            matchLabels:
              app: @SIGNAME-livy
              heritage: {{.Release.Service | quote }}
              release: {{.Release.Name | quote }}
              chart: "{{.Chart.Name}}"
          template:
            metadata:
              labels:
                heritage: {{.Release.Service | quote }}
                release: {{.Release.Name | quote }}
                chart: "{{.Chart.Name}}"
                app: @SIGNAME-livy
              annotations:
                {{- if .Values.dli.enabled }}
                productName: "@SIGNAME:IBM Spectrum Conductor Deep Learning Impact"
                productVersion: "1.2"
                productID: "IBMSpectrumDLI-Eval"
                {{- else }}
                productName: "@SIGNAME:IBM Spectrum conductor"
                productVersion: "2.3"
                productID: "IBMSpectrumConductor-Eval"
                {{- end }}
            spec:
              affinity:
              {{- include "nodeaffinity" . | indent 14 }}
              imagePullSecrets:
                - name: sigregistrykey-@SIGNAME
              containers:
                - name: @SIGNAME-livy
                  image: @LIVYIMAGE
                  imagePullPolicy: IfNotPresent
                  ports:
                    - containerPort: 8998
                  command: ["/bin/bash","-c"]
                  args: ["$(bash /var/tmp/cfc/startLivy.sh @LIVYDIR)"]
                  resources:
                    requests:
                      cpu: 1
                      memory: 2Gi
                    limits:
                      cpu: 1
                      memory: 4Gi
                  livenessProbe:
                    tcpSocket:
                      port: 8998
                    initialDelaySeconds: 120
                    periodSeconds: 10
                  volumeMounts:
                  - mountPath: /opt/sparkhome
                    name: sparkhome
                  - mountPath: /var/tmp/cfc
                    name: conductor-slave-bootstrap
                  - mountPath: /dev/shm
                    name: dshm
                  - mountPath: /etc/localtime
                    name: host-timezone
                    readOnly: true
              initContainers:
                - name: sig-image
                  image: @SIGIMAGE
                  imagePullPolicy: Always
                  resources:
                    requests:
                      cpu: 0.5
                      memory: 512Mi
                    limits:
                      cpu: 0.5
                      memory: 512Mi
                  volumeMounts:
                  - mountPath: /var/tmp/cfc
                    name: conductor-slave-bootstrap
                  - mountPath: /opt/sparkhome
                    name: sparkhome
                  args:
                  - "cp -r @SPARKHOME /opt/sparkhome; cp /opt/ibm/spectrumcomputing/security/caKeyStore.jks /opt/sparkhome; cp /opt/ibm/spectrumcomputing/security/cacert.pem /opt/sparkhome"
                  command:
                  - /bin/bash
                  - -c
              volumes:
                - name: sparkhome
                  emptyDir: {}
                - name: conductor-slave-bootstrap
                  configMap:
                    name: "@SIGNAME-bootstrap"
                - name: host-timzone
                  hostPath:
                    path: /etc/localtime
              terminationGracePeriodSeconds: 30
  postDeploy.sh: |-
        topdir=/opt/ibm/spectrumcomputing
        exec 1<>$topdir/conductorspark/logs/postDeploy.log
        exec 2>&1
        set -x
        sig_name={{ .Release.Name }}-$1
        spark_home=$2
        k8sParams=$3
        if [ "x$1" = "x" ]; then
          echo "Missed to specify a SIG name, exit."
          exit -1
        fi
        if [ "x$2" = "x" ]; then
          echo "Missed to specify spark home directory of Spark instance group $1, exit."
          exit -1
        fi
        if [ "x$3" = "x" ]; then
          echo "Missed to specify Kubernetes parameters of Spark instance group $1, exit."
          exit -1
        fi

        livy_top_dir=
        livy_image=
        namespace=
        OLD_IFS="$IFS"
        IFS="*"
        params=($k8sParams)
        for param_pair in ${params[@]}
        do
          k=`echo $param_pair|cut -d "=" -f 1`
          v=`echo $param_pair|cut -d "=" -f 2`
          case $k in
            livy_top_dir)
              livy_top_dir=$v
            ;;
            livy_image)
              livy_image=$v
            ;;
            namespace)
              namespace=$v
            ;;
          esac
        done
        IFS="$OLD_IFS"

        if [ x$namespace = x ]; then
           echo "Invalid Kubernetes parameters of Spark instance group $1, exit."
           exit -1
        fi
        if [ x$livy_top_dir = x -o x$livy_image = x ]; then
           echo "Not enable livy server."
           exit 0
        fi

        livy_image=${livy_image//\//\\/}
        livy_top_dir=${livy_top_dir//\//\\/}
        spark_home=${spark_home//\//\\/}

        cp -r /var/shareDir/livy-template/ /tmp/$sig_name-livy
        sed -i "s/@LIVYIMAGE/$livy_image/g" /tmp/$sig_name-livy/templates/livy-deployment.yaml
        sed -i "s/@LIVYDIR/$livy_top_dir/g" /tmp/$sig_name-livy/templates/livy-deployment.yaml
        sed -i "s/@SIGNAME/$sig_name/g" /tmp/$sig_name-livy/templates/livy-deployment.yaml
        sed -i "s/@SPARKHOME/$spark_home/g" /tmp/$sig_name-livy/templates/livy-deployment.yaml
        sed -i "s/@SIGIMAGE/{{.Values.sig.registry}}\/default\/$sig_name/g" /tmp/$sig_name-livy/templates/livy-deployment.yaml
        sed -i "s/@SIGNAME/$sig_name/g" /tmp/$sig_name-livy/Chart.yaml

        export HELM_HOST={{ template "helmHost" . }}
        source /var/tmp/cfc/setupHelmCLI.sh
        helm install /tmp/$sig_name-livy/ --name $sig_name-livy --namespace $namespace {{ template "helmFlag" . }}
        rm -rf /tmp/$sig_name-livy

  updateK8sServiceForSig.sh: |-
        topdir=/opt/ibm/spectrumcomputing
        exec 1<>$topdir/conductorspark/logs/updateK8sServiceForSig.log
        exec 2>&1
        sig_name={{ .Release.Name }}-$1
        namespace=$2
        batchmasters=$3
        notebookmasters=$4
        if [ "x$1" = "x" ]; then
          echo "Missed to specify a SIG name, exit."
          exit -1
        fi
        if [ "x$namespace" = "x" ]; then
          echo "Missed to specify the namespace, exit."
          exit -1
        fi
        if [ "x$batchmasters" = "x" -o "x$batchmasters" = "xnull" ]; then
           curl -X PATCH http://127.0.0.1:8001/api/v1/namespaces/$namespace/endpoints/$sig_name -H 'content-type: application/strategic-merge-patch+json' -d '{"subsets":[]}'
        else
          epjson=`curl http://127.0.0.1:8001/api/v1/namespaces/$namespace/endpoints/$sig_name|sed 's/[[:space:]]//g'`

          sparkhosts=`echo $batchmasters|awk -F ';' '{for(i=1;i<=NF;i++)print $i}'`
          ipchanged=yes
          portchanged=yes
          for shost in $sparkhosts
          do
            {{- if eq .Values.cluster.proxyOption "IngressProxy" }}
            sparkport=`echo $shost|awk -F "-" {'print $NF'}`
            tmp=`echo $shost|cut -d "/" -f4|cut -d "-" -f 2-`
            tmp2="-$sparkport"
            sparkhost=${tmp%$tmp2*}
            {{- else }}
            sparkhost=`echo $shost|awk -F ':' '{print $2}'|cut -d "/" -f 3`
            sparkport=`echo $shost|awk -F ':' '{print $3}'`
            {{- end }}
            ip=`cat /etc/hosts|grep $sparkhost|head -n 1|awk '{print $1}'`
            if [ x`echo $epjson|grep \"ip\":\"$ip\"` != x ]; then
              ipchanged=no
              if [ x`echo $epjson|grep \"port\":$sparkport` != x ]; then
                 portchanged=no
              fi
              break
            fi
          done
          if [ $ipchanged = yes -o $portchanged = yes ]; then
             jsonstr=$(echo {\"subsets\":[\{\"addresses\":[\{\"ip\":\"$ip\"\}],\"ports\":[\{\"name\":\"spark-master\",\"port\":$sparkport,\"protocol\":\"TCP\"\}\]\}\]\})
             curl -X PATCH http://127.0.0.1:8001/api/v1/namespaces/$namespace/endpoints/$sig_name -H 'content-type: application/strategic-merge-patch+json' --data $jsonstr
             if [ $portchanged = yes ]; then
               jstr=$(echo {\"spec\":\{\"ports\":[\{\"name\":\"spark-master\",\"port\":7077,\"protocol\":\"TCP\",\"targetPort\":$sparkport\}\]\}\})
               curl -X PATCH http://127.0.0.1:8001/api/v1/namespaces/$namespace/services/$sig_name -H 'content-type: application/strategic-merge-patch+json' --data $jstr
             fi
          fi
        fi

  sig-template_Chart.yaml: |-
        name: @SIGNAME
        version: {{.Chart.Version}}
        description: IBM Spectrum Conductor Spark Instance Group
        home: https://www.ibm.com/support/knowledgecenter/SSZU2E/product_welcome_conductorspark.html
        icon: https://www.ibm.com/developerworks/community/groups/service/html/image?communityUuid=281605c9-7369-46dc-ad03-70d9ad377480&lastMod=1464891144123&showDefaultForNoPermissions=true
        keywords:
        - Conductor
        - Spark Instance Group
        appVersion: 2.3.0
        maintainers:
        - name: IBM Spectrum Conductor Team
  sig-template_templates_cws-slave-deployment.yaml: |-
        @[@[- if @new_namespace @]@]
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: cws-@[@[ .Release.Name @]@]
          labels:
            heritage: {{.Release.Service | quote }}
            release: {{.Release.Name | quote }}
            chart: "{{.Chart.Name}}"
            app: @SIGNAME
        ---
        kind: Role
        apiVersion: rbac.authorization.k8s.io/v1beta1
        metadata:
          name: cws-@[@[ .Release.Name @]@]
          labels:
            heritage: {{.Release.Service | quote }}
            release: {{.Release.Name | quote }}
            chart: "{{.Chart.Name}}"
            app: @SIGNAME
        rules:
        - apiGroups: [""]
          resources: ["pods"]
          verbs: ["create","delete","get","list","patch","update","watch"]
        - apiGroups: [""]
          resources: ["secrets"]
          verbs: ["get"]
        - apiGroups: ["extensions"]
          resources: ["deployments", "deployments/scale"]
          verbs: ["create","delete","get","list","patch","update","watch"]
        ---
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: Role
        metadata:
          name: privileged-@[@[ .Release.Name @]@]
          labels:
            heritage: {{.Release.Service | quote }}
            release: {{.Release.Name | quote }}
            chart: "{{.Chart.Name}}"
            app: @SIGNAME
        rules:
          -
            apiGroups:
              - extensions
            resourceNames:
              - privileged-{{ .Release.Name }}
            resources:
              - podsecuritypolicies
            verbs:
              - use
        ---
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: RoleBinding
        metadata:
          name: privileged-psp-users-@[@[ .Release.Name @]@]
          labels:
            heritage: {{.Release.Service | quote }}
            release: {{.Release.Name | quote }}
            chart: "{{.Chart.Name}}"
            app: @SIGNAME
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: Role
          name: privileged-@[@[ .Release.Name @]@]
        subjects:
          - kind: ServiceAccount
            name: cws-@[@[ .Release.Name @]@]
        ---
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: RoleBinding
        metadata:
          name: cws-@[@[ .Release.Name @]@]
          labels:
            heritage: {{.Release.Service | quote }}
            release: {{.Release.Name | quote }}
            chart: "{{.Chart.Name}}"
            app: @SIGNAME
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: Role
          name: cws-@[@[ .Release.Name @]@]
        subjects:
        - kind: ServiceAccount
          name: cws-@[@[ .Release.Name @]@]
        ---
        {{- if .Values.dli.enabled }}
        @[@[- if @need_create_pvc @]@]
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: {{ .Release.Name }}-dl
          labels:
            heritage: {{.Release.Service | quote }}
            release: {{.Release.Name | quote }}
            chart: "{{.Chart.Name}}-{{.Chart.Version}}"
            app: {{ template "master-fullname" . }}
        spec:
          {{- if .Values.cluster.useDynamicProvisioning }}
          storageClassName: {{ default nil .Values.dli.frameworksStorageClassName | quote }}
          {{- else }}
          storageClassName: {{ default "" .Values.dli.frameworksStorageClassName | quote }}
          {{- end }}
          accessModes:
            - ReadOnlyMany
          resources:
           requests:
             storage: 4Gi
        ---
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: {{ .Release.Name }}-mygpfs
          labels:
            heritage: {{.Release.Service | quote }}
            release: {{.Release.Name | quote }}
            chart: "{{.Chart.Name}}-{{.Chart.Version}}"
            app: {{ template "master-fullname" . }}
        spec:
          {{- if .Values.cluster.useDynamicProvisioning }}
          storageClassName: {{ default nil .Values.dli.sharedFsStorageClassName | quote }}
          {{- else }}
          storageClassName: {{ default "" .Values.dli.sharedFsStorageClassName | quote }}
          {{- end }}
          accessModes:
            - ReadWriteMany
          resources:
           requests:
             storage: 4Gi
        @[@[- end @]@]
        {{- end }}
        ---
        @[@[- end @]@]
        apiVersion: v1
        kind: Secret
        metadata:
          name: @[@[ .Release.Name @]@]-registrykey
          labels:
            heritage: {{.Release.Service | quote }}
            release: {{.Release.Name | quote }}
            chart: "{{.Chart.Name}}"
            app: @SIGNAME
        data:
          .dockerconfigjson: {{ template "cwsImagePullSecret" . }}
        type: kubernetes.io/dockerconfigjson
        ---
        apiVersion: v1
        kind: Secret
        metadata:
          name: sigregistrykey-@[@[ .Release.Name @]@]
          labels:
            heritage: {{.Release.Service | quote }}
            release: {{.Release.Name | quote }}
            chart: "{{.Chart.Name}}"
            app: @SIGNAME
        data:
          .dockerconfigjson: {{ template "sigImagePullSecret" . }}
        type: kubernetes.io/dockerconfigjson
        ---
        @[@[- if @storage_class_name @]@]
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: "@SIGNAME-sig-share"
          labels:
            env: cws-share
            heritage: {{.Release.Service | quote }}
            release: {{.Release.Name | quote }}
            chart: "{{.Chart.Name}}"
            app: @SIGNAME
        spec:
          storageClassName: "@[@[ @storage_class_name @]@]"
          accessModes:
            - ReadWriteMany
          resources:
            requests:
              storage: 3Gi
        ---
        @[@[- end @]@]
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: @SIGNAME-bootstrap
          labels:
            heritage: {{.Release.Service | quote }}
            release: {{.Release.Name | quote }}
            chart: "{{.Chart.Name}}"
            app: @SIGNAME
        data:
          cpImageToContainer.sh: |-
              image=$1
              @[@[- range $i, $di := .Values.dataimages @]@]
              if [ x"$image" = x@[@[$di.imagename@]@] ]; then
              @[@[- range $index, $volume := $di.volumes @]@]
              @[@[- range $, $v := $volume.hostpaths @]@]
              @[@[- if eq $v.type "File" @]@]
              cp @[@[$v.hostpath@]@] /@[@[ $di.imagename | replace ":" "-" @]@]-@[@[$index@]@]
              @[@[- else if eq $v.type "Directory" @]@]
              cp -r @[@[trimSuffix "/" $v.hostpath@]@]/* /@[@[ $di.imagename | replace ":" "-" @]@]-@[@[$index@]@]
              @[@[- end @]@]
              @[@[- end @]@]
              @[@[- end @]@]
              fi
              @[@[- end @]@]

          generate_ssl.sh: |-
              topdir=/opt/ibm/spectrumcomputing
              exec 1<>$topdir/generatessl.log
              exec 2>&1
              set -x
              CLUSTERADMIN=root
              domain=`dnsdomainname`
              if [ "$domain" = "" ]; then
                domain=`hostname -f`
              else
                domain="*.$domain"
              fi
              . $topdir/jre/profile.jre
              dnsname=`hostname -f`
              cd $topdir/security
              rm -f tier2and3ServerKeyStore.jks tier*
              egojre=$topdir/jre
              egokeytool=`find $egojre -name keytool`
              $egokeytool -genkeypair -noprompt -alias tier2alias -dname "CN=$domain,O=IBM,C=CA" -keystore tier2and3ServerKeyStore.jks -storepass SparkPassword -keypass tier2passwd -keyalg rsa -validity 1095 -keysize 2048 -sigalg SHA256withRSA -ext "san=dns:$dnsname"
              $egokeytool -certreq -alias tier2alias -file tier2alias.csr -storepass SparkPassword -keypass tier2passwd -keystore tier2and3ServerKeyStore.jks -ext "san=dns:$dnsname"
              $egokeytool -gencert -infile tier2alias.csr -outfile tier2aliascertcasigned.pem -alias caalias -keystore caKeyStore.jks -storepass Liberty -validity 1095 -ext "san=dns:$dnsname"
              $egokeytool -importcert -noprompt -alias caalias -file cacert.pem -keystore tier2and3ServerKeyStore.jks -storepass SparkPassword
              $egokeytool -import -noprompt -alias tier2alias -file tier2aliascertcasigned.pem -storepass SparkPassword -keypass tier2passwd -keystore tier2and3ServerKeyStore.jks
              $egokeytool -genkeypair -noprompt -alias tier3alias -dname "CN=$domain,O=IBM,C=CA" -keystore tier2and3ServerKeyStore.jks -storepass SparkPassword -keypass tier3passwd -keyalg rsa -validity 1095 -keysize 2048 -sigalg SHA256withRSA -ext "san=dns:$dnsname"
              $egokeytool -certreq -alias tier3alias -file tier3alias.csr -storepass SparkPassword -keypass tier3passwd  -keystore tier2and3ServerKeyStore.jks -ext "san=dns:$dnsname"
              $egokeytool -gencert -infile tier3alias.csr -outfile tier3aliascertcasigned.pem -alias caalias -keystore caKeyStore.jks -storepass Liberty -validity 1095 -ext "san=dns:$dnsname"
              $egokeytool -import -noprompt -alias tier3alias -file tier3aliascertcasigned.pem -storepass SparkPassword -keypass tier3passwd -keystore tier2and3ServerKeyStore.jks
              #Convert Tier 3 keys in Java Keystore to OpenSSL PKCS12 format
              $egokeytool -importkeystore -srckeystore tier2and3ServerKeyStore.jks -srcalias tier3alias -srcstoretype jks -srcstorepass SparkPassword -srckeypass tier3passwd -destkeystore tier3KeyStore.p12 -deststoretype pkcs12 -deststorepass tier3passwd -destkeypass tier3passwd -noprompt
              openssl pkcs12 -in tier3KeyStore.p12 -passin pass:tier3passwd -nocerts -out tier3opensslprivate.key -passout pass:tier3passwd
              openssl pkcs12 -in tier3KeyStore.p12 -passin pass:tier3passwd -clcerts -nokeys -out tier3opensslpublic.pem
              chmod 444 $topdir/security/tier2and3ServerKeyStore.jks
              chmod 444 $topdir/security/tier3KeyStore.p12
              chown -Rh $CLUSTERADMIN:$CLUSTERADMIN $topdir/security
              #=========
              cd $topdir/wlp/usr/shared/resources/security
              rm -f servercertcasigned.pem serverKeyStore.jks srvcertreq.csr serverTrustStore.jks
              $egokeytool -genkeypair -noprompt -alias srvalias -dname "CN=$domain,O=IBM,C=CA" -keystore serverKeyStore.jks -storepass Liberty -keypass Liberty -keyalg rsa -validity 1095 -keysize 2048 -sigalg SHA256withRSA -ext "san=dns:$dnsname"
              $egokeytool -certreq -alias srvalias -file srvcertreq.csr -storepass Liberty -keystore serverKeyStore.jks -ext "san=dns:$dnsname"
              $egokeytool -gencert -infile srvcertreq.csr -outfile servercertcasigned.pem -alias caalias -keystore caKeyStore.jks -storepass Liberty -validity 1095 -ext "san=dns:$dnsname"
              $egokeytool -importcert -noprompt -alias caalias -file cacert.pem -keystore serverKeyStore.jks -storepass Liberty
              $egokeytool -import -noprompt -alias srvalias -file servercertcasigned.pem -storepass Liberty -keystore serverKeyStore.jks
              $egokeytool -importcert -noprompt -alias srvalias -file cacert.pem -keystore serverTrustStore.jks -storepass Liberty
              #==========
              {{- if .Values.dli.enabled }}
              # create certificate and key to enable https for monitor and optimizer Flask server
              CWS_TOP=$topdir
              #DLMAO_HOME=/opt/ibm/spectrumcomputing/dli/dlmao

              KEYSTR=$CWS_TOP/wlp/usr/shared/resources/security/serverKeyStore.jks
              if [ -z "$KEYSTRPASS" ]; then
                KEYSTRPASS=Liberty
              fi
              #DESTKEYSTR_DIR=$DLMAO_HOME/conf
              DESTKEYSTR_DIR=/opt/ibm/spectrumcomputing/dli/conf/dlinsights
              DESTKEYSTR=$DESTKEYSTR_DIR/serverKeyStore.p12
              DESTCRT=$DESTKEYSTR_DIR/srv.crt
              DESTKEY=$DESTKEYSTR_DIR/srv.key
              keytool -importkeystore -srckeystore $KEYSTR -srcstorepass $KEYSTRPASS -destkeystore $DESTKEYSTR \
              -deststoretype PKCS12 -srckeypass $KEYSTRPASS -destkeypass $KEYSTRPASS -deststorepass $KEYSTRPASS \
              -noprompt -srcalias srvalias -storepass $KEYSTRPASS
              openssl pkcs12 -in $DESTKEYSTR -nokeys -out $DESTCRT -password pass:$KEYSTRPASS
              openssl pkcs12 -in $DESTKEYSTR -nodes -nocerts -out $DESTKEY -password pass:$KEYSTRPASS
              #Important: secure the key
              rm -f $DESTKEYSTR
              chmod 400 $DESTCRT $DESTKEY
              chown $CLUSTERADMIN:$CLUSTERADMIN $DESTCRT $DESTKEY
              {{- end }}
          getMasterIP.sh: |-
              masterIP=
              while [ -z $masterIP ]
              do
                sleep 3
                master=`curl {{ template "etcdHostDir" .}}/{{ template "master-fullname" . }} | grep value`
                if [ x"$master" != x ];then
                  masterIP=`echo $master | python -c 'import json,sys;obj=json.load(sys.stdin); print obj["node"]["value"]'`
                fi
              done
              echo $masterIP
          enableLdapLogon: |-
            topdir=/opt/ibm/spectrumcomputing
            sed -i '/EGO_SEC_PLUGIN/d' $topdir/kernel/conf/ego.conf
            echo "EGO_SEC_PLUGIN=sec_ego_pam_default" >> $topdir/kernel/conf/ego.conf
            if [ "x$LDAP_SERVER_IP" = "x" -o "x$BASE_DN" = "x" ]; then
              exit 0
            fi
            disabled=`grep @Ldap_server /etc/nslcd.conf`
            if [ "x$disabled" != "x" ]; then
              sed -i "s/@Ldap_server/$LDAP_SERVER_IP/g" /etc/nslcd.conf
              sed -i "s/@Ldap_server/$LDAP_SERVER_IP/g" /etc/ldap.conf
              sed -i "s/@Base_DN/$BASE_DN/g" /etc/nslcd.conf
              sed -i "s/@Base_DN/$BASE_DN/g" /etc/ldap.conf
              auth-client-config -t nss -p lac_ldap
              echo "session required pam_mkhomedir.so skel=/etc/skel umask=0022" >> /etc/pam.d/common-session
              update-rc.d nslcd enable
              cp /etc/pam.d/common-password /etc/pam.d/common-password.bak
              sed -i 's/use_authtok//' /etc/pam.d/common-password
              /etc/init.d/nscd restart
            fi
          elim.sig: |-
              #!/bin/bash
              while [ true ]
              do
                echo "1 sigName @SIG_NAME"
                sleep 3600
              done
          startSlave.sh: |-
              topdir=/opt/ibm/spectrumcomputing
              exec 1<>$topdir/startslave.log
              exec 2>&1
              set -x
              cd /bin
              rm -rf sh
              ln -s bash sh
              TZ=`date +%z`
              echo "GMT$TZ" >/etc/timezone
              # workaroud to delete the zero byte Nvidia binaries for PAIE
              {{- if eq (include "isPAIE" .) "yes" }}
              rm -rf /usr/bin/nvidia*
              {{- end }}
              # workaroud to help get ego linux version
              binarytypename=`cat $topdir/kernel/conf/profile.ego |grep "BINARY_TYPE = \"fail\""|awk '{print $3}'|awk -F$ '{print $2 }'`
              egoversion=`ls $topdir/|grep 3`
              binarytype=`ls $topdir/$egoversion |grep linux`
              sed -i "/BINARY_TYPE = \"fail\"/i\\$binarytypename=$binarytype" $topdir/kernel/conf/profile.ego

              echo "export PATH=$PATH" >> /etc/profile
              sh /var/tmp/cfc/enableLdapLogon
              ldconfig

              # create the deploy home in advance
              userInfo=`id @EXECUTOR`
              if [ $? -ne 0 ]; then
                  echo "executor:@EXECUTOR is not a valid user"
                  exit -1
              fi
              group=`echo $userInfo| awk 'BEGIN{FS="("} {print $3}'|cut -d ")" -f1`
              home=`getent passwd|grep "^@EXECUTOR:"|cut -d ":" -f 6`
              #init user's home environment
              su @EXECUTOR << EOF
              exit
              EOF
              if [ ! -d $home/@SIG_NAME ]; then
                mkdir -p $home/@SIG_NAME
                chown -R @EXECUTOR:$group $home
              fi
              home_ownership=`stat -c "%U:%G" $home/@SIG_NAME`
              if [ "@EXECUTOR:$group" != "$home_ownership" ]; then
                chown -R @EXECUTOR:$group $home/@SIG_NAME
              fi

              if [ ! -d /tmp/@EXECUTOR ]; then
                mkdir -p /tmp/@EXECUTOR
                chown @EXECUTOR:$group /tmp/@EXECUTOR
              fi
              tmp_ownership=`stat -c "%U:%G" /tmp/@EXECUTOR`
              if [ "@EXECUTOR:$group" != "$tmp_ownership" ]; then
                chown @EXECUTOR:$group /tmp/@EXECUTOR
              fi

              if [ ! -d /var/shareDir/ha ]; then
                mkdir -p /var/shareDir/ha
                chmod -R 777 /var/shareDir/ha
              fi

              sh /var/tmp/cfc/generate_ssl.sh
              echo $(sh /var/tmp/cfc/getMasterIP.sh) {{ template "master-fullname" . }}.{{ template "master-fullname" . }}.{{.Release.Namespace}}.svc.cluster.local {{ template "master-fullname" . }} >> /etc/hosts
              sed -i "s/iCluster_docker/{{ .Release.Name }}/g" $topdir/eservice/esc/conf/services/elk_elasticsearch.xml
              sed -i "s/iCluster_docker/{{ .Release.Name }}/g" $topdir/eservice/esc/conf/services/elk_elasticsearch_master.xml
              sed -i "s/iCluster_docker/{{ .Release.Name }}/g" $topdir/eservice/esc/conf/services/elk_manager.xml
              sed -i "s/iCluster_docker/{{ .Release.Name }}/g" $topdir/eservice/esc/conf/services/elk_elasticsearch_data.xml
              sed -i "s/iCluster_docker/{{ .Release.Name }}/g" $topdir/kernel/conf/ego.shared
              mv $topdir/kernel/conf/ego.cluster.iCluster_docker $topdir/kernel/conf/ego.cluster.{{ .Release.Name }}
              . $topdir/profile.platform
              echo "EGO_DYNAMIC_HOST_WAIT_TIME=1" >> $EGO_CONFDIR/ego.conf
              echo "EGO_RESOURCE_UPDATE_INTERVAL=1" >> $EGO_CONFDIR/ego.conf
              egoconfig join {{ template "master-fullname" . }} -f
              cp /var/tmp/cfc/elim.sig $EGO_SERVERDIR/elim.sig
              chmod +x $EGO_SERVERDIR/elim.sig
              egosh ego start
              #record self IP and name to ETCD
              curl -X PUT {{ template "etcdHostDir" .}}/$(hostname) -d value="$(hostname -i|awk '{print $1}')" -d ttl=30
              curl -X PUT {{ template "etcdHostDir" .}}/$(hostname).$(hostname).{{.Release.Namespace}}.svc.cluster.local -d value="$(hostname -i|awk '{print $1}')" -d ttl=30
              cp /etc/hosts /hosts.original
              sed -i '/{{ template "master-fullname" . }}.{{ template "master-fullname" . }}.{{.Release.Namespace}}.svc.cluster.local/'d /hosts.original
              while [ true ]
              do
                  sh /var/tmp/cfc/appendEtcHostfromShare.sh
                  sh /var/tmp/cfc/upgradeSIG.sh
                  sleep 10
              done
          appendEtcHostfromShare.sh: |-
              #set -x
              if [ -f /hosts.tmp ]; then
                  rm -f /hosts.tmp
              fi
              cat /hosts.original >> /hosts.tmp
              masterIP=`cat /etc/hosts | grep {{ template "master-fullname" . }}.{{ template "master-fullname" . }}.{{.Release.Namespace}}.svc.cluster.local | awk '{print $1}'`
              curl -X PUT {{ template "etcdHostDir" .}}/$(hostname) -d value="$(hostname -i|awk '{print $1}')" -d ttl=20
              curl -X PUT {{ template "etcdHostDir" .}}/$(hostname).$(hostname).{{.Release.Namespace}}.svc.cluster.local -d value="$(hostname -i|awk '{print $1}')" -d ttl=20
              curl --silent  {{ template "etcdHostDir" .}} | python -c 'import json,sys;obj=json.load(sys.stdin); print "\n".join([":".join([x["value"], x["key"]]) for x in obj["node"]["nodes"]])' | sed "s/\/cwsnodemap\///" > /hostlist.yaml
              tac /hostlist.yaml | while read -r line
              do
                  ip=$(echo $line | cut -f 1 -d':' | xargs)
                  host=$(echo $line | cut -f 2 -d':' | xargs)
                  if [ ! -z "$host" ]; then
                      ifit=$(grep "$ip  ${host}$" /hosts.tmp)
                      if [ -z "$ifit" ]; then
                          echo "get new host line - $line"
                          echo "$ip  $host"  >> /hosts.tmp
                          if [ "$host" = {{ template "master-fullname" . }}.{{ template "master-fullname" . }}.{{.Release.Namespace}}.svc.cluster.local -a x"$ip" != x"$masterIP" ]; then
                              echo "EGO master node {{ template "master-fullname" . }}.{{ template "master-fullname" . }}.{{.Release.Namespace}} IP changed from $masterIP to $ip, restart ego"
                              . /opt/ibm/spectrumcomputing/profile.platform
                              egosh ego restart -f
                          fi
                      fi
                  fi
              done
              cat /hosts.tmp > /etc/hosts

          startLivy.sh: |-
                livydir=$1
                echo "livy.spark.deploy-mode = client" > $livydir/conf/livy.conf
                echo "livy.repl.enable-hive-context = true" >> $livydir/conf/livy.conf
                echo "livy.server.port = 8998" >> $livydir/conf/livy.conf
                echo "livy.spark.master = spark://@SIGNAME.@[@[.Release.Namespace@]@]:7077" >> $livydir/conf/livy.conf
                spark_dir=`ls /opt/sparkhome/|grep '^spark-[0-9].[0-9]'`
                export SPARK_HOME=/opt/sparkhome/$spark_dir
                oldsparkhome=`cat $SPARK_HOME/conf/spark-env.sh|grep SPARK_EGO_NATIVE_LIBRARY|cut -d "=" -f2`
                oldsparkhome=${oldsparkhome%$spark_dir*}
                oldsparkhome=${oldsparkhome//\//\\/}
                sed -i "s/$oldsparkhome/\/opt\/sparkhome\//g" $SPARK_HOME/conf/spark-env.sh
                sed -i '/JAVA_HOME/d' $SPARK_HOME/conf/spark-env.sh
                ssl_enabled=`cat $SPARK_HOME/conf/spark-defaults.conf | grep caKeyStore.jks |cut -d " " -f1`
                history_log=`cat $SPARK_HOME/conf/spark-defaults.conf | grep "spark.eventLog.dir" |cut -d " " -f2`
                if [ x$history_log != x ]; then
                    history_log=${history_log:7}
                    mkdir -p $history_log
                fi
                if [ x$ssl_enabled != x ]; then
                  sed -i "/caKeyStore.jks/d" $SPARK_HOME/conf/spark-defaults.conf
                  echo "spark.ego.ssl.rpc.client.keyStore /opt/sparkhome/caKeyStore.jks" >> $SPARK_HOME/conf/spark-defaults.conf
                  sed -i "s/\/opt\/ibm\/spectrumcomputing\/security/\/opt\/sparkhome/g" $SPARK_HOME/conf/spark-defaults.conf
                  domain=`dnsdomainname`
                  if [ "$domain" = "" ]; then
                    domain=`hostname -f`
                  else
                    domain="*.$domain"
                  fi
                  dnsname=`hostname -f`
                  cd /opt/sparkhome
                  egokeytool="keytool"
                  $egokeytool -genkeypair -noprompt -alias tier2alias -dname "CN=$domain,O=IBM,C=CA" -keystore tier2and3ServerKeyStore.jks -storepass SparkPassword -keypass tier2passwd -keyalg rsa -validity 1095 -keysize 2048 -sigalg SHA256withRSA -ext "san=dns:$dnsname"
                  $egokeytool -certreq -alias tier2alias -file tier2alias.csr -storepass SparkPassword -keypass tier2passwd -keystore tier2and3ServerKeyStore.jks -ext "san=dns:$dnsname"
                  $egokeytool -gencert -infile tier2alias.csr -outfile tier2aliascertcasigned.pem -alias caalias -keystore caKeyStore.jks -storepass Liberty -validity 1095 -ext "san=dns:$dnsname"
                  $egokeytool -importcert -noprompt -alias caalias -file cacert.pem -keystore tier2and3ServerKeyStore.jks -storepass SparkPassword
                  $egokeytool -import -noprompt -alias tier2alias -file tier2aliascertcasigned.pem -storepass SparkPassword -keypass tier2passwd -keystore tier2and3ServerKeyStore.jks
                  $egokeytool -genkeypair -noprompt -alias tier3alias -dname "CN=$domain,O=IBM,C=CA" -keystore tier2and3ServerKeyStore.jks -storepass SparkPassword -keypass tier3passwd -keyalg rsa -validity 1095 -keysize 2048 -sigalg SHA256withRSA -ext "san=dns:$dnsname"
                  $egokeytool -certreq -alias tier3alias -file tier3alias.csr -storepass SparkPassword -keypass tier3passwd  -keystore tier2and3ServerKeyStore.jks -ext "san=dns:$dnsname"
                  $egokeytool -gencert -infile tier3alias.csr -outfile tier3aliascertcasigned.pem -alias caalias -keystore caKeyStore.jks -storepass Liberty -validity 1095 -ext "san=dns:$dnsname"
                  $egokeytool -import -noprompt -alias tier3alias -file tier3aliascertcasigned.pem -storepass SparkPassword -keypass tier3passwd -keystore tier2and3ServerKeyStore.jks
                  chmod 444 tier2and3ServerKeyStore.jks
                fi
                /$livydir/bin/livy-server &
                while [ true ]
                do
                   curl -X PUT {{ template "etcdHostDir" .}}/$(hostname) -d value="$(hostname -i|awk '{print $1}')" -d ttl=20
                   sleep 10
                done
          commitSIG.sh: |-
                topdir=/opt/ibm/spectrumcomputing
                exec 1<>$topdir/conductorspark/logs/commitSIG.log
                exec 2>&1
                set -x
                touch $topdir/conductorspark/work/ascd_pkg_deployed
                echo "Commit the SIG image to a registry."
                sig_name=@SIGNAME
                if [ "$sig_name" = "" ]; then
                  echo "Missed to specify a SIG name, exit."
                  exit -1
                fi
                hostname=`hostname`
                sigimage={{.Values.sig.registry}}/{{template "imageNamespace" . }}/$sig_name
                docker commit $(docker ps |grep $hostname | grep -v pause | grep -v kubectl | awk '{print $1}') $sigimage
                if [ $? -ne 0 ]; then
                    echo "docker commit failed."
                    exit -2
                fi
                docker login -u {{.Values.sig.registryUser}} -p {{.Values.sig.registryPasswd}} {{.Values.sig.registry}} || exit -3
                docker push $sigimage
                if [ $? -ne 0 ]; then
                    echo "docker push failed."
                    exit -4
                fi
                curl -X PUT {{ template "etcdInstanceCreation" .}}/{{.Values.sig.registry}}@@{{template "imageNamespace" . }}@@$sig_name -d value="latest"
                touch $topdir/conductorspark/work/sig_committed
                exit 0
          upgradeSIG.sh: |-
                topdir=/opt/ibm/spectrumcomputing
                exec 1<>$topdir/conductorspark/logs/updateSIG.log
                exec 2>&1
                set -x
                if [ -e $topdir/conductorspark/work/sig_committed ];then
                    rm -rf $topdir/conductorspark/work/sig_committed
                    echo "Replace the deployment image with the SIG one."
                    sig_name=@SIGNAME
                    sigimage={{.Values.sig.registry}}/{{template "imageNamespace" . }}/$sig_name
                    jstr=$(echo {\"spec\":\{\"template\":\{\"spec\":\{\"containers\":[\{\"name\":\"$sig_name\",\"image\":\"$sigimage\"\}]\}\}\}\})
                    retries=0
                    while [ $retries -le 5 ]
                    do
                        curlReturn=$(curl --write-out %{http_code} --silent --output /dev/null -X PATCH  -H 'Content-Type: application/strategic-merge-patch+json' --data $jstr "http://127.0.0.1:8001/apis/extensions/v1beta1/namespaces/@namespace/deployments/$sig_name" )
                        if [ $curlReturn -ne 200 ]; then
                            echo "Patching the deployment image got return code: $curlReturn"
                            sleep 3
                            retries=$(expr $retries + 1)
                        else
                            break
                        fi
                    done
                    if [ $retries -ge 6  ]; then
                        echo "Failed to replace the deployment image. Need a manually update from ICP console."
                    fi
                fi
        ---
        apiVersion: v1
        kind: Service
        metadata:
          name: @SIGNAME
          labels:
            heritage: {{.Release.Service | quote }}
            release: {{.Release.Name | quote }}
            chart: "{{.Chart.Name}}"
            app: @SIGNAME
        spec:
          ports:
            - name: spark-master
              port: 7077
              targetPort: 7077
              protocol: TCP
          type: NodePort
        ---
        apiVersion: v1
        kind: Endpoints
        metadata:
          name: @SIGNAME
          labels:
            heritage: {{.Release.Service | quote }}
            release: {{.Release.Name | quote }}
            chart: "{{.Chart.Name}}"
            app: @SIGNAME
        subsets:
        - notReadyAddresses:
          - ip: 10.0.0.1
          ports:
          - name: spark-master
            port: 7077
            protocol: TCP
        ---
        apiVersion: extensions/v1beta1
        kind: Deployment
        metadata:
          name: @SIGNAME
          labels:
            heritage: {{.Release.Service | quote }}
            release: {{.Release.Name | quote }}
            chart: "{{.Chart.Name}}"
            app: @SIGNAME
        spec:
          replicas: 1
          strategy:
            type: RollingUpdate
          selector:
            matchLabels:
              app: @SIGNAME
              heritage: {{.Release.Service | quote }}
              release: {{.Release.Name | quote }}
              chart: "{{.Chart.Name}}"
          template:
            metadata:
              labels:
                heritage: {{.Release.Service | quote }}
                release: {{.Release.Name | quote }}
                chart: "{{.Chart.Name}}"
                app: @SIGNAME
              annotations:
                {{- if .Values.dli.enabled }}
                productName: "@SIGNAME:IBM Spectrum Conductor Deep Learning Impact"
                productVersion: "1.2"
                productID: "IBMSpectrumDLI-Eval"
                {{- else }}
                productName: "@SIGNAME:IBM Spectrum conductor"
                productVersion: "2.3"
                productID: "IBMSpectrumConductor-Eval"
                {{- end }}
            spec:
              @[@[- if @new_namespace @]@]
              serviceAccountName: cws-@[@[ .Release.Name @]@]
              @[@[- else @]@]
              serviceAccountName: cws-{{ .Release.Name }}
              @[@[- end @]@]
              imagePullSecrets:
                - name: @[@[ .Release.Name @]@]-registrykey
                - name: sigregistrykey-@[@[ .Release.Name @]@]
              affinity:
              {{- include "nodeaffinity" . | indent 14 }}
              containers:
                - args:
                  - /kubectl
                  - proxy
                  - -p
                  - "8001"
                  image: {{ template "kubectlImage" . }}
                  resources:
                    requests:
                      cpu: 0.5
                      memory: 256Mi
                    limits:
                      cpu: 0.5
                      memory: 256Mi
                  imagePullPolicy: IfNotPresent
                  name: kubectlproxy
                  lifecycle:
                    preStop:
                      exec:
                         command: ["sh", "-c", "sleep 30"]
                - name: @SIGNAME
                  image: {{template "cwsImage" .}}
                  imagePullPolicy: IfNotPresent
                  ports:
                    - containerPort: 33333
                  command: ["/bin/bash","-c"]
                  args: ["$(bash /var/tmp/cfc/startSlave.sh)"]
                  livenessProbe:
                    tcpSocket:
                      port: 17869
                    initialDelaySeconds: 120
                    periodSeconds: 10
                  securityContext:
                    capabilities:
                      add:
                        - SETGID
                        - SETUID
                        - SYS_CHROOT
                        - SYS_ADMIN
                        - SYS_NICE
                        - SYS_RESOURCE
                        - SYS_TIME
                        - NET_BROADCAST
                        - NET_ADMIN
                        - LEASE
                  env:
                    - name: LDAP_SERVER_IP
                      value: {{.Values.cluster.ldapServerIp}}
                    - name: BASE_DN
                      value: {{.Values.cluster.ldapBaseDn}}
                  ports:
                  resources:
                    requests:
                      cpu: "@CPUREQUEST"
                      memory: "@MEMREQUEST"
                    limits:
                      {{- if or .Values.dli.enabled (gt (.Values.sig.gpu|int) 0) }}
                      {{- if eq (include "global.icpVersion" .) "2.x" }}
                      alpha.kubernetes.io/nvidia-gpu: "@GPUREQUEST"
                      {{- else }}
                      nvidia.com/gpu: "@GPUREQUEST"
                      {{- end }}
                      {{- end }}
                      cpu: "@CPUREQUEST"
                      memory: "@MEMREQUEST"
                  volumeMounts:
                      {{- if .Values.dli.enabled }}
                      {{- if eq (include "isPAIE" .) "no" }}
                      - mountPath: /opt/DL
                        name: dldir
                      {{- end }}
                      - mountPath: /mygpfs
                        name: mygpfs
                      {{- end }}
                      @[@[- if or @storage_class_name (not @new_namespace) @]@]
                      - mountPath: /var/shareDir
                        name: persistsharedir
                      @[@[- end @]@]
                      - mountPath: /var/tmp/cfc
                        name: conductor-slave-bootstrap
                      - mountPath: /var/run/docker.sock
                        name: docker-sock
                      - mountPath: /dev/shm
                        name: dshm
                      - mountPath: /etc/localtime
                        name: host-timezone
                        readOnly: true
                      @[@[- range $index, $v := .Values.volumes @]@]
                      - name: host-volume-@[@[$index@]@]
                      @[@[- if eq $v.type "File" @]@]
                        mountPath: @[@[trimSuffix "/" $v.containerpath@]@]/@[@[ base $v.hostpath @]@]
                      @[@[- else if eq $v.type "Directory" @]@]
                        mountPath: @[@[$v.containerpath@]@]
                      @[@[- end @]@]
                      @[@[- end @]@]
                      @[@[- range $i, $di := .Values.dataimages @]@]
                      @[@[- range $index, $v := $di.volumes @]@]
                      - name: @[@[ $di.imagename | replace ":" "-" @]@]-@[@[$index@]@]
                        mountPath: @[@[$v.containerpath@]@]
                      @[@[- end @]@]
                      @[@[- end @]@]
              @[@[- if .Values.dataimages @]@]
              initContainers:
                @[@[- range $i, $di := .Values.dataimages @]@]
                - name: @[@[$di.imagename | replace ":" "-"@]@]
                  image: @[@[$di.imagename@]@]
                  imagePullPolicy: IfNotPresent
                  resources:
                    requests:
                      cpu: 0.5
                      memory: 256Mi
                    limits:
                      cpu: 0.5
                      memory: 256Mi
                  volumeMounts:
                  - mountPath: /var/tmp/cfc
                    name: conductor-slave-bootstrap
                  @[@[- range $index, $v := $di.volumes @]@]
                  - name: @[@[ $di.imagename | replace ":" "-" @]@]-@[@[$index@]@]
                    mountPath: /@[@[ $di.imagename | replace ":" "-" @]@]-@[@[$index@]@]
                  @[@[- end @]@]
                  command: ["/bin/sh","-c"]
                  args: ["$(sh /var/tmp/cfc/cpImageToContainer.sh @[@[ $di.imagename@]@])"]
                @[@[- end @]@]
              @[@[- end @]@]
              volumes:
                {{- if .Values.dli.enabled }}
                {{- if eq (include "isPAIE" .) "no" }}
                - name: dldir
                  persistentVolumeClaim:
                    claimName: {{ .Release.Name }}-dl
                {{- end }}
                - name: mygpfs
                  persistentVolumeClaim:
                    claimName: {{ .Release.Name }}-mygpfs
                {{- end }}
                @[@[- if @storage_class_name @]@]
                - name: persistsharedir
                  persistentVolumeClaim:
                    claimName: "@SIGNAME-sig-share"
                @[@[- else if not @new_namespace @]@]
                - name: persistsharedir
                  persistentVolumeClaim:
                    claimName: "{{ .Release.Name }}-cws-share"
                @[@[- end @]@]
                - name: conductor-slave-bootstrap
                  configMap:
                    name: "@SIGNAME-bootstrap"
                - name: docker-sock
                  hostPath:
                     path: /var/run/docker.sock
                - name: dshm
                  emptyDir:
                     medium: Memory
                - name: host-timezone
                  hostPath:
                     path: /etc/localtime
                @[@[- range $index, $v := .Values.volumes @]@]
                - name: host-volume-@[@[$index@]@]
                  hostPath:
                     type: @[@[$v.type@]@]
                     path: @[@[$v.hostpath@]@]
                @[@[- end @]@]
                @[@[- range $i, $di := .Values.dataimages @]@]
                @[@[- range $index, $v := $di.volumes @]@]
                - name: @[@[$di.imagename | replace ":" "-"@]@]-@[@[$index@]@]
                  emptyDir: {}
                @[@[- end @]@]
                @[@[- end @]@]
              terminationGracePeriodSeconds: 30
