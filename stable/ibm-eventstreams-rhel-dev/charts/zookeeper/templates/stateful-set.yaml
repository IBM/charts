###############################################################################
#
# Licensed Materials - Property of IBM
#
# 5737-H33
#
# (C) Copyright IBM Corp. 2018, 2019  All Rights Reserved.
#
# US Government Users Restricted Rights - Use, duplication or
# disclosure restricted by GSA ADP Schedule Contract with IBM Corp.
#
###############################################################################
# Defines the Kubernetes pods that will make up the cluster of ZooKeeper nodes
###############################################################################
{{- include "sch.config.init" (list . "sch.chart.config.values") -}}
{{- include "sch.config.init" (list . "zookeeper.sch.chart.config.values" ) -}}
{{ $namePrefix := .sch.chart.components.zookeeper.statefulSet.name -}}
{{ $name := include "sch.names.fullCompName" (list . $namePrefix ) }}
{{ $statefulSetName := include "sch.names.statefulSetName" (list . $namePrefix) -}}
# Component is 'zookeeper' as this makes up part of implementing the ZooKeeper cluster
{{ $compName := .sch.chart.components.zookeeper.compName -}}
{{ $labels := include "sch.metadata.labels.standard" (list . $compName (dict "serviceSelector" $namePrefix)) -}}
# The headless service that will provide access to the ZooKeeper nodes defined below
{{ $serviceName := .sch.chart.components.zookeeper.headless.name -}}
# Services allowing access to individual ZooKeeper pods - there will be one of these for each node
{{ $zookeeperFixedIPService := .sch.chart.components.zookeeper.fixed.name -}}
{{ $zookeeperFixedIPServiceName := include "sch.names.fullCompName" (list . $zookeeperFixedIPService) -}}
# Secret containing cert and key for pod to pod security
{{ $proxySecretNamePrefix := .sch.chart.components.proxy.secret.name -}}
{{ $proxySecretName := include "sch.names.fullCompName" (list . $proxySecretNamePrefix) | quote -}}
# Service Account to run the ZooKeeper pods as
{{ $serviceAccount := .sch.chart.components.kafka.serviceAccount.name -}}
{{ $serviceAccountName := include "sch.names.fullCompName" (list . $serviceAccount ) -}}
# import port definitions
{{- include "sch.config.init" (list . "ports.sch.chart.config.values") | trim -}}
{{ $ports := .sch.config.ports }}
# Default user for security context
{{ $defaultUser := .sch.securityContext.defaultUser -}}
# length chewcked pvc name
{{ $shortPvcName := include "sch.names.volumeClaimTemplateName" (list . .Values.dataPVC.name $statefulSetName) -}}
# Proxy config map name
{{ $configMap := .sch.chart.components.zookeeper.configMap.name -}}
{{ $configMapName := include "sch.names.fullCompName" (list . $configMap ) }}

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ $statefulSetName | quote }}
  labels:
{{ $labels | indent 4 }}
spec:
  # identify the headless service that will enable access to ZooKeeper nodes
  serviceName: {{ include "sch.names.fullCompName" (list . $serviceName) | quote }}
  # Deployed in parallel to enable faster deployments
  podManagementPolicy: "Parallel"
  # number of nodes in the ZooKeeper cluster
  #  changing this after install may lead to problems with ZooKeeper
  replicas: {{ .sch.config.zookeeper.replicas }}
  # ZooKeeper node pods are uniquely identified based on release name
  selector:
    matchLabels:
      release: {{ .Release.Name | quote }}
      serviceSelector: {{ $namePrefix | quote }}
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      namespace: {{ include "restrict.namespace" (list . .Release.Namespace) }}
      labels:
{{ $labels | indent 8 }}
      annotations:
{{ include "metering" (list . ) | indent 8 }}
    spec:
      serviceAccountName: {{ $serviceAccountName | quote }}
      {{- if .Values.global.image.pullSecret }}
      imagePullSecrets:
        - name: {{ .Values.global.image.pullSecret }}
      {{- end }}
      hostNetwork: false
      hostPID: false
      hostIPC: false
      securityContext:
        runAsNonRoot: true
        runAsUser: {{ $defaultUser }}
{{ include "fsGroupGid" (list . ) | indent 8 }}
      volumes:
        - name: proxy-config-map
          configMap:
            name: {{ $configMapName }}
        - name: es-cert-volume
          secret:
            secretName: {{ $proxySecretName }}
            items:
            - key: podtls.cert
              path: ca-certificates.crt
        {{ if .Values.persistence.enabled -}}
        {{ else }}
        - name: "tempdir"
          emptyDir: {}
        {{ end }}
      {{- include "license.accept.ref" . | indent 6 }}
      affinity:
{{ include "customNodeaffinity"  (list .) | indent 8 }}
        # we don't want multiple co-located ZooKeeper nodes
        #  so this anti-affinity rule should prevent
        #  nodes with the zookeeper name being scheduled on
        #  the same host
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 1
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: "release"
                      operator: In
                      values:
                        -  {{ .Release.Name | quote }}
                    - key: "serviceSelector"
                      operator: In
                      values:
                        - {{ $namePrefix | quote }}
                topologyKey: "kubernetes.io/hostname"
        # we do want to colocate ZooKeeper nodes with Kafka nodes to
        #  help minimise network traffic between them, but this is
        #  requested in the Kafka stateful set definition
      containers:
        - name: "zookeeper"
          image: {{ include "eventstreams.image" (list . "eventstreams-zookeeper" .Values.global.image.imageTags.zookeeperTag) | quote }}
          imagePullPolicy: {{ .Values.global.image.pullPolicy }}
          securityContext:
            privileged: false
            readOnlyRootFilesystem: false
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            # Running as user 65534 for backwards compatability with upgrades
            runAsUser: {{ $defaultUser }}
            capabilities:
              drop:
              - ALL
          resources:
            limits:
              cpu: {{ .Values.resources.limits.cpu }}
              memory: {{ .Values.resources.limits.memory }}
            requests:
              cpu: {{ .Values.resources.requests.cpu }}
              memory: {{ .Values.resources.requests.memory }}
          ports:
            - containerPort: {{ $ports.zookeeper.clientInternal }}
              name: client
            - containerPort: {{ $ports.zookeeper.serverInternal }}
              name: server
            - containerPort: {{ $ports.zookeeper.electionInternal }}
              name: leader-election
          env:
            {{- include "license.accept.env.ref" . | indent 10 }}
            - name: ZK_ENSEMBLE
              value: '{{ range $index, $_ := until (.sch.config.zookeeper.replicas | int) }}{{ $zookeeperFixedIPServiceName }}-{{ $index }}.{{ $.Release.Namespace }};{{ end }}'
            - name: ZK_TICK_TIME
              value: '2000'
            - name: ZK_INIT_LIMIT
              value: '10'
            - name: ZK_SYNC_LIMIT
              value: '5'
            - name: ZK_MAX_CLIENT_CNXNS
              value: '60'
            - name: ZK_SNAP_RETAIN_COUNT
              value: '3'
            - name: ZK_PURGE_INTERVAL
              value: '1'
            - name: ZK_CLIENT_PORT
              value: {{ $ports.zookeeper.clientInternal | quote }}
            - name: ZK_SERVER_PORT
              value: {{ $ports.zookeeper.serverInternal | quote }}
            - name: ZK_ELECTION_PORT
              value: {{ $ports.zookeeper.electionInternal | quote }}
            - name: ZK_REPLICAS
              value: {{ .sch.config.zookeeper.replicas | quote }}
            - name: NAMESPACE
              value: {{ .Release.Namespace | quote }}
            - name: ZK_ONPREM
              value: "true"
          readinessProbe:
            exec:
              command:
                - "zkReady.sh"
            initialDelaySeconds: 15
            periodSeconds: 20
            timeoutSeconds: 15
            failureThreshold: 10
          livenessProbe:
            exec:
              command:
                - "zkOk.sh"
            initialDelaySeconds: 15
            periodSeconds: 20
            timeoutSeconds: 15
            failureThreshold: 10
          volumeMounts:
          {{- if .Values.persistence.enabled }}
            - name: {{ $shortPvcName }}
              mountPath: /var/lib/zookeeper
          {{- else }}
            - name: "tempdir"
              mountPath: /var/lib/zookeeper
          {{ end }}
        # Proxy sidecar
        - name: proxy
          image: {{ include "eventstreams.image" (list . "eventstreams-proxy" .Values.global.image.imageTags.proxyTag) | quote }}
          imagePullPolicy: {{ .Values.global.image.pullPolicy }}
          securityContext:
            privileged: false
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: {{ $defaultUser }}
            capabilities:
              drop:
              - ALL
          resources:
            requests:
{{ toYaml .Values.global.resources.tlsProxy.requests | indent 14 }}
            limits:
{{ toYaml .Values.global.resources.tlsProxy.limits | indent 14 }}
          env:
            {{- include "license.accept.env.ref" . | indent 10 }}
            - name: ES_CONFIG_PATH
              value: {{ .sch.chart.components.zookeeper.configPath | quote }}
            - name: TLS_CERT
              valueFrom:
                secretKeyRef:
                  name: {{ $proxySecretName }}
                  key: "podtls.cert"
            - name: TLS_KEY
              valueFrom:
                secretKeyRef:
                  name: {{ $proxySecretName }}
                  key: "podtls.key"
          ports:
            - containerPort: {{ $ports.zookeeper.client }}
              name: proxy
            - containerPort: {{ $ports.zookeeper.server }}
              name: zk-server
            - containerPort: {{ $ports.zookeeper.election }}
              name: zk-election
            - containerPort: {{ $ports.proxy.health }}
              name: proxy-health
          readinessProbe:
            httpGet:
              path: "/ready"
              port: {{ $ports.proxy.health }}
            initialDelaySeconds: 1
            periodSeconds: 5
            timeoutSeconds: 5
          livenessProbe:
            httpGet:
              path: "/live"
              port: {{ $ports.proxy.health }}
            initialDelaySeconds: 15
            periodSeconds: 15
            timeoutSeconds: 15
          volumeMounts:
          - name: proxy-config-map
            mountPath: {{ .sch.chart.components.zookeeper.configPath | quote }}
            readOnly: true
          - name: es-cert-volume
            mountPath: /etc/ssl/certs
            readOnly: true
{{- if .Values.persistence.enabled }}
  volumeClaimTemplates:
    - metadata:
        name: {{ $shortPvcName }}
        labels:
{{ $labels | indent 10 }}
      spec:
        {{- if .Values.persistence.useDynamicProvisioning }}
        # If present, use the storageClassName from the values.yaml, else use the
        # default storageClass setup by Kubernetes Administrator
        #
        # Setting storageClassName to nil means use the default storage class
        storageClassName: {{ default nil .Values.dataPVC.storageClassName | quote }}
        {{- else }}
        # Disable dynamic provisioning
        storageClassName: {{ default "" .Values.dataPVC.storageClassName | quote }}
        {{- end }}
        accessModes: [ "ReadWriteOnce" ]
        resources:
          requests:
            storage: {{ .Values.dataPVC.size | quote }}
{{ end -}}
