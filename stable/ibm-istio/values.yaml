# Default values for ibm-istio-chart.
# Use with Kubernetes 1.9+
# This is a YAML-formatted file.
# Declare name/value pairs to be passed into your templates.
# name: value

# Common settings.
global:
  # Comma-separated minimum per-scope logging level of messages to output, in the form of <scope>:<level>,<scope>:<level>
  # The control plane has different scopes depending on component, but can configure default log level across all components
  # If empty, default scope and level will be used as configured in code
  logging:
    level: "default:info"

  # monitoring port used by mixer, pilot, galley
  monitoringPort: 15014

  k8sIngress:
    enabled: false
    # Gateway used for k8s Ingress resources. By default it is
    # using 'istio:ingressgateway' that will be installed by setting
    # 'gateways.enabled' and 'gateways.istio-ingressgateway.enabled'
    # flags to true.
    gatewayName: ingressgateway
    # enableHttps will add port 443 on the ingress.
    # It REQUIRES that the certificates are installed in the
    # expected secrets - enabling this option without certificates
    # will result in LDS rejection and the ingress will not work.
    enableHttps: false
  
  # Deploy the Istio remote chart on each remote cluster to connect
  # the central Istio control plane
  istioRemote: false
  createRemoteSvcEndpoints: false
  remotePilotCreateSvcEndpoint: false
  remotePilotAddress: ""
  remotePolicyAddress: ""
  remoteTelemetryAddress: ""
  remoteZipkinAddress: ""

  proxy:
    repository: ibmcom/istio-proxyv2
    tag: 1.1.7

    # cluster domain. Default value is "cluster.local".
    clusterDomain: "cluster.local"

    # Resources for the sidecar.
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 2000m
        memory: 1024Mi

    # Controls number of Proxy worker threads.
    # If set to 0 (default), then start worker thread for each CPU thread/core.
    concurrency: 2

    # Configures the access log for each sidecar.
    # Options:
    #   "" - disables access log
    #   "/dev/stdout" - enables access log
    accessLogFile: ""

    # Configure how and what fields are displayed in sidecar access log. Setting to
    # empty string will result in default log format
    accessLogFormat: ""

    # Configure the access log for sidecar to JSON or TEXT.
    accessLogEncoding: TEXT

    # Log level for proxy, applies to gateways and sidecars.  If left empty, "warning" is used.
    # Expected values are: trace|debug|info|warning|error|critical|off
    logLevel: ""

    # Configure the DNS refresh rate for Envoy cluster of type STRICT_DNS
    # 5 seconds is the default refresh rate used by Envoy
    dnsRefreshRate: 5s

    #If set to true, istio-proxy container will have privileged securityContext
    privileged: false

    # If set, newly injected sidecars will have core dumps enabled.
    enableCoreDump: false

    # Default port for Pilot agent health checks. A value of 0 will disable health checking.
    statusPort: 15020

    # The initial delay for readiness probes in seconds.
    readinessInitialDelaySeconds: 1

    # The period between readiness probes.
    readinessPeriodSeconds: 2

    # The number of successive failed probes before indicating readiness failure.
    readinessFailureThreshold: 30

    # istio egress capture whitelist
    # https://istio.io/docs/tasks/traffic-management/egress.html#calling-external-services-directly
    # example: includeIPRanges: "172.30.0.0/16,172.20.0.0/16"
    # would only capture egress traffic on those two IP Ranges, all other outbound traffic would
    # be allowed by the sidecar
    includeIPRanges: "*"
    excludeIPRanges: ""

    # pod internal interfaces
    #kubevirtInterfaces: ""

    # istio ingress capture whitelist
    # examples:
    #     Redirect no inbound traffic to Envoy:    --includeInboundPorts=""
    #     Redirect all inbound traffic to Envoy:   --includeInboundPorts="*"
    #     Redirect only selected ports:            --includeInboundPorts="80,8080"
    #includeInboundPorts: "*"
    excludeInboundPorts: ""

    # This controls the 'policy' in the sidecar injector.
    autoInject: enabled

    # Sets the destination Statsd in envoy (the value of the "--statsdUdpAddress" proxy argument
    # would be <host>:<port>).
    # Disabled by default.
    # The istio-statsd-prom-bridge is deprecated and should not be used moving forward.
    envoyStatsd:
      # If enabled is set to true, host and port must also be provided. Istio no longer provides a statsd collector.
      enabled: false
      host: # example: statsd-svc.istio-system
      port: # example: 9125

    # Sets the Envoy Metrics Service address, used to push Envoy metrics to an external collector
    # via the Metrics Service gRPC API. This contains detailed stats information emitted directly
    # by Envoy and should not be confused with the the Istio telemetry. The Envoy stats are also
    # available to scrape via the Envoy admin port at either /stats or /stats/prometheus.
    #
    # See https://www.envoyproxy.io/docs/envoy/latest/api-v2/config/metrics/v2/metrics_service.proto
    # for details about Envoy's Metrics Service API. 
    #
    # Disabled by default.
    envoyMetricsService:
      enabled: false
      host: # example: metrics-service.istio-system
      port: # example: 15000

    # Specify which tracer to use. One of: lightstep, zipkin, datadog
    tracer: "zipkin"

  proxy_init:
    # Image for the proxy_init container, used to configure iptables.
    repository: ibmcom/istio-proxy_init
    tag: 1.1.7

  kubectl:
    repository: ibmcom/kubectl
    tag: v1.13.5

  # imagePullPolicy is applied to istio control plane components.
  # local tests require IfNotPresent, to avoid uploading to dockerhub.
  # TODO: Switch to Always as default, and override in the local tests.
  imagePullPolicy: IfNotPresent

  # controlPlaneMtls enabled. Will result in delays starting the pods while secrets are
  # propagated, not recommended for tests.
  controlPlaneSecurityEnabled: false

  # disablePolicyChecks disables mixer policy checks.
  # if mixer.policy.enabled==true then disablePolicyChecks has affect.
  # Will set the value with same name in istio config map - pilot needs to be restarted to take effect.
  disablePolicyChecks: true

  # policyCheckFailOpen allows traffic in cases when the mixer policy service cannot be reached.
  # Default is false which means the traffic is denied when the client is unable to connect to Mixer.
  policyCheckFailOpen: false

  # EnableTracing sets the value with same name in istio config map, requires pilot restart to take effect.
  enableTracing: true

  # Configuration for each of the supported tracers
  tracer:
    # Configuration for envoy to send trace data to LightStep.
    # Disabled by default.
    # address: the <host>:<port> of the satellite pool
    # accessToken: required for sending data to the pool
    # secure: specifies whether data should be sent with TLS
    # cacertPath: the path to the file containing the cacert to use when verifying TLS. If secure is true, this is
    #   required. If a value is specified then a secret called "lightstep.cacert" must be created in the destination
    #   namespace with the key matching the base of the provided cacertPath and the value being the cacert itself.
    #
    lightstep:
      address: ""                # example: lightstep-satellite:443
      accessToken: ""            # example: abcdefg1234567
      secure: true               # example: true|false
      cacertPath: ""             # example: /etc/lightstep/cacert.pem
    zipkin:
      # Host:Port for reporting trace data in zipkin format. If not specified, will default to
      # zipkin service (port 9411) in the same namespace as the other istio components.
      address: ""
    datadog:
      # Host:Port for submitting traces to the Datadog agent.
      address: "$(HOST_IP):8126"

  # Default mtls policy. If true, mtls between services will be enabled by default.
  mtls:
    # Default setting for service-to-service mtls. Can be set explicitly using
    # destination rules or service annotations.
    enabled: false

  # ImagePullSecrets for all ServiceAccount, list of secrets in the same namespace
  # to use for pulling any images in pods that reference this ServiceAccount.
  # For components that don't use ServiceAccounts (i.e. grafana, servicegraph, tracing)
  # ImagePullSecrets will be added to the corresponding Deployment(StatefulSet) objects.
  # Must be set for any clustser configured with private docker registry.
  imagePullSecrets:
    # - private-registry-key

  # Specify architecture (amd64, ppc64le, s390x) and weight to be  used for scheduling as follows:
  #   0 - Do not use
  #   1 - Least preferred
  #   2 - No preference
  #   3 - Most preferred
  arch:
    amd64: 2
    ppc64le: 2
    s390x: 2

  # Whether to restrict the applications namespace the controller manages;
  # If not set, controller watches all namespaces
  oneNamespace: false

  # Default node selector to be applied to all deployments so that all pods can be 
  # constrained to run a particular nodes. Each component can overwrite these default 
  # values by adding its node selector block in the relevant section below and setting 
  # the desired values.
  defaultNodeSelector: {}
  
  # Default node tolerations to be applied to all deployments so that all pods can be 
  # scheduled to a particular nodes. Each component can overwrite these default 
  # values by adding its tolerations block in the relevant section below and setting 
  # the desired values.
  defaultTolerations: []

  # Whether to perform server-side validation of configuration.
  configValidation: true

  # Custom DNS config for the pod to resolve names of services in other
  # clusters. Use this to add additional search domains, and other settings.
  # see
  # https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#dns-config
  # This does not apply to gateway pods as they typically need a different
  # set of DNS settings than the normal application pods (e.g., in
  # multicluster scenarios).
  # NOTE: If using templates, follow the pattern in the commented example below.
  #podDNSSearchNamespaces:
  #- global
  #- "[[ valueOrDefault .DeploymentMeta.Namespace \"default\" ]].global"

  # If set to true, the pilot and citadel mtls will be exposed on the
  # ingress gateway
  meshExpansion:
    enabled: false
    # If set to true, the pilot and citadel mtls and the plain text pilot ports
    # will be exposed on an internal gateway
    useILB: false

  # Include the crd definition when generating the template.
  # For 'helm template' and helm install > 2.10 it should be true, and 
  # you don't need manually create the Istio CRDS;
  # For helm < 2.9, crds must be installed ahead of time with
  # 'kubectl apply -f <path to crds yaml>
  # and this options must be set off.
  crds: true

  # Istio control plane namespace: This specifies where the Istio control
  # plane was installed earlier.  Modify this if you installed the control
  # plane in a different namespace than istio-system.
  istioNamespace: "istio-system"

  omitSidecarInjectorConfigMap: false

  multiCluster:
    # Set to true to connect two kubernetes clusters via their respective
    # ingressgateway services when pods in each cluster cannot directly
    # talk to one another. All clusters should be using Istio mTLS and must
    # have a shared root CA for this model to work.
    enabled: false

  # A minimal set of requested resources to applied to all deployments so that
  # Horizontal Pod Autoscaler will be able to function (if set).
  # Each component can overwrite these default values by adding its own resources
  # block in the relevant section below and setting the desired resources values.
  defaultResources:
    requests:
      cpu: 10m
    #   memory: 128Mi
    # limits:
    #   cpu: 100m
    #   memory: 128Mi

  # enable pod distruption budget for the control plane, which is used to
  # ensure Istio control plane components are gradually upgraded or recovered.
  defaultPodDisruptionBudget:
    enabled: true
    # The values aren't mutable due to a current PodDisruptionBudget limitation
    # minAvailable: 1

  # Kubernetes >=v1.11.0 will create two PriorityClass, including system-cluster-critical and
  # system-node-critical, it is better to configure this in order to make sure your Istio pods
  # will not be killed because of low priority class.
  # Refer to https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
  # for more detail.
  priorityClassName: ""

  # Use the Mesh Control Protocol (MCP) for configuring Mixer and
  # Pilot. Requires galley (`--set galley.enabled=true`).
  useMCP: true

  # The trust domain corresponds to the trust root of a system
  # Refer to https://github.com/spiffe/spiffe/blob/master/standards/SPIFFE-ID.md#21-trust-domain
  # Indicate the domain used in SPIFFE identity URL
  # The default depends on the environment.
  #   kubernetes: cluster.local
  #   else:  default dns domain
  trustDomain: ""

  # Set the default behavior of the sidecar for handling outbound traffic from the application:
  # ALLOW_ANY - outbound traffic to unknown destinations will be allowed, in case there are no
  #   services or ServiceEntries for the destination port
  # REGISTRY_ONLY - restrict outbound traffic to services defined in the service registry as well
  #   as those defined through ServiceEntries
  # ALLOW_ANY is the default in 1.1.  This means each pod will be able to make outbound requests 
  # to services outside of the mesh without any ServiceEntry.
  # REGISTRY_ONLY was the default in 1.0.  If this behavior is desired, set the value below to REGISTRY_ONLY.
  outboundTrafficPolicy:
    mode: ALLOW_ANY

  # The namespace where globally shared configurations should be present.
  # DestinationRules that apply to the entire mesh (e.g., enabling mTLS),
  # default Sidecar configs, etc. should be added to this namespace.
  # configRootNamespace: istio-config

  # set the default set of namespaces to which services, service entries, virtual services, destination
  # rules should be exported to. Currently only one value can be provided in this list. This value
  # should be one of the following two options:
  # * implies these objects are visible to all namespaces, enabling any sidecar to talk to any other sidecar.
  # . implies these objects are visible to only to sidecars in the same namespace, or if imported as a Sidecar.egress.host  
  #defaultConfigVisibilitySettings:
  #- '*'

  sds:
    # SDS enabled. IF set to true, mTLS certificates for the sidecars will be
    # distributed through the SecretDiscoveryService instead of using K8S secrets to mount the certificates.
    enabled: false
    udsPath: ""
    useTrustworthyJwt: false
    useNormalJwt: false

  # Configure the mesh networks to be used by the Split Horizon EDS.
  #
  # The following example defines two networks with different endpoints association methods.
  # For `network1` all endpoints that their IP belongs to the provided CIDR range will be
  # mapped to network1. The gateway for this network example is specified by its public IP
  # address and port.
  # The second network, `network2`, in this example is defined differently with all endpoints
  # retrieved through the specified Multi-Cluster registry being mapped to network2. The
  # gateway is also defined differently with the name of the gateway service on the remote
  # cluster. The public IP for the gateway will be determined from that remote service (not
  # supported yet).
  #
  # meshNetworks:
  #   network1:
  #     endpoints:
  #     - fromCidr: "192.168.0.1/24"
  #     gateways:
  #     - address: 1.1.1.1
  #       port: 80
  #   network2:
  #     endpoints:
  #     - fromRegistry: reg1
  #     gateways:
  #     - registryServiceName: istio-ingressgateway
  #       port: 443
  #
  meshNetworks: {}

  # Specifies the global locality load balancing settings.
  # Locality-weighted load balancing allows administrators to control the distribution of traffic to
  # endpoints based on the localities of where the traffic originates and where it will terminate.
  # Please set either failover or distribute configuration but not both.
  #
  # localityLbSetting:
  #   distribute:
  #   - from: "us-central1/*"
  #     to:
  #       "us-central1/*": 80
  #       "us-central2/*": 20
  #
  # localityLbSetting:
  #   failover:
  #   - from: us-east
  #     to: eu-west
  #   - from: us-west
  #     to: us-east
  localityLbSetting: {}

  # Specifies whether helm test is enabled or not.
  # This field is set to false by default, so 'helm template ...'
  # will ignore the helm test yaml files when generating the template
  enableHelmTest: false

#
# Gateways Configuration
# By default (if enabled) a pair of Ingress and Egress Gateways will be created for the mesh.
# You can add more gateways in addition to the defaults but make sure those are uniquely named
# and that NodePorts are not conflicting.
# Disable specifc gateway by setting the `enabled` to false.
#
gateways:
  enabled: true
  
  istio-ingressgateway:
    enabled: true
    #
    # Secret Discovery Service (SDS) configuration for ingress gateway.
    #
    sds:
      # If true, ingress gateway fetches credentials from SDS server to handle TLS connections.
      enabled: false
      # SDS server that watches kubernetes secrets and provisions credentials to ingress gateway.
      # This server runs in the same pod as ingress gateway.
      image:
        repository: ibmcom/istio-node-agent-k8s
        tag: 1.1.7
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
        limits:
          cpu: 2000m
          memory: 1024Mi
    labels:
      app: istio-ingressgateway
      istio: ingressgateway
    autoscaleEnabled: true
    autoscaleMin: 1
    autoscaleMax: 5
    # specify replicaCount when autoscaleEnabled: false
    # replicaCount: 1
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 2000m
        memory: 1024Mi
    cpu:
      targetAverageUtilization: 80
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    externalIPs: []
    serviceAnnotations: {}
    podAnnotations: {}
    type: LoadBalancer #change to NodePort, ClusterIP or LoadBalancer if need be
    #externalTrafficPolicy: Local #change to Local to preserve source IP or Cluster for default behaviour or leave commented out
    ports:
      ## You can add custom gateway ports
      # Note that AWS ELB will by default perform health checks on the first port
      # on this list. Setting this to the health check port will ensure that health
      # checks always work. https://github.com/istio/istio/issues/12503
    - port: 15020
      targetPort: 15020
      name: status-port
    - port: 80
      targetPort: 80
      name: http2
      nodePort: 31380
    - port: 443
      name: https
      nodePort: 31390
    # Example of a port to add. Remove if not needed
    - port: 31400
      name: tcp
      nodePort: 31400
    ### PORTS FOR UI/metrics #####
    ## Disable if not needed
    - port: 15029
      targetPort: 15029
      name: https-kiali
    - port: 15030
      targetPort: 15030
      name: https-prometheus
    - port: 15031
      targetPort: 15031
      name: https-grafana
    - port: 15032
      targetPort: 15032
      name: https-tracing
      # This is the port where sni routing happens
    - port: 15443
      targetPort: 15443
      name: tls
    #### MESH EXPANSION PORTS  ########
    # Pilot and Citadel MTLS ports are enabled in gateway - but will only redirect
    # to pilot/citadel if global.meshExpansion settings are enabled.
    # Delete these ports if mesh expansion is not enabled, to avoid
    # exposing unnecessary ports on the web.
    # You can remove these ports if you are not using mesh expansion
    meshExpansionPorts:
    - port: 15011
      targetPort: 15011
      name: tcp-pilot-grpc-tls
    - port: 15004
      targetPort: 15004
      name: tcp-mixer-grpc-tls
    - port: 8060
      targetPort: 8060
      name: tcp-citadel-grpc-tls
    - port: 853
      targetPort: 853
      name: tcp-dns-tls
    ####### end MESH EXPANSION PORTS ######
    ##############
    secretVolumes:
    - name: ingressgateway-certs
      secretName: istio-ingressgateway-certs
      mountPath: /etc/istio/ingressgateway-certs
    - name: ingressgateway-ca-certs
      secretName: istio-ingressgateway-ca-certs
      mountPath: /etc/istio/ingressgateway-ca-certs
    ### Advanced options ############

    # Ports to explicitly check for readiness. If configured, the readiness check will expect a
    # listener on these ports. A comma separated list is expected, such as "80,443".
    #
    # Warning: If you do not have a gateway configured for the ports provided, this check will always
    # fail. This is intended for use cases where you always expect to have a listener on the port,
    # such as 80 or 443 in typical setups.
    applicationPorts: ""

    env:
      # A gateway with this mode ensures that pilot generates an additional
      # set of clusters for internal services but without Istio mTLS, to
      # enable cross cluster routing.
      ISTIO_META_ROUTER_MODE: "sni-dnat"
    nodeSelector: {}
    tolerations: []

    # Specify the pod anti-affinity that allows you to constrain which nodes
    # your pod is eligible to be scheduled based on labels on pods that are
    # already running on the node rather than based on labels on nodes.
    # There are currently two types of anti-affinity:
    #    "requiredDuringSchedulingIgnoredDuringExecution"
    #    "preferredDuringSchedulingIgnoredDuringExecution"
    # which denote “hard” vs. “soft” requirements, you can define your values
    # in "podAntiAffinityLabelSelector" and "podAntiAffinityTermLabelSelector"
    # correspondingly.
    # For example:
    # podAntiAffinityLabelSelector:
    # - key: security
    #   operator: In
    #   values: S1,S2
    #   topologyKey: "kubernetes.io/hostname"
    # This pod anti-affinity rule says that the pod requires not to be scheduled
    # onto a node if that node is already running a pod with label having key
    # “security” and value “S1”.
    podAntiAffinityLabelSelector: []
    podAntiAffinityTermLabelSelector: []

  istio-egressgateway:
    enabled: false
    labels:
      app: istio-egressgateway
      istio: egressgateway
    autoscaleEnabled: true
    autoscaleMin: 1
    autoscaleMax: 5
    # specify replicaCount when autoscaleEnabled: false
    # replicaCount: 1
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 2000m
        memory: 256Mi
    cpu:
      targetAverageUtilization: 80
    serviceAnnotations: {}
    podAnnotations: {}
    type: ClusterIP #change to NodePort or LoadBalancer if need be
    ports:
    - port: 80
      name: http2
    - port: 443
      name: https
      # This is the port where sni routing happens
    - port: 15443
      targetPort: 15443
      name: tls
    secretVolumes:
    - name: egressgateway-certs
      secretName: istio-egressgateway-certs
      mountPath: /etc/istio/egressgateway-certs
    - name: egressgateway-ca-certs
      secretName: istio-egressgateway-ca-certs
      mountPath: /etc/istio/egressgateway-ca-certs
    #### Advanced options ########
    env:
      # Set this to "external" if and only if you want the egress gateway to
      # act as a transparent SNI gateway that routes mTLS/TLS traffic to
      # external services defined using service entries, where the service
      # entry has resolution set to DNS, has one or more endpoints with
      # network field set to "external". By default its set to "" so that
      # the egress gateway sees the same set of endpoints as the sidecars
      # preserving backward compatibility
      # ISTIO_META_REQUESTED_NETWORK_VIEW: ""
      # A gateway with this mode ensures that pilot generates an additional
      # set of clusters for internal services but without Istio mTLS, to
      # enable cross cluster routing.
      ISTIO_META_ROUTER_MODE: "sni-dnat"
    nodeSelector: {}
    tolerations: []

    # Specify the pod anti-affinity that allows you to constrain which nodes
    # your pod is eligible to be scheduled based on labels on pods that are
    # already running on the node rather than based on labels on nodes.
    # There are currently two types of anti-affinity:
    #    "requiredDuringSchedulingIgnoredDuringExecution"
    #    "preferredDuringSchedulingIgnoredDuringExecution"
    # which denote “hard” vs. “soft” requirements, you can define your values
    # in "podAntiAffinityLabelSelector" and "podAntiAffinityTermLabelSelector"
    # correspondingly.
    # For example:
    # podAntiAffinityLabelSelector:
    # - key: security
    #   operator: In
    #   values: S1,S2
    #   topologyKey: "kubernetes.io/hostname"
    # This pod anti-affinity rule says that the pod requires not to be scheduled
    # onto a node if that node is already running a pod with label having key
    # “security” and value “S1”.
    podAntiAffinityLabelSelector: []
    podAntiAffinityTermLabelSelector: []

  # Mesh ILB gateway creates a gateway of type InternalLoadBalancer,
  # for mesh expansion. It exposes the mtls ports for Pilot,CA as well
  # as non-mtls ports to support upgrades and gradual transition.
  istio-ilbgateway:
    enabled: false
    labels:
      app: istio-ilbgateway
      istio: ilbgateway
    autoscaleEnabled: true
    autoscaleMin: 1
    autoscaleMax: 5
    # specify replicaCount when autoscaleEnabled: false
    # replicaCount: 1
    cpu:
      targetAverageUtilization: 80
    resources:
      requests:
        cpu: 800m
        memory: 512Mi
      #limits:
      #  cpu: 1800m
      #  memory: 512Mi
    loadBalancerIP: ""
    serviceAnnotations:
      cloud.google.com/load-balancer-type: "internal"
    podAnnotations: {}
    type: LoadBalancer
    ports:
    ## You can add custom gateway ports - google ILB default quota is 5 ports,
    - port: 15011
      name: grpc-pilot-mtls
    # Insecure port - only for migration from 0.8. Will be removed in 1.1
    - port: 15010
      name: grpc-pilot
    - port: 8060
      targetPort: 8060
      name: tcp-citadel-grpc-tls
    # Port 5353 is forwarded to kube-dns
    - port: 5353
      name: tcp-dns
    secretVolumes:
    - name: ilbgateway-certs
      secretName: istio-ilbgateway-certs
      mountPath: /etc/istio/ilbgateway-certs
    - name: ilbgateway-ca-certs
      secretName: istio-ilbgateway-ca-certs
      mountPath: /etc/istio/ilbgateway-ca-certs
    nodeSelector: {}
    tolerations: []

    # Specify the pod anti-affinity that allows you to constrain which nodes
    # your pod is eligible to be scheduled based on labels on pods that are
    # already running on the node rather than based on labels on nodes.
    # There are currently two types of anti-affinity:
    #    "requiredDuringSchedulingIgnoredDuringExecution"
    #    "preferredDuringSchedulingIgnoredDuringExecution"
    # which denote “hard” vs. “soft” requirements, you can define your values
    # in "podAntiAffinityLabelSelector" and "podAntiAffinityTermLabelSelector"
    # correspondingly.
    # For example:
    # podAntiAffinityLabelSelector:
    # - key: security
    #   operator: In
    #   values: S1,S2
    #   topologyKey: "kubernetes.io/hostname"
    # This pod anti-affinity rule says that the pod requires not to be scheduled
    # onto a node if that node is already running a pod with label having key
    # “security” and value “S1”.
    podAntiAffinityLabelSelector: []
    podAntiAffinityTermLabelSelector: []

#
# sidecar-injector webhook configuration
#
sidecarInjectorWebhook:
  enabled: true
  replicaCount: 1
  image:
    repository: ibmcom/istio-sidecar_injector
    tag: 1.1.7
  enableNamespacesByDefault: false
  resources: {}
  nodeSelector: {}
  tolerations: []

  # Specify the pod anti-affinity that allows you to constrain which nodes
  # your pod is eligible to be scheduled based on labels on pods that are
  # already running on the node rather than based on labels on nodes.
  # There are currently two types of anti-affinity:
  #    "requiredDuringSchedulingIgnoredDuringExecution"
  #    "preferredDuringSchedulingIgnoredDuringExecution"
  # which denote “hard” vs. “soft” requirements, you can define your values
  # in "podAntiAffinityLabelSelector" and "podAntiAffinityTermLabelSelector"
  # correspondingly.
  # For example:
  # podAntiAffinityLabelSelector:
  # - key: security
  #   operator: In
  #   values: S1,S2
  #   topologyKey: "kubernetes.io/hostname"
  # This pod anti-affinity rule says that the pod requires not to be scheduled
  # onto a node if that node is already running a pod with label having key
  # “security” and value “S1”.
  podAntiAffinityLabelSelector: []
  podAntiAffinityTermLabelSelector: []

  # If true, webhook or istioctl injector will rewrite PodSpec for liveness
  # health check to redirect request to sidecar. This makes liveness check work
  # even when mTLS is enabled.
  rewriteAppHTTPProbe: false

#
# galley configuration
#
galley:
  enabled: true
  replicaCount: 1
  image:
    repository: ibmcom/istio-galley
    tag: 1.1.7
  resources: {}
  nodeSelector: {}
  tolerations: []

  # Specify the pod anti-affinity that allows you to constrain which nodes
  # your pod is eligible to be scheduled based on labels on pods that are
  # already running on the node rather than based on labels on nodes.
  # There are currently two types of anti-affinity:
  #    "requiredDuringSchedulingIgnoredDuringExecution"
  #    "preferredDuringSchedulingIgnoredDuringExecution"
  # which denote “hard” vs. “soft” requirements, you can define your values
  # in "podAntiAffinityLabelSelector" and "podAntiAffinityTermLabelSelector"
  # correspondingly.
  # For example:
  # podAntiAffinityLabelSelector:
  # - key: security
  #   operator: In
  #   values: S1,S2
  #   topologyKey: "kubernetes.io/hostname"
  # This pod anti-affinity rule says that the pod requires not to be scheduled
  # onto a node if that node is already running a pod with label having key
  # “security” and value “S1”.
  podAntiAffinityLabelSelector: []
  podAntiAffinityTermLabelSelector: []

#
# mixer configuration
#
mixer:
  enabled: true
  image:
    repository: ibmcom/istio-mixer
    tag: 1.1.7

  env:
    GODEBUG: gctrace=1
    # max procs should be ceil(cpu limit + 1)
    GOMAXPROCS: "6"

  policy:
    # if policy is enabled, global.disablePolicyChecks has affect.
    enabled: false
    replicaCount: 1
    autoscaleEnabled: true
    autoscaleMin: 1
    autoscaleMax: 5
    cpu:
      targetAverageUtilization: 80
    resources: {}

  telemetry:
    enabled: true
    replicaCount: 1
    autoscaleEnabled: true
    autoscaleMin: 1
    autoscaleMax: 5
    cpu:
      targetAverageUtilization: 80
    sessionAffinityEnabled: false

    # mixer load shedding configuration.
    # When mixer detects that it is overloaded, it starts rejecting grpc requests.
    loadshedding:
      # disabled, logonly or enforce
      mode: enforce
      # based on measurements 100ms p50 translates to p99 of under 1s. This is ok for telemetry which is inherently async.
      latencyThreshold: 100ms
    resources:
      requests:
        cpu: 1000m
        memory: 1Gi
      limits:
        # It is best to do horizontal scaling of mixer using moderate cpu allocation.
        # We have experimentally found that these values work well.
        cpu: 4800m
        memory: 4Gi

  podAnnotations: {}
  nodeSelector: {}
  tolerations: []

  # Specify the pod anti-affinity that allows you to constrain which nodes
  # your pod is eligible to be scheduled based on labels on pods that are
  # already running on the node rather than based on labels on nodes.
  # There are currently two types of anti-affinity:
  #    "requiredDuringSchedulingIgnoredDuringExecution"
  #    "preferredDuringSchedulingIgnoredDuringExecution"
  # which denote “hard” vs. “soft” requirements, you can define your values
  # in "podAntiAffinityLabelSelector" and "podAntiAffinityTermLabelSelector"
  # correspondingly.
  # For example:
  # podAntiAffinityLabelSelector:
  # - key: security
  #   operator: In
  #   values: S1,S2
  #   topologyKey: "kubernetes.io/hostname"
  # This pod anti-affinity rule says that the pod requires not to be scheduled
  # onto a node if that node is already running a pod with label having key
  # “security” and value “S1”.
  podAntiAffinityLabelSelector: []
  podAntiAffinityTermLabelSelector: []

  adapters:
    kubernetesenv:
      enabled: true

    # stdio is a debug adapter in istio-telemetry, it is not recommended for production use.
    stdio:
      enabled: false
      outputAsJson: true
    prometheus:
      enabled: true
      metricsExpiryDuration: 10m
    # Setting this to false sets the useAdapterCRDs mixer startup argument to false
    useAdapterCRDs: true

#
# pilot configuration
#
pilot:
  enabled: true
  autoscaleEnabled: true
  autoscaleMin: 1
  autoscaleMax: 5
  # specify replicaCount when autoscaleEnabled: false
  # replicaCount: 1
  image:
    repository: ibmcom/istio-pilot
    tag: 1.1.7
  sidecar: true
  traceSampling: 1.0
  # Resources for a small pilot install
  resources:
    requests:
      cpu: 500m
      memory: 2048Mi
  env:
    PILOT_PUSH_THROTTLE: 100
    GODEBUG: gctrace=1
  cpu:
    targetAverageUtilization: 80
  nodeSelector: {}
  tolerations: []

  # Specify the pod anti-affinity that allows you to constrain which nodes
  # your pod is eligible to be scheduled based on labels on pods that are
  # already running on the node rather than based on labels on nodes.
  # There are currently two types of anti-affinity:
  #    "requiredDuringSchedulingIgnoredDuringExecution"
  #    "preferredDuringSchedulingIgnoredDuringExecution"
  # which denote “hard” vs. “soft” requirements, you can define your values
  # in "podAntiAffinityLabelSelector" and "podAntiAffinityTermLabelSelector"
  # correspondingly.
  # For example:
  # podAntiAffinityLabelSelector:
  # - key: security
  #   operator: In
  #   values: S1,S2
  #   topologyKey: "kubernetes.io/hostname"
  # This pod anti-affinity rule says that the pod requires not to be scheduled
  # onto a node if that node is already running a pod with label having key
  # “security” and value “S1”.
  podAntiAffinityLabelSelector: []
  podAntiAffinityTermLabelSelector: []

  # The following is used to limit how long a sidecar can be connected
  # to a pilot. It balances out load across pilot instances at the cost of
  # increasing system churn.
  keepaliveMaxServerConnectionAge: 30m

#
# security configuration
#
security:
  enabled: true
  image:
    repository: ibmcom/istio-citadel
    tag: 1.1.7
  selfSigned: true # indicate if self-signed CA is used.
  resources: {}
  nodeSelector: {}
  tolerations: []

  # Specify the pod anti-affinity that allows you to constrain which nodes
  # your pod is eligible to be scheduled based on labels on pods that are
  # already running on the node rather than based on labels on nodes.
  # There are currently two types of anti-affinity:
  #    "requiredDuringSchedulingIgnoredDuringExecution"
  #    "preferredDuringSchedulingIgnoredDuringExecution"
  # which denote “hard” vs. “soft” requirements, you can define your values
  # in "podAntiAffinityLabelSelector" and "podAntiAffinityTermLabelSelector"
  # correspondingly.
  # For example:
  # podAntiAffinityLabelSelector:
  # - key: security
  #   operator: In
  #   values: S1,S2
  #   topologyKey: "kubernetes.io/hostname"
  # This pod anti-affinity rule says that the pod requires not to be scheduled
  # onto a node if that node is already running a pod with label having key
  # “security” and value “S1”.
  podAntiAffinityLabelSelector: []
  podAntiAffinityTermLabelSelector: []

#
# nodeagent configuration
#
nodeagent:
  enabled: false
  image:
    repository: ibmcom/istio-node-agent-k8s
    tag: 1.1.7
  env:
    # name of authentication provider.
    CA_PROVIDER: ""
    # CA endpoint.
    CA_ADDR: ""  
    # names of authentication provider's plugins.
    Plugins: ""
  nodeSelector: {}
  tolerations: []

  # Specify the pod anti-affinity that allows you to constrain which nodes
  # your pod is eligible to be scheduled based on labels on pods that are
  # already running on the node rather than based on labels on nodes.
  # There are currently two types of anti-affinity:
  #    "requiredDuringSchedulingIgnoredDuringExecution"
  #    "preferredDuringSchedulingIgnoredDuringExecution"
  # which denote “hard” vs. “soft” requirements, you can define your values
  # in "podAntiAffinityLabelSelector" and "podAntiAffinityTermLabelSelector"
  # correspondingly.
  # For example:
  # podAntiAffinityLabelSelector:
  # - key: security
  #   operator: In
  #   values: S1,S2
  #   topologyKey: "kubernetes.io/hostname"
  # This pod anti-affinity rule says that the pod requires not to be scheduled
  # onto a node if that node is already running a pod with label having key
  # “security” and value “S1”.
  podAntiAffinityLabelSelector: []
  podAntiAffinityTermLabelSelector: []

#
# addon istiocni configuration
#
istiocni:
  enabled: false
  image:
    repository: ibmcom/istio-cni
    tag: 1.1.7

  logLevel: info

  # Configuration file to insert istiocni plugin configuration
  # by default this will be the first file found in the cni-conf-dir
  # Example
  # cniConfFileName: 10-calico.conflist
  cniConfFileName: ""

  # CNI bin and conf dir override settings
  # defaults:
  cniBinDir: /opt/cni/bin
  cniConfDir: /etc/cni/net.d

  excludeNamespaces:
  - istio-system

  tolerations:
  # Make sure istio-cni-node gets scheduled on all nodes.
  - effect: NoSchedule
    operator: Exists
  # Mark the pod as a critical add-on for rescheduling.
  - key: CriticalAddonsOnly
    operator: Exists
  - effect: NoExecute
    operator: Exists

  # Specify the pod anti-affinity that allows you to constrain which nodes
  # your pod is eligible to be scheduled based on labels on pods that are
  # already running on the node rather than based on labels on nodes.
  # There are currently two types of anti-affinity:
  #    "requiredDuringSchedulingIgnoredDuringExecution"
  #    "preferredDuringSchedulingIgnoredDuringExecution"
  # which denote “hard” vs. “soft” requirements, you can define your values
  # in "podAntiAffinityLabelSelector" and "podAntiAffinityTermLabelSelector"
  # correspondingly.
  # For example:
  # podAntiAffinityLabelSelector:
  # - key: security
  #   operator: In
  #   values: S1,S2
  #   topologyKey: "kubernetes.io/hostname"
  # This pod anti-affinity rule says that the pod requires not to be scheduled
  # onto a node if that node is already running a pod with label having key
  # “security” and value “S1”.
  podAntiAffinityLabelSelector: []
  podAntiAffinityTermLabelSelector: []

# addon Istio CoreDNS configuration
#
istiocoredns:
  enabled: false
  replicaCount: 1
  coreDNSImage:
    repository: ibmcom/coredns
    tag: 1.2.6
  # Source code for the plugin can be found at
  # https://github.com/istio-ecosystem/istio-coredns-plugin
  # The plugin listens for DNS requests from coredns server at 127.0.0.1:8053
  coreDNSPluginImage:
    repository: ibmcom/istio-coredns-plugin
    tag: 0.2-istio-1.1
  resources: {}
  nodeSelector: {}
  tolerations: []

  # Specify the pod anti-affinity that allows you to constrain which nodes
  # your pod is eligible to be scheduled based on labels on pods that are
  # already running on the node rather than based on labels on nodes.
  # There are currently two types of anti-affinity:
  #    "requiredDuringSchedulingIgnoredDuringExecution"
  #    "preferredDuringSchedulingIgnoredDuringExecution"
  # which denote “hard” vs. “soft” requirements, you can define your values
  # in "podAntiAffinityLabelSelector" and "podAntiAffinityTermLabelSelector"
  # correspondingly.
  # For example:
  # podAntiAffinityLabelSelector:
  # - key: security
  #   operator: In
  #   values: S1,S2
  #   topologyKey: "kubernetes.io/hostname"
  # This pod anti-affinity rule says that the pod requires not to be scheduled
  # onto a node if that node is already running a pod with label having key
  # “security” and value “S1”.
  podAntiAffinityLabelSelector: []
  podAntiAffinityTermLabelSelector: []

#
# addon grafana configuration
#
grafana:
  enabled: false
  replicaCount: 1
  image:
    repository: ibmcom/grafana
    tag: 5.2.0-f3
  ingress:
    enabled: false
    ## Used to create an Ingress record.
    hosts: []
    #  - grafana.local
    annotations:
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    tls:
      # Secrets must be manually created in the namespace.
      # - secretName: grafana-tls
      #   hosts:
      #     - grafana.local
  persist: false
  storageClassName: ""
  accessMode: ReadWriteMany
  security:
    enabled: false
    secretName: grafana
    usernameKey: username
    passphraseKey: passphrase
  resources: {}
  nodeSelector: {}
  tolerations: []

  # Specify the pod anti-affinity that allows you to constrain which nodes
  # your pod is eligible to be scheduled based on labels on pods that are
  # already running on the node rather than based on labels on nodes.
  # There are currently two types of anti-affinity:
  #    "requiredDuringSchedulingIgnoredDuringExecution"
  #    "preferredDuringSchedulingIgnoredDuringExecution"
  # which denote “hard” vs. “soft” requirements, you can define your values
  # in "podAntiAffinityLabelSelector" and "podAntiAffinityTermLabelSelector"
  # correspondingly.
  # For example:
  # podAntiAffinityLabelSelector:
  # - key: security
  #   operator: In
  #   values: S1,S2
  #   topologyKey: "kubernetes.io/hostname"
  # This pod anti-affinity rule says that the pod requires not to be scheduled
  # onto a node if that node is already running a pod with label having key
  # “security” and value “S1”.
  podAntiAffinityLabelSelector: []
  podAntiAffinityTermLabelSelector: []

  contextPath: /grafana
  service:
    annotations: {}
    name: http
    type: ClusterIP
    externalPort: 3000
    loadBalancerIP: ""
    loadBalancerSourceRanges: []

#
# addon prometheus configuration
#
prometheus:
  enabled: true
  replicaCount: 1
  image:
    repository: ibmcom/prometheus
    tag: v2.8.0
  retention: 6h
  resources: {}
  nodeSelector: {}
  tolerations: []

  # Specify the pod anti-affinity that allows you to constrain which nodes
  # your pod is eligible to be scheduled based on labels on pods that are
  # already running on the node rather than based on labels on nodes.
  # There are currently two types of anti-affinity:
  #    "requiredDuringSchedulingIgnoredDuringExecution"
  #    "preferredDuringSchedulingIgnoredDuringExecution"
  # which denote “hard” vs. “soft” requirements, you can define your values
  # in "podAntiAffinityLabelSelector" and "podAntiAffinityTermLabelSelector"
  # correspondingly.
  # For example:
  # podAntiAffinityLabelSelector:
  # - key: security
  #   operator: In
  #   values: S1,S2
  #   topologyKey: "kubernetes.io/hostname"
  # This pod anti-affinity rule says that the pod requires not to be scheduled
  # onto a node if that node is already running a pod with label having key
  # “security” and value “S1”.
  podAntiAffinityLabelSelector: []
  podAntiAffinityTermLabelSelector: []

  # Controls the frequency of prometheus scraping
  scrapeInterval: 15s

  contextPath: /prometheus

  ingress:
    enabled: false
    ## Used to create an Ingress record.
    hosts: []
    #  - prometheus.local
    annotations:
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    tls:
      # Secrets must be manually created in the namespace.
      # - secretName: prometheus-tls
      #   hosts:
      #     - prometheus.local

  service:
    annotations: {}
    nodePort:
      enabled: false
      port: 32090

  security:
    enabled: true

#
# addon servicegraph configuration
#
servicegraph:
  enabled: false
  replicaCount: 1
  image:
    repository: ibmcom/istio-servicegraph
    tag: 1.1.7
  resources: {}
  nodeSelector: {}
  tolerations: []

  # Specify the pod anti-affinity that allows you to constrain which nodes
  # your pod is eligible to be scheduled based on labels on pods that are
  # already running on the node rather than based on labels on nodes.
  # There are currently two types of anti-affinity:
  #    "requiredDuringSchedulingIgnoredDuringExecution"
  #    "preferredDuringSchedulingIgnoredDuringExecution"
  # which denote “hard” vs. “soft” requirements, you can define your values
  # in "podAntiAffinityLabelSelector" and "podAntiAffinityTermLabelSelector"
  # correspondingly.
  # For example:
  # podAntiAffinityLabelSelector:
  # - key: security
  #   operator: In
  #   values: S1,S2
  #   topologyKey: "kubernetes.io/hostname"
  # This pod anti-affinity rule says that the pod requires not to be scheduled
  # onto a node if that node is already running a pod with label having key
  # “security” and value “S1”.
  podAntiAffinityLabelSelector: []
  podAntiAffinityTermLabelSelector: []

  service:
    annotations: {}
    name: http
    type: ClusterIP
    externalPort: 8088
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
  ingress:
    enabled: false
    # Used to create an Ingress record.
    hosts: []
    #  - servicegraph.local
    annotations:
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    tls:
      # Secrets must be manually created in the namespace.
      # - secretName: servicegraph-tls
      #   hosts:
      #     - servicegraph.local
  # prometheus address
  prometheusAddr: http://prometheus:9090

#
# addon jaeger tracing configuration
#
tracing:
  enabled: false
  provider: jaeger
  contextPath: "/jaeger"
  nodeSelector: {}
  tolerations: []

  # Specify the pod anti-affinity that allows you to constrain which nodes
  # your pod is eligible to be scheduled based on labels on pods that are
  # already running on the node rather than based on labels on nodes.
  # There are currently two types of anti-affinity:
  #    "requiredDuringSchedulingIgnoredDuringExecution"
  #    "preferredDuringSchedulingIgnoredDuringExecution"
  # which denote “hard” vs. “soft” requirements, you can define your values
  # in "podAntiAffinityLabelSelector" and "podAntiAffinityTermLabelSelector"
  # correspondingly.
  # For example:
  # podAntiAffinityLabelSelector:
  # - key: security
  #   operator: In
  #   values: S1,S2
  #   topologyKey: "kubernetes.io/hostname"
  # This pod anti-affinity rule says that the pod requires not to be scheduled
  # onto a node if that node is already running a pod with label having key
  # “security” and value “S1”.
  podAntiAffinityLabelSelector: []
  podAntiAffinityTermLabelSelector: []
  
  jaeger:
    image:
      repository: ibmcom/jaegertracing-all-in-one
      tag: 1.9
    memory:
      max_traces: 50000
    resources: {}

  zipkin:
    image:
      repository: ibmcom/zipkin
      tag: 2
    probeStartupDelay: 200
    queryPort: 9411
    resources:
      limits:
        cpu: 300m
        memory: 900Mi
      requests:
        cpu: 150m
        memory: 900Mi
    javaOptsHeap: 700
    # From: https://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/main/resources/zipkin-server-shared.yml#L51
    # Maximum number of spans to keep in memory.  When exceeded, oldest traces (and their spans) will be purged.
    # A safe estimate is 1K of memory per span (each span with 2 annotations + 1 binary annotation), plus
    # 100 MB for a safety buffer.  You'll need to verify in your own environment.
    maxSpans: 500000
    node:
      cpus: 2

  service:
    annotations: {}
    name: http
    type: ClusterIP
    externalPort: 9411

  ingress:
    enabled: false
    # Used to create an Ingress record.
    hosts: []
      # - tracing.local
    annotations:
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    tls:
      # Secrets must be manually created in the namespace.
      # - secretName: tracing-tls
      #   hosts:
      #     - tracing.local

#
# addon kiali tracing configuration
#
kiali:
  enabled: false
  replicaCount: 1
  image:
    repository: ibmcom/kiali
    tag: v0.16.2
  contextPath: /kiali
  resources: {}
  nodeSelector: {}
  tolerations: []

  # Specify the pod anti-affinity that allows you to constrain which nodes
  # your pod is eligible to be scheduled based on labels on pods that are
  # already running on the node rather than based on labels on nodes.
  # There are currently two types of anti-affinity:
  #    "requiredDuringSchedulingIgnoredDuringExecution"
  #    "preferredDuringSchedulingIgnoredDuringExecution"
  # which denote “hard” vs. “soft” requirements, you can define your values
  # in "podAntiAffinityLabelSelector" and "podAntiAffinityTermLabelSelector"
  # correspondingly.
  # For example:
  # podAntiAffinityLabelSelector:
  # - key: security
  #   operator: In
  #   values: S1,S2
  #   topologyKey: "kubernetes.io/hostname"
  # This pod anti-affinity rule says that the pod requires not to be scheduled
  # onto a node if that node is already running a pod with label having key
  # “security” and value “S1”.
  podAntiAffinityLabelSelector: []
  podAntiAffinityTermLabelSelector: []

  ingress:
    enabled: false
    ## Used to create an Ingress record.
    hosts: []
    # - kiali.local
    annotations:
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    tls:
      # Secrets must be manually created in the namespace.
      # - secretName: kiali-tls
      #   hosts:
      #     - kiali.local

  dashboard:
    secretName: kiali

    # Override the automatically detected Grafana URL, useful when Grafana service has no ExternalIPs
    grafanaURL: ""

    # Override the automatically detected Jaeger URL, useful when Jaeger service has no ExternalIPs
    jaegerURL: ""
  prometheusAddr: http://prometheus:9090

  # When true, a secret will be created with a default username and password. Useful for demos.
  createDemoSecret: false

# Certmanager uses ACME to sign certificates. Since Istio gateways are
# mounting the TLS secrets the Certificate CRDs must be created in the
# istio-system namespace. Once the certificate has been created, the
# gateway must be updated by adding 'secretVolumes'. After the gateway
# restart, DestinationRules can be created using the ACME-signed certificates.
certmanager:
  enabled: false
  replicaCount: 1
  image:
    repository: ibmcom/icp-cert-manager-controller
    tag: 0.7.0
  extraArgs: []
  podAnnotations: {}
  podLabels: {}

  # Optional DNS settings, useful if you have a public and private DNS zone for
  # the same domain on Route 53. What follows is an example of ensuring
  # cert-manager can access an ingress or DNS TXT records at all times.
  # NOTE: This requires Kubernetes 1.10 or `CustomPodDNS` feature gate enabled for
  # the cluster to work.
  podDnsPolicy: ""
  podDnsConfig: {}
  #   nameservers:
  #     - "1.1.1.1"
  #     - "8.8.8.8"

  email: ""
  resources: {}
  nodeSelector: {}
  tolerations: []

  # Specify the pod anti-affinity that allows you to constrain which nodes
  # your pod is eligible to be scheduled based on labels on pods that are
  # already running on the node rather than based on labels on nodes.
  # There are currently two types of anti-affinity:
  #    "requiredDuringSchedulingIgnoredDuringExecution"
  #    "preferredDuringSchedulingIgnoredDuringExecution"
  # which denote “hard” vs. “soft” requirements, you can define your values
  # in "podAntiAffinityLabelSelector" and "podAntiAffinityTermLabelSelector"
  # correspondingly.
  # For example:
  # podAntiAffinityLabelSelector:
  # - key: security
  #   operator: In
  #   values: S1,S2
  #   topologyKey: "kubernetes.io/hostname"
  # This pod anti-affinity rule says that the pod requires not to be scheduled
  # onto a node if that node is already running a pod with label having key
  # “security” and value “S1”.
  podAntiAffinityLabelSelector: []
  podAntiAffinityTermLabelSelector: []
