# The reason for creating a ServiceAccount and ClusterRole specifically for this
# post-delete hooked job is because the certmanager ServiceAccount is being deleted
# before this hook is launched.
#
# It's also important that the ServiceAccount, ClusterRole and ClusterRoleBinding
# will be ready before running the hooked Job therefore the hook weights.

apiVersion: v1
kind: ServiceAccount
imagePullSecrets:
  - name: sa-{{ .Release.Namespace }}
{{- if .Values.global.imagePullSecrets }}
{{- range .Values.global.imagePullSecrets }}
  - name: {{ . }}
{{- end }}
{{- end }}
metadata:
  name: certmanager-cleanup-crds-service-account
  namespace: {{ .Release.Namespace }}
  annotations:
    "helm.sh/hook": post-delete
    "helm.sh/hook-delete-policy": hook-succeeded
    "helm.sh/hook-weight": "1"
  labels:
    app: {{ template "certmanager.name" . }}
    chart: {{ template "certmanager.chart" . }}
    heritage: {{ .Release.Service }}
    release: {{ .Release.Name }}
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: certmanager-cleanup-crds-{{ .Release.Namespace }}
  annotations:
    "helm.sh/hook": post-delete
    "helm.sh/hook-delete-policy": hook-succeeded
    "helm.sh/hook-weight": "1"
  labels:
    app: {{ template "certmanager.name" . }}
    chart: {{ template "certmanager.chart" . }}
    heritage: {{ .Release.Service }}
    release: {{ .Release.Name }}
rules:
- apiGroups: ["apiextensions.k8s.io"]
  resources: ["customresourcedefinitions"]
  verbs: ["list", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: certmanager-cleanup-crds-{{ .Release.Namespace }}
  annotations:
    "helm.sh/hook": post-delete
    "helm.sh/hook-delete-policy": hook-succeeded
    "helm.sh/hook-weight": "2"
  labels:
    app: {{ template "certmanager.name" . }}
    chart: {{ template "certmanager.chart" . }}
    heritage: {{ .Release.Service }}
    release: {{ .Release.Name }}
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: certmanager-cleanup-crds-{{ .Release.Namespace }}
subjects:
  - kind: ServiceAccount
    name: certmanager-cleanup-crds-service-account
    namespace: {{ .Release.Namespace }}
---
apiVersion: batch/v1
kind: Job
metadata:
  name: certmanager-cleanup-crds
  namespace: {{ .Release.Namespace }}
  annotations:
    "helm.sh/hook": post-delete
    "helm.sh/hook-delete-policy": hook-succeeded
    "helm.sh/hook-weight": "3"
  labels:
    app: {{ template "certmanager.name" . }}
    chart: {{ template "certmanager.chart" . }}
    heritage: {{ .Release.Service }}
    release: {{ .Release.Name }}
spec:
  template:
    metadata:
      name: certmanager-cleanup-crds
      labels:
        app: {{ template "certmanager.name" . }}
        chart: {{ template "certmanager.chart" . }}
        heritage: {{ .Release.Service }}
        release: {{ .Release.Name }}
      annotations:
        sidecar.istio.io/inject: "false"
        scheduler.alpha.kubernetes.io/critical-pod: ""
    spec:
      serviceAccountName: certmanager-cleanup-crds-service-account
{{- if .Values.global.priorityClassName }}
      priorityClassName: "{{ .Values.global.priorityClassName }}"
{{- end }}
      containers:
        - name: kubectl
          image: "{{ .Values.global.kubectl.repository }}:{{ .Values.global.kubectl.tag }}"
          imagePullPolicy: "{{ .Values.global.imagePullPolicy }}"
          command:
          - /bin/sh
          - -c
          - >
              kubectl get crd | grep "certmanager.k8s.io" |  while read -r entry; do
                name=$(echo $entry | awk '{print $1}');
                kubectl delete crd $name;
              done
      restartPolicy: OnFailure
      affinity:
{{- include "nodeaffinity" . | indent 6 }}
      {{- if .Values.tolerations }}
      tolerations:
{{ toYaml .Values.tolerations | indent 6 }}
      {{- else if .Values.global.defaultTolerations }}
      tolerations:
{{ toYaml .Values.global.defaultTolerations | indent 6 }}
      {{- end }}