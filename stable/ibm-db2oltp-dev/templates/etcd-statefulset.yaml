{{- if .Values.hadr.enabled }} 
apiVersion: v1
kind: Service
metadata:
  name: {{ template "fullname" . }}-etcd
  labels:
    app: {{ template "fullname" . }}
    chart: "{{ .Chart.Name }}"
    component: "etcd"
    release: "{{ .Release.Name }}"
    heritage: "{{ .Release.Service }}"
spec:
  ports:
  - port: 2380
    name: etcd-server
  - port: 2379
    name: etcd-client
    protocol: TCP
  clusterIP: None
  selector:
    app: {{ template "fullname" . }}
    component: "etcd"

---
{{- if .Capabilities.APIVersions.Has "apps/v1beta2" }}
apiVersion: apps/v1beta2
{{- else }}
apiVersion: apps/v1beta1
{{- end }}
kind: StatefulSet
metadata:
  name: {{ template "fullname" . }}-etcd
  labels:
    app: {{ template "fullname" . }}
    chart: {{ .Chart.Name }}
    component: "etcd"
    heritage: {{ .Release.Service }}
    release: {{ .Release.Name }}
spec:
  selector:
    matchLabels:
      app: {{ template "fullname" . }}
      release: {{ .Release.Name }}
      heritage: {{ .Release.Service }}
      component: "etcd"
  serviceName: {{ template "fullname" . }}-etcd
  podManagementPolicy: "Parallel"
  replicas: 3
  {{- if and (.Capabilities.KubeVersion.Major | hasPrefix "1") (.Capabilities.KubeVersion.Minor | hasPrefix "7") }}
  # Set updateStrategy to "RollingUpdate", if we're on Kubernetes 1.7.
  # It's already the default for apps/v1beta2 (Kubernetes 1.8 onwards)
  updateStrategy:
    type: RollingUpdate
  {{- end }}
  template:
    metadata:
      labels:
        app: {{ template "fullname" . }}
        component: etcd
        release: {{ .Release.Name }}
        chart: {{ .Chart.Name }}-{{ .Chart.Version }}
        heritage: {{ .Release.Service }}
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                {{- if .Values.arch }}
                - {{ .Values.arch }}
                {{- else }}
                - {{ template "arch" . }}
                {{- end }}
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - topologyKey: "kubernetes.io/hostname"
              labelSelector:
                matchLabels:
                  app: "{{ template "fullname" . }}"
                  release: "{{ .Release.Name }}"
                  component: "etcd"
      containers:
      - name: {{ template "fullname" . }}-etcd
        {{- if .Values.arch }}
        image: "k8s.gcr.io/etcd-{{ .Values.arch }}:3.0.17"
        {{- else }}
        image: "k8s.gcr.io/etcd-{{ template "arch" . }}:3.0.17"
        {{- end }}
        imagePullPolicy: "Always"
        livenessProbe:
          tcpSocket:
            port: 2379
          initialDelaySeconds: 15
          periodSeconds: 20
        readinessProbe:
          exec:
            command:
            - touch
            - x
          initialDelaySeconds: 0
          periodSeconds: 20
          failureThreshold: 10
        ports:
        - containerPort: 2379
          name: peer
        - containerPort: 2380
          name: client
        resources:
          requests:
            cpu: "100m"
            memory: "512Mi"
        env:
        - name: INITIAL_CLUSTER_SIZE
          value: "3"
        - name: SET_NAME
          value: {{ template "fullname" . }}-etcd
        volumeMounts:
        - name: {{ .Values.etcdVolume.name | quote }}
          mountPath: /var/run/etcd
        lifecycle:
          preStop:
            exec:
              command:
                - "/bin/sh"
                - "-ec"
                - |
                  EPS=""
                  for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                      EPS="${EPS}${EPS:+,}http://${SET_NAME}-${i}.${SET_NAME}:2379"
                  done

                  HOSTNAME=$(hostname)

                  member_hash() {
                      etcdctl member list | grep http://${HOSTNAME}.${SET_NAME}:2380 | cut -d':' -f1 | cut -d'[' -f1
                  }

                  echo "Removing ${HOSTNAME} from etcd cluster"

                  ETCDCTL_ENDPOINT=${EPS} etcdctl member remove $(member_hash)
                  if [ $? -eq 0 ]; then
                      # Remove everything otherwise the cluster will no longer scale-up
                      rm -rf /var/run/etcd/*
                  fi
        command:
          - "/bin/sh"
          - "-ec"
          - |
            HOSTNAME=$(hostname)

            # store member id into PVC for later member replacement
            collect_member() {
                while ! etcdctl member list &>/dev/null; do sleep 1; done
                etcdctl member list | grep http://${HOSTNAME}.${SET_NAME}:2380 | cut -d':' -f1 | cut -d'[' -f1 > /var/run/etcd/member_id
                exit 0
            }

            eps() {
                EPS=""
                for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                    EPS="${EPS}${EPS:+,}http://${SET_NAME}-${i}.${SET_NAME}:2379"
                done
                echo ${EPS}
            }

            member_hash() {
                etcdctl member list | grep http://${HOSTNAME}.${SET_NAME}:2380 | cut -d':' -f1 | cut -d'[' -f1
            }

            # re-joining after failure?
            if [ -e /var/run/etcd/default.etcd ]; then
                echo "Re-joining etcd member"
                member_id=$(cat /var/run/etcd/member_id)

                # re-join member
                ETCDCTL_ENDPOINT=$(eps) etcdctl member update ${member_id} http://${HOSTNAME}.${SET_NAME}:2380
                exec etcd --name ${HOSTNAME} \
                    --listen-peer-urls http://${HOSTNAME}.${SET_NAME}:2380 \
                    --listen-client-urls http://${HOSTNAME}.${SET_NAME}:2379,http://127.0.0.1:2379 \
                    --advertise-client-urls http://${HOSTNAME}.${SET_NAME}:2379 \
                    --data-dir /var/run/etcd/default.etcd
            fi

            # etcd-SET_ID
            SET_ID=${HOSTNAME:5:${#HOSTNAME}}

            # adding a new member to existing cluster (assuming all initial pods are available)
            if [ "${SET_ID}" -ge ${INITIAL_CLUSTER_SIZE} ]; then
                export ETCDCTL_ENDPOINT=$(eps)

                # member already added?
                MEMBER_HASH=$(member_hash)
                if [ -n "${MEMBER_HASH}" ]; then
                    # the member hash exists but for some reason etcd failed
                    # as the datadir has not be created, we can remove the member
                    # and retrieve new hash
                    etcdctl member remove ${MEMBER_HASH}
                fi

                echo "Adding new member"
                etcdctl member add ${HOSTNAME} http://${HOSTNAME}.${SET_NAME}:2380 | grep "^ETCD_" > /var/run/etcd/new_member_envs

                if [ $? -ne 0 ]; then
                    echo "Exiting"
                    rm -f /var/run/etcd/new_member_envs
                    exit 1
                fi

                cat /var/run/etcd/new_member_envs
                source /var/run/etcd/new_member_envs

                collect_member &

                exec etcd --name ${HOSTNAME} \
                    --listen-peer-urls http://${HOSTNAME}.${SET_NAME}:2380 \
                    --listen-client-urls http://${HOSTNAME}.${SET_NAME}:2379,http://127.0.0.1:2379 \
                    --advertise-client-urls http://${HOSTNAME}.${SET_NAME}:2379 \
                    --data-dir /var/run/etcd/default.etcd \
                    --initial-advertise-peer-urls http://${HOSTNAME}.${SET_NAME}:2380 \
                    --initial-cluster ${ETCD_INITIAL_CLUSTER} \
                    --initial-cluster-state ${ETCD_INITIAL_CLUSTER_STATE}
            fi

            for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                while true; do
                    echo "Waiting for ${SET_NAME}-${i}.${SET_NAME} to come up"
                    ping -W 1 -c 1 ${SET_NAME}-${i}.${SET_NAME} > /dev/null && break
                    sleep 1s
                done
            done

            PEERS=""
            for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                PEERS="${PEERS}${PEERS:+,}${SET_NAME}-${i}=http://${SET_NAME}-${i}.${SET_NAME}:2380"
            done

            collect_member &

            # join member
            exec etcd --name ${HOSTNAME} \
                --initial-advertise-peer-urls http://${HOSTNAME}.${SET_NAME}:2380 \
                --listen-peer-urls http://${HOSTNAME}.${SET_NAME}:2380 \
                --listen-client-urls http://${HOSTNAME}.${SET_NAME}:2379,http://127.0.0.1:2379 \
                --advertise-client-urls http://${HOSTNAME}.${SET_NAME}:2379 \
                --initial-cluster-token etcd-cluster-1 \
                --initial-cluster ${PEERS} \
                --initial-cluster-state new \
                --data-dir /var/run/etcd/default.etcd
  volumeClaimTemplates:
  {{- if (.Values.persistence.enabled) }}
  - metadata:
      name: {{ .Values.etcdVolume.name | quote }}
      labels:
        app: {{ template "fullname" . }}
        chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
        release: "{{ .Release.Name }}"
        component: etcd
        heritage: "{{ .Release.Service }}"
    spec:
      {{- if .Values.persistence.useDynamicProvisioning }}
      # If present, use the storageClassName from the values.yaml, else use the
      # default storageClass setup by Kubernetes Administrator
      #
      # Setting storageClassName to nil means use the default storage class
      storageClassName: {{ default nil .Values.etcdVolume.storageClassName | quote }}
      {{- else }}
      storageClassName: {{ default "" .Values.etcdVolume.storageClassName | quote }}
      {{- end }}
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: {{ .Values.etcdVolume.size | quote }}
  {{- end }}
{{- end }}

