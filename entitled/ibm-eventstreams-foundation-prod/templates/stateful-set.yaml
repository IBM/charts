###############################################################################
#
# Licensed Materials - Property of IBM
#
# 5737-H33
#
# (C) Copyright IBM Corp. 2018, 2019  All Rights Reserved.
#
# US Government Users Restricted Rights - Use, duplication or
# disclosure restricted by GSA ADP Schedule Contract with IBM Corp.
#
###############################################################################
# Defines the Kubernetes pods that will make up the cluster of Kafka brokers
###############################################################################
{{ $root := . -}}
{{ if eq .Values.license "accept" -}}
{{ $zookeeperConfig := include "sch.config.init" (list . "zookeeper.sch.chart.config.values" ) -}}
{{ $securityConfig := include "sch.config.init" (list . "security.sch.chart.config.values") -}}
{{- include "sch.config.init" (list . "securitycontext.sch.chart.config.values") | trim -}}
{{ $namePrefix := .sch.chart.components.kafka.statefulSet.name -}}
{{ $statefulSetName := include "sch.names.statefulSetName" (list . $namePrefix) -}}
# Component is 'kafka' as this makes up part of implementing the Kafka cluster
{{ $compName := .sch.chart.components.kafka.compName -}}
{{ $labels := include "sch.metadata.labels.standard" (list . $compName (dict "serviceSelector" $namePrefix)) -}}
# The internal headless service that will provide access to the Kafka brokers defined below
{{ $serviceName := .sch.chart.components.kafka.internalHeadless.name -}}
# Name of the ZooKeeper stateful deployment that will orchestrate the Kafka brokers
{{ $zookeeperName := .sch.chart.components.zookeeper.statefulSet.name -}}
# number of ZooKeeper nodes in the cluster
{{ $zookeeperReplicas := int .sch.config.zookeeper.replicas -}}
# Config map that identifies the metrics that should be pushed to Prometheus
{{ $kafkaMetricsConfigMap := .sch.chart.components.kafka.metricsConfigMap.name -}}
{{ $kafkaMetricsConfigMapName := include "sch.names.fullCompName" (list . $kafkaMetricsConfigMap) | quote -}}
# Service Account to grant Kubernetes access
{{ $serviceAccount := .sch.chart.components.kafka.serviceAccount.name -}}
{{ $serviceAccountName := include "sch.names.fullCompName" (list . $serviceAccount ) -}}
# Kafka ports are defined in a helper template
{{ $kafkaconfig := include "sch.config.init" (list . "kafka.listeners.sch.chart.config.values") -}}
{{ $kafkaListeners := .sch.config.kafka.listeners -}}
{{ $kafkaProtocols := .sch.config.kafka.protocols -}}
{{ $kafkaAdvertisedListeners := .sch.config.kafka.internalAdvertisedListeners -}}
# Elastic search service name required by kafka-metrics-proxy
{{ $elasticSearchService := .sch.chart.components.elasticSearch.service.name -}}
{{ $elasticSearchServiceName := include "sch.names.fullCompName" (list . $elasticSearchService) -}}
# import port definitions
{{- include "sch.config.init" (list . "ports.sch.chart.config.values") | trim -}}
{{ $ports := .sch.config.ports }}
# IAM Secret name containing the API Key
{{ $iamSecret := .sch.chart.components.security.iamSecret.name -}}
{{ $iamSecretName := include "sch.names.fullCompName" (list . $iamSecret ) -}}
# Eventstreams IAM Service Name
{{ $iamServiceName := .sch.chart.components.security.serviceName -}}
# Access controller service that we need to connect to do IAM Authorisations
{{ $accessControllerService := .sch.chart.components.security.accesscontroller.service.name -}}
{{ $accessControllerServiceName := include "sch.names.fullCompName" (list . $accessControllerService) -}}
# JMX Secret name containing the username & password for JMX
{{ $jmxSecret := .sch.chart.components.kafka.jmxSecret.name -}}
{{ $jmxSecretName := include "sch.names.fullCompName" (list . $jmxSecret ) -}}
# Proxy Secret name containing the private key and public cert for Kafka to open JMX securely
{{ $proxySecret := .sch.chart.components.proxy.secret.name -}}
{{ $proxySecretName := include "sch.names.fullCompName" (list . $proxySecret ) -}}
# Internal configmap name for cluster env vars
{{ $releaseConfigMap := .sch.chart.components.essential.releaseConfigMap.name -}}
{{ $releaseConfigMapName := include "sch.names.fullCompName" (list . $releaseConfigMap) -}}

# length chewcked pvc name
{{ $shortPvcName := include "sch.names.volumeClaimTemplateName" (list . .Values.persistence.dataPVC.name $statefulSetName) -}}
# Proxy config map name
{{ $configMap := .sch.chart.components.kafka.mproxyConfigMap.name -}}
{{ $configMapName := include "sch.names.fullCompName" (list . $configMap ) }}
# TLS Proxy config map name
{{ $tlsConfigMap := .sch.chart.components.kafka.configMap.name -}}
{{ $tlsConfigMapName := include "sch.names.fullCompName" (list . $tlsConfigMap ) }}
# k8s configuration
{{ $clusterName := .Values.global.k8s.clusterName -}}
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ $statefulSetName | quote }}
  labels:
{{ $labels | indent 4 }}
    internalCertificate: {{ .Values.global.security.internalCertificateLabel | quote }}
    externalCertificate: {{ .Values.global.security.externalCertificateLabel | quote }}
    externalHost: {{ $root.Values.proxy.externalEndpoint | quote }}
spec:
  # identify the headless service that will enable access to these pods
  serviceName: {{ include "sch.names.fullCompName" (list $root $serviceName) | quote }}
  # Kafka pods are deployed in parallel to enable faster deployments
  podManagementPolicy: "Parallel"
  # the number of brokers in the kafka cluster
  {{- if not (regexMatch "(^[3-9]+[0-9]*$)|(^[1-9]+[0-9]+$)" ($root.Values.kafka.brokers | toString)) }}
    {{ fail "Configuration error: Minimum of 3 Kafka brokers required." }}
  {{- end }}
  replicas: {{ $root.Values.kafka.brokers }}
  # Kafka broker pods are uniquely identified based on the release name
  selector:
    matchLabels:
      release: {{ $root.Release.Name | quote }}
      serviceSelector: {{ $namePrefix | quote }}
  updateStrategy:
    type: RollingUpdate
  # definition of the pods that run the Kafka brokers in the cluster
  template:
    metadata:
      # Check release name is valid.
      {{- if not (regexMatch "^[a-z][a-z0-9-]*[a-z0-9]*$" $root.Release.Name) }}
        {{ fail (cat "Configuration error: Format of release name is invalid:" $root.Release.Name) }}
      {{- end }}
      # Check release name does not end with a dash.
      {{- if regexMatch "-$" $root.Release.Name }}
        {{ fail (cat "Configuration error: Format of release name is invalid:" $root.Release.Name) }}
      {{- end }}
      name: {{ $root.Release.Name | quote }}
      namespace: {{ include "restrict.namespace" (list $root $root.Release.Namespace) }}
      labels:
{{ $labels | indent 8 }}
        internalCertificate: {{ .Values.global.security.internalCertificateLabel | quote }}
        externalCertificate: {{ .Values.global.security.externalCertificateLabel | quote }}
        externalHost: {{ $root.Values.proxy.externalEndpoint | quote }}
        configMapRevision: "default"
        releaseConfigMapRevision: "default"
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '{{ $ports.prometheus.collectorKafka }}'
        prometheus.io/scheme: https
{{- if $root.Values.externalMonitoring.datadog }}
  {{- range $annotation, $value := $root.Values.externalMonitoring.datadog }}
    {{- if not (regexMatch "^([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9]$" $annotation ) }}
      {{ fail (cat "Configuration error: Format of datadog annotation is invalid:" $annotation) }}
    {{- end }}
    {{- if not (kindIs "string" $value) }}
        ad.datadoghq.com/kafka.{{ $annotation | trim }}: {{ $value | toJson | quote }}
    {{- else }}
        ad.datadoghq.com/kafka.{{ $annotation | trim }}: {{ $value | quote }}
    {{- end }}
  {{- end }}
{{- end }}
{{ include "ibm-eventstreams.metering" (list $root "kafka") | indent 8 }}
    spec:
      # # Hostname is kafka, used for internal pod communication
      # hostname: kafka
      serviceAccountName: {{ $serviceAccountName | quote }}
      hostNetwork: false
      hostPID: false
      hostIPC: false
      securityContext:
{{- include "sch.security.securityContext" (list $root $root.sch.chart.securitycontexts.pod) | indent 8 }}
{{ include "ibm-eventstreams.fsGroupGid" (list $root ) | indent 8 }}
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
{{- include "ibm-eventstreams.archMatchExpression"  (list $root) | indent 16 }}
{{- include "ibm-eventstreams.stsMatchExpression"  (list . $root.Values.global.zones.kafkaLabel) | indent 16 }}
        #  we don't want multiple co-located Kafka nodes
        #  so this anti-affinity rule should prevent
        #  nodes with the kafka name being scheduled on
        #  the same host
        # it should then also try to separate zookeeper and kafka
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 5
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: "release"
                      operator: In
                      values:
                        -  {{ $root.Release.Name | quote }}
                    - key: "serviceSelector"
                      operator: In
                      values:
                        -  {{ $zookeeperName }}
                topologyKey: "kubernetes.io/hostname"
{{- if gt (int (include "zones.to.template" (list $root)) ) 1 }}
          requiredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
{{- include "ibm-eventstreams.isolationMatchExpression"  (list $root $namePrefix) | indent 14 }}
{{- else }}
            - weight: 10
              podAffinityTerm:
{{- include "ibm-eventstreams.isolationMatchExpression"  (list $root $namePrefix) | indent 16 }}
{{- end }}
      # Kafka pods are made up of four containers
      #  1) Kafka broker
      #  2) Metrics reporter that submits metrics from the Kafka broker to Prometheus
      #  3) Kafka metrics proxy, an interceptor that reads kafka protocol and pipes out metadata to elasticsearch
      #  4) Healthcheck
      containers:
        #
        # Kafka broker pods
        - name: kafka
          image: {{ include "eventstreams.image" (list $root "eventstreams-kafka" $root.Values.global.image.imageTags.kafkaTag) | quote }}
          imagePullPolicy: {{ $root.Values.global.image.pullPolicy }}
          securityContext:
{{- include "sch.security.securityContext" (list $root $root.sch.chart.securitycontexts.containerWritableFilesystem) | indent 12 }}
          resources:
            limits:
{{ toYaml $root.Values.kafka.resources.limits | indent 14 }}
            requests:
{{ toYaml $root.Values.kafka.resources.requests | indent 14 }}
          ports:
            - containerPort: {{ $ports.kafka.internalKafka }}
              name: "kafka"
            - containerPort: {{ $ports.kafka.externalSecure }}
              name: "ext-secure"
            - containerPort: {{ $ports.kafka.internalEventStreamsSecure }}
              name: "es-tlsonly"
            - containerPort: {{ $ports.kafka.internalLoopback }}
              name: "es-loopback"
            {{- if $root.Values.kafka.openJMX }}
            - containerPort: {{ $ports.kafka.jmx }}
              name: "jmx"
            {{- end }}
          readinessProbe:
            httpGet:
              port: {{ $ports.kafka.healthcheck }}
              path: /dynamic
            initialDelaySeconds: 120
            periodSeconds: 15
            failureThreshold: 1
            timeoutSeconds: 15
          livenessProbe:
            httpGet:
              port: {{ $ports.kafka.healthcheck }}
              path: /activeports
            initialDelaySeconds: 360
            periodSeconds: 15
            failureThreshold: 3
            timeoutSeconds: 15
          env:
            {{- include "license.accept.env.ref" $root | indent 10 }}
            - name: ZK_FIXED_IP_NAME
              value: "127.0.0.1"
            - name: ZK_CLIENT_PORT
              value: {{ $ports.zookeeper.clientInternal | quote}}
            - name: NAMESPACE
              value: {{ $root.Release.Namespace | quote }}
            - name: RELEASE
              value: {{ $root.Release.Name | quote }}
            - name: CLUSTER_NAME
              valueFrom:
                configMapKeyRef:
                  name: {{ $releaseConfigMapName }}
                  key: "CLUSTER_NAME"
            - name: "ES_API_KEY"
              valueFrom:
                secretKeyRef:
                  name: "{{ $iamSecretName }}"
                  key: "{{ $iamServiceName }}-{{ $root.Release.Name }}-api-key"
            - name: KAFKA_HEAP_OPTS
              value: {{ $root.Values.kafka.heapOpts | quote }}
            - name: KAFKA_LISTENERS
              value: {{ $kafkaListeners }}
            - name: LISTENER_SECURITY_PROTOCOL_MAP
              value: {{ $kafkaProtocols }}
            - name: ACCESS_CONTROLLER_HOSTNAME
              value: "{{ $accessControllerServiceName }}.{{ $root.Release.Namespace }}.svc.{{ $clusterName }}"
            - name: ACCESS_CONTROLLER_PORT
              value: {{ $ports.security.accessController | quote }}
            - name: ADVERTISED_LISTENERS
              value: {{ $kafkaAdvertisedListeners }}
            - name: INTER_BROKER_PROTOCOL_VERSION
              valueFrom:
                configMapKeyRef:
                  name: "{{ $releaseConfigMapName }}"
                  key: "inter.broker.protocol.version"
            - name: LOG_MESSAGE_FORMAT_VERSION
              valueFrom:
                configMapKeyRef:
                  name: "{{ $releaseConfigMapName }}"
                  key: "log.message.format.version"
            - name: ROOT_LOGGING_LEVEL
              value: "INFO"
            - name: BROKER_LOGGING_LEVEL
              value: "INFO"
            - name: REQUEST_LOGGING_LEVEL
              value: "WARN"
            - name: LOG_CLEANER_LOGGING_LEVEL
              value: "INFO"
            - name: KAFKA_AUTH_LOGGING_LEVEL
              value: "INFO"
            - name: LOG_TO_STDOUT
              value: "false"
            {{- if $root.Values.kafka.openJMX }}
            - name: TLS_JMX
              value: "true"
            - name: JMX_HOST
              value: "127.0.0.1"
            - name: JMX_USERNAME
              valueFrom:
                secretKeyRef:
                  name: {{ $jmxSecretName }}
                  key: "jmx_username"
            - name: JMX_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ $jmxSecretName }}
                  key: "jmx_password"
            {{- end }}
            - name: CONFIGMAP
              value: {{ $releaseConfigMapName | quote }}
          volumeMounts:
            - name: proxy-secret
              mountPath: /etc/tls-cert
              readOnly: true
          {{- if $root.Values.kafka.configMapName }}
            - name: cluster-config-volume
              mountPath: /opt/cluster-configmap
              readOnly: true
          {{- end }}
          {{- if $root.Values.persistence.enabled }}
            - name: {{ $shortPvcName }}
              mountPath: /var/data
          {{- else }}
            - name: "tempdir"
              mountPath: /var/data
          {{- end }}
        #
        # Metrics reporter sidecar - receives Kafka metrics and pushes to Prometheus
        - name: metrics-reporter
          image: {{ include "eventstreams.image" (list $root "eventstreams-kafka-metrics-reporter" $root.Values.global.image.imageTags.metricsReporterTag) | quote }}
          imagePullPolicy: {{ $root.Values.global.image.pullPolicy }}
          securityContext:
{{- include "sch.security.securityContext" (list $root $root.sch.chart.securitycontexts.containerWritableFilesystem) | indent 12 }}
          resources:
            limits:
{{ toYaml $root.Values.kafka.metricsReporterResources.limits | indent 14 }}
            requests:
{{ toYaml $root.Values.kafka.metricsReporterResources.requests | indent 14 }}
          env:
            {{- include "license.accept.env.ref" $root | indent 10 }}
            - name: NAMESPACE
              value: {{ $root.Release.Namespace }}
            - name: METRICS_PORT
              value: "{{ $ports.kafka.metrics }}"
            - name: IBM_JAVA_OPTIONS
              value: "-XX:+UseContainerSupport"
            {{- if $root.Values.kafka.openJMX }}
            - name: JMX_USERNAME
              valueFrom:
                secretKeyRef:
                  name: {{ $jmxSecretName }}
                  key: "jmx_username"
            - name: JMX_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ $jmxSecretName }}
                  key: "jmx_password"
            - name: JMX_HOST
              value: "127.0.0.1"
            - name: TRUST_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ $jmxSecretName }}
                  key: "trust_store_password"
            {{- end }}
          ports:
            - containerPort: {{ $ports.kafka.metrics }}
              name: metrics-port
          readinessProbe:
            timeoutSeconds: 30
            periodSeconds: 30
            initialDelaySeconds: 90
            failureThreshold: 100
            httpGet:
              port: {{ $ports.kafka.metrics }}
              path: /metrics
          livenessProbe:
            timeoutSeconds: 30
            periodSeconds: 30
            initialDelaySeconds: 120
            failureThreshold: 100
            tcpSocket:
              port: {{ $ports.kafka.metrics }}
          volumeMounts:
            - name: metrics-config-volume
              mountPath: /etc/kafka-metrics-reporter
            {{- if $root.Values.kafka.openJMX }}
            - name: jmx-secret
              mountPath: /etc/jmx-cert
           {{- end }}
        #
        # Metrics proxy sidecar - interprets Kafka traffic metrics from the protocol and pushes to the index manager
        - name: metrics-proxy
          image: {{ include "eventstreams.image" (list $root "eventstreams-kafka-metrics-proxy" $root.Values.global.image.imageTags.kafkaMetricsProxyTag) | quote }}
          imagePullPolicy: {{ $root.Values.global.image.pullPolicy }}
          resources:
            limits:
{{ toYaml $root.Values.kafka.metricsProxyResources.limits | indent 14 }}
            requests:
{{ toYaml $root.Values.kafka.metricsProxyResources.requests | indent 14 }}
          securityContext:
{{- include "sch.security.securityContext" (list $root $root.sch.chart.securitycontexts.containerReadOnlyFilesystem) | indent 12 }}
          env:
            {{- include "license.accept.env.ref" $root | indent 10 }}
            - name: ES_CONFIG_PATH
              value: {{ $root.sch.chart.components.kafka.configPath | quote }}
            - name: TLS_CERT
              valueFrom:
                secretKeyRef:
                  name: {{ $proxySecretName }}
                  key: "podtls.cert"
            - name: TLS_KEY
              valueFrom:
                secretKeyRef:
                  name: {{ $proxySecretName }}
                  key: "podtls.key"
            - name: MESSAGE_INDEXING_DISABLED
              value: {{ (ne $root.Values.messageIndexing.messageIndexingEnabled true) | toString | quote }}
            - name: ACCESS_CONTROLLER_SERVICE_NAME
              value: "{{ $accessControllerServiceName }}.{{ $.Release.Namespace }}.svc.{{ $clusterName }}"
            - name: ACCESS_CONTROLLER_SERVICE_PORT
              value: {{ $ports.security.accessController | quote}}
            - name: NAMESPACE
              value: {{ $root.Release.Namespace | quote }}
            - name: RELEASE
              value: {{ $root.Release.Name | quote }}
            - name: CLUSTER_NAME
              valueFrom:
                configMapKeyRef:
                  name: {{ $releaseConfigMapName }}
                  key: "CLUSTER_NAME"
          volumeMounts:
            - name: proxy-certs-volume
              mountPath: /etc/ssl/certs
              readOnly: true
            - name: proxy-config-map
              mountPath: {{ $root.sch.chart.components.kafka.configPath | quote }}
              readOnly: true
          ports:
            - containerPort: {{ $ports.kafka.externalProxySecure }}
              name: external
            - containerPort: {{ $ports.kafka.internalEventStreamsSecureIntercept }}
              name: internal
            - containerPort: {{ $ports.kafka.internalLoopbackIntercept }}
              name: loopback
            - containerPort: {{ $ports.proxy.health }}
              name: proxy-health
          readinessProbe:
            httpGet:
              path: "/ready"
              port: {{ $ports.proxy.health }}
            initialDelaySeconds: 1
            periodSeconds: 5
            timeoutSeconds: 5
          livenessProbe:
            httpGet:
              path: "/live"
              port: {{ $ports.proxy.health }}
            initialDelaySeconds: 15
            periodSeconds: 15
            timeoutSeconds: 15
        # tls proxy sidecar - encrypts traffic to zookeeper and between kafka peers
        - name: tls-proxy
          image: {{ include "eventstreams.image" (list $root "eventstreams-proxy" $root.Values.global.image.imageTags.proxyTag) | quote }}
          imagePullPolicy: {{ $root.Values.global.image.pullPolicy }}
          securityContext:
{{- include "sch.security.securityContext" (list $root $root.sch.chart.securitycontexts.containerReadOnlyFilesystem) | indent 12 }}
          resources:
            limits:
{{ toYaml $root.Values.kafka.tlsProxyResources.limits | indent 14 }}
            requests:
{{ toYaml $root.Values.kafka.tlsProxyResources.requests | indent 14 }}
          env:
            {{- include "license.accept.env.ref" $root | indent 10 }}
            - name: ES_CONFIG_PATH
              value: {{ $root.sch.chart.components.kafka.tlsConfigPath | quote }}
            - name: HEALTH_PORT
              value: {{ $ports.proxy.altHealth | quote }}
            - name: READY_PORT
              value: {{ $ports.proxy.altHealth | quote }}
            - name: TLS_CERT
              valueFrom:
                secretKeyRef:
                  name: {{ $proxySecretName }}
                  key: "podtls.cert"
            - name: TLS_KEY
              valueFrom:
                secretKeyRef:
                  name: {{ $proxySecretName }}
                  key: "podtls.key"
          volumeMounts:
            - name: proxy-certs-volume
              mountPath: /etc/ssl/certs
              readOnly: true
            - name: tlsproxy-config-map
              mountPath: {{ $root.sch.chart.components.kafka.tlsConfigPath | quote }}
              readOnly: true
          ports:
            - containerPort: {{ $ports.proxy.altHealth }}
              name: proxy-health
          readinessProbe:
            httpGet:
              path: "/ready"
              port: {{ $ports.proxy.altHealth }}
            initialDelaySeconds: 1
            periodSeconds: 5
            timeoutSeconds: 5
          livenessProbe:
            httpGet:
              path: "/live"
              port: {{ $ports.proxy.altHealth }}
            initialDelaySeconds: 15
            periodSeconds: 15
            timeoutSeconds: 15
        #
        # Healthcheck container
        - name: healthcheck
          image: {{ include "eventstreams.image" (list $root "eventstreams-healthcheck" $root.Values.global.image.imageTags.healthcheckTag) | quote }}
          imagePullPolicy: {{ $root.Values.global.image.pullPolicy }}
          resources:
            limits:
{{ toYaml $root.Values.kafka.healthcheckResources.limits | indent 14 }}
            requests:
{{ toYaml $root.Values.kafka.healthcheckResources.requests | indent 14 }}
          securityContext:
{{- include "sch.security.securityContext" (list $root $root.sch.chart.securitycontexts.containerReadOnlyFilesystem) | indent 12 }}
          ports:
            - containerPort: {{ $ports.kafka.healthcheck }}
              name: hc-port
          env:
            {{- include "license.accept.env.ref" $root | indent 10 }}
            - name: ENABLE_ENDPOINTS
              value: "dynamic,activeports"
            - name: NAMESPACE
              value: {{ $root.Release.Namespace | quote }}
            - name: ZOOKEEPER_CONNECT
              value: '{{ range $index, $_ := until $zookeeperReplicas }}{{ $uniquePort := $index | add1 }}{{if $index}},{{end}}127.0.0.1:10{{ $uniquePort }}00{{ end }}'
            - name: PORT_CONNECT
              value: '{{ $ports.kafka.internalKafka }},{{ $ports.kafka.externalSecure }},{{ $ports.kafka.internalEventStreamsSecure }},{{ $ports.kafka.internalLoopback }}'
          readinessProbe:
            httpGet:
              port: {{ $ports.kafka.healthcheck }}
              path: /healthz
            initialDelaySeconds: 150
            periodSeconds: 15
            timeoutSeconds: 15
            failureThreshold: 3
          livenessProbe:
            httpGet:
              port: {{ $ports.kafka.healthcheck }}
              path: /healthz
            initialDelaySeconds: 180
            periodSeconds: 15
            timeoutSeconds: 15
            failureThreshold: 3
      volumes:
        - name: metrics-config-volume
          configMap:
            name: {{ $kafkaMetricsConfigMapName }}
        - name: proxy-certs-volume
          secret:
            secretName: {{ $proxySecretName }}
            items:
            - key: podtls.cacert
              path: ca-certificates.crt
        - name: proxy-config-map
          configMap:
            name: {{ $configMapName }}
        - name: tlsproxy-config-map
          configMap:
            name: {{ $tlsConfigMapName }}
        {{- if $root.Values.kafka.configMapName }}
        - name: cluster-config-volume
          configMap:
            name: {{ $root.Values.kafka.configMapName }}
        {{- end }}
        {{- if $root.Values.persistence.enabled }}
        {{- else }}
        - name: "tempdir"
          emptyDir: {}
        {{ end }}
        - name: proxy-secret
          secret:
            secretName: {{$proxySecretName}}
            items:
              - key: tls.cert
                path: tls.cert
              - key: tls.key
                path: tls.key
        {{- if $root.Values.kafka.openJMX }}
        - name: jmx-secret
          secret:
            secretName: {{$jmxSecretName}}
            items:
              - key: truststore.jks
                path: truststore.jks
        {{- end }}
  {{- if not $root.Values.persistence.enabled }}
    # Validate persistence
    {{- if $root.Values.persistence.useDynamicProvisioning }}
      {{ fail "Cannot use dynamic provisioning with persistence disabled" }}
    {{- end  }}
    {{- if not (eq $root.Values.persistence.dataPVC.storageClassName "") }}
      {{ fail "Cannot use a storage class if persistence disabled" }}
    {{- end  }}
  {{- end  }}
  {{- if $root.Values.persistence.enabled }}
  volumeClaimTemplates:
    - metadata:
        name: {{ $shortPvcName }}
        labels:
{{ $labels | indent 10 }}
      spec:
        {{ if $root.Values.persistence.useDynamicProvisioning -}}
        # If present, use the storageClassName from the values.yaml, else use the
        # default storageClass setup by Kubernetes Administrator
        #
        # Setting storageClassName to nil means use the default storage class
        storageClassName: {{ default nil $root.Values.persistence.dataPVC.storageClassName | quote }}
        {{ else -}}
        # Disable dynamic provisioning
        storageClassName: ""
        {{ end -}}
        accessModes: [ "ReadWriteOnce" ]
        resources:
          requests:
            storage: {{ $root.Values.persistence.dataPVC.size | quote }}
  {{- end }}
{{ end -}}
