{{- /*
########################################################################
#
# Licensed Materials - Property of IBM
#
# 5725Q09
#
# (C) Copyright IBM Corp.
#
# 2020 All Rights Reserved
#
# US Government Users Restricted Rights - Use, duplication or disclosure 
# restricted by GSA ADP Schedule Contract with IBM Corp.
#
########################################################################
*/ -}}
{{- include "sch.config.init" (list . "sch.chart.config.values") -}}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "sch.names.fullName" (list .) }}-config
  labels:
{{ include "sch.metadata.labels.standard" (list . "") | indent 4 }}
data:
  incidents.conf: |2
    input {
        kafka {
            bootstrap_servers => "${KAFKA_BROKERS}"
            topics => ["incidents"]
            sasl_jaas_config => "org.apache.kafka.common.security.plain.PlainLoginModule required username='${KAFKA_USER}' password='${KAFKA_PASSWORD}';"
            sasl_mechanism => "PLAIN"
            codec => "json"
            client_id => "logstash-incidents"
            group_id => "logstash-incidents"
        }
    }
    filter {
        mutate {
            remove_field => [ "rel_resources" ] # Delete rel_resources, as it can contain a lot of data which is not
                                                # required here.
        }
        # The following filters will tag messages that are required for metrics aggregation
        if [type] == "resource-created" {
            mutate {
                add_tag => [ "relevant", "incidentCreated" ]
            }
        }

        if [type] == "resource-updated" and [resource][incidentState] != "onHold" {
            # timespan between prev_resource and resource is time that was spent on hold, no matter what the new state is
            mutate {
                add_tag => [ "relevant", "incidentMayHaveBeenHeld" ]
            }
        }

        if [type]  == "resource-updated" and [resource][incidentState] == "onHold" {
            mutate {
                add_tag => [ "relevant", "incidentOnHold" ]
            }
        }

        if [type] == "resource-updated" and [resource][incidentState] == "inProgress" {
            # definition of "responded" is the first time an incident reaches "inProgress" state
            mutate {
                add_tag => [ "relevant", "incidentResponded" ]
            }
        } 
        if [type] == "resource-updated" and [resource][incidentState] == "resolved" {
            mutate {
                add_tag => [ "relevant", "incidentResolved" ]
            }
        }
        if [type] == "resource-updated" and [resource][incidentState] == "closed" {
            mutate {
                add_tag => [ "relevant", "incidentClosed" ]
            }
        }
    
        # drop all messages that are not relevant for metrics calculation
        if "relevant" not in [tags] {
            drop { }
        } else {
            mutate {
                remove_tag => [ "relevant" ]
            }
        }
        date {
            match => [ "[resource][createdTime]", "UNIX_MS", "ISO8601" ]
        }
    }
    output {
        elasticsearch {
            hosts => ["${ELASTICSEARCH_API}"]
            index => "logstash-incidents-%{+YYYY.MM.dd}"
            workers => 1
            document_id => "%{uuid}"
            action => "update"
            scripted_upsert => true
            script_type => "inline" # could be set to "indexed" later if the script is already present on elasticsearch - may improve performance
            script_lang => "painless"
            script => "
                // Parse lastChanged, which is the timestamp of the incident update described by this message
                ZonedDateTime epochTime = ZonedDateTime.of(1970, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC);
                long lastChanged;
                if (params.event.resource.lastChanged instanceof java.lang.Long) {
                    lastChanged = params.event.resource.lastChanged;
                } else {
                    // This means that params.event.resource.lastChanged must be a String
                    lastChanged = epochTime.until(ZonedDateTime.parse(params.event.resource.lastChanged), ChronoUnit.MILLIS);
                }
                // If this is a create operation (as opposed to an update operation) write metrics fundamentals
                if ('create'.equals(ctx.op)) {
                    ctx._source['subscription'] = params.event.subscription;
                    ctx._source['uuid'] = params.event.uuid;
                    ctx._source['state'] = 'open';
                    ctx._source['timeOnHold'] = 0;
                    // The start time of a hold--defaults to zero when the incident is not on hold
                    ctx._source['holdStartTime'] = 0;
                    // determine if pararms.event.resource.createdTime is a long with epoch milliseconds or a string, & turn it into a long if it is a string
                    if (params.event.resource.createdTime instanceof java.lang.Long) {
                        ctx._source['createTime'] =  params.event.resource.createdTime;
                    } else {
                        // This means that params.event.resource.createdTime must be a String
                        ctx._source['createTime'] = epochTime.until(ZonedDateTime.parse(params.event.resource.createdTime), ChronoUnit.MILLIS);
                    }
                }
                // only set priority if there is no priority yet, or the stored priority is less recent than the incoming priority
                if (!(ctx._source.containsKey('priorityTime') && lastChanged < ctx._source['priorityTime'])) {
                    ctx._source['priority'] = params.event.resource.priority;
                    ctx._source['priorityTime'] = lastChanged;
                }
                // for incidentMayHaveBeenHeld messages increase the stored timeOnHold by the time range covered in this message if the incident
                // was previously on hold, but now is not (also ensure that negative times due to race conditions are never stored)
                if (params.event.tags.contains('incidentMayHaveBeenHeld') && ctx._source['holdStartTime'] != 0 && (lastChanged - ctx._source['holdStartTime']) > 0) {
                    ctx._source['timeOnHold'] += lastChanged - ctx._source['holdStartTime'];
                    // ChronoUnit.MILLIS.between(epochTime.plus(ctx._source['holdStartTime'], ChronoUnit.MILLIS), lastChanged);
                    ctx._source['holdStartTime'] = 0;
                }
                // For incidentOnHold messages, mark the time when the incident was first put on hold
                if (params.event.tags.contains('incidentOnHold') && ctx._source['holdStartTime'] == 0) {
                    ctx._source['holdStartTime'] = lastChanged;
                }
                // for incidentResponded messages set the timeToRespond metric if required
                if (params.event.tags.contains('incidentResponded')) {
                    // only set timeToRespond if there is no timeToRespond yet or the stored respondTime is later than the incoming respondTime
                    // (which can be caused by kafka messages not arriving in same order as they were sent)
                    if (!(ctx._source.containsKey('respondTime') && lastChanged > ctx._source['respondTime'])) {
                        ctx._source['respondTime'] = lastChanged;
                        ctx._source['timeToRespond'] = lastChanged - ctx._source['createTime'];
                        // ctx._source['timeToRespond'] = ChronoUnit.MILLIS.between(ZonedDateTime.parse(ctx._source['createTime']), lastChanged);
                    }
                }
                // For incidentResolved messages, set the timeToResolve metric if required
                if (params.event.tags.contains('incidentResolved')) {
                    // only set timeToResolve if there is no timeToResolve yet or the stored resolveTime is less recent than the incoming resolveTime
                    // (could theoretically be caused by reopened incidents)
                    if (!(ctx._source.containsKey('resolveTime') && !(lastChanged > ctx._source['resolveTime']))) {

                        ctx._source['resolveTime'] = lastChanged;
                        ctx._source['timeToResolve'] = lastChanged - ctx._source['createTime'];
                        // ctx._source['timeToResolve'] = ChronoUnit.MILLIS.between(ZonedDateTime.parse(ctx._source['createTime']), lastChanged);
                    }
                }
                // For incidentClosed messages set the state to 'closed'
                if (params.event.tags.contains('incidentClosed')) {
                    ctx._source['state'] = 'closed';
                }
            "
        }
    }

  ea-collatedactions.conf: |2
    input{
        kafka {
            bootstrap_servers => "${KAFKA_BROKERS}"
            topics => ["ea-actions"]
            sasl_jaas_config => "org.apache.kafka.common.security.plain.PlainLoginModule required username='${KAFKA_USER}' password='${KAFKA_PASSWORD}';"
            sasl_mechanism => "PLAIN"
            codec => "json"
            group_id => "ea-pattern-metrics"
            auto_offset_reset => "earliest"
            client_id => "ea-pattern-metrics"
        }
    }
    filter {
        fingerprint {
            source => "[id]"
            method => "MURMUR3"
        }
    }
    output{
        if [metadata][source] == "analytics.temporal-patterns" and [metricData]{
            elasticsearch{
                hosts => ["${ELASTICSEARCH_API}"]
                index => "ea-pattern-metrics-%{+dd.MM.YYYY}"
                template_overwrite => true
                template_name => "ea-pattern-metrics"
                template => "/home/logstash/logstash/config/ea-pattern-metrics-mapping.json"
            }
        }
    }

  noi-events.conf: |2
    input {
        kafka {
            bootstrap_servers => "${KAFKA_BROKERS}"
            topics => ["noi-events"]
            sasl_jaas_config => "org.apache.kafka.common.security.plain.PlainLoginModule required username='${KAFKA_USER}' password='${KAFKA_PASSWORD}';"
            sasl_mechanism => "PLAIN"
            codec => "json"
            client_id => "logstash-noi-events"
            group_id => "logstash-noi-events"
        }
    }
    filter {
        if ![event][instanceUuid] {
            drop { }
        } else if [event][instanceUuid] == "undefined:NaN" {
            drop { }
        }
        mutate {
            rename => { "event" => "renamed_event" }
        }
        mutate {
            add_field => { "[event][objectServer]" => "%{[renamed_event][objectServer]}" }
        }
        mutate {
            remove_field => [ "renamed_event" ]
        }
        json {
            source => "[event][objectServer]"
            target => "[event][objectServer]"
        }
        date {
            match => [ "[event][objectServer][FirstOccurrence]", "ISO8601" ]
        }
    }
    output {
        elasticsearch {
            hosts => ["${ELASTICSEARCH_API}"]
            index => "logstash-noi-events-%{+YYYY.MM.dd}"
            workers => 1
            document_id => "%{[event][objectServer][InstanceUuid]}"
            action => "update"
            doc_as_upsert => true
        }
    }

  event-reduction.conf: |2
    input {
        kafka {
            bootstrap_servers => "${KAFKA_BROKERS}"
            topics => ["noi-events"]
            sasl_jaas_config => "org.apache.kafka.common.security.plain.PlainLoginModule required username='${KAFKA_USER}' password='${KAFKA_PASSWORD}';"
            sasl_mechanism => "PLAIN"
            codec => "json"
            client_id => "logstash-event-reduction"
            group_id => "logstash-event-reduction"
        }
    }
    filter {
        if ![event][instanceUuid] {
            drop { }
        } else if [event][instanceUuid] == "undefined:NaN" {
            drop { }
        } else if ![event][objectServer][Tally] {
            drop { }
        } else if ![event][lastOccurrence] {
            drop { }
        } else {
            date {
                match => [ "[event][lastOccurrence]", "ISO8601" ]
            }
            mutate { 
                add_field => { 
                    "id" => "%{[event][instanceUuid]}"
                    "occurrence" => "%{[event][objectServer][Tally]}"
                    "timestamp" => "%{[event][lastOccurrence]}"
                }
            }
            if [event][objectServer][CEACorrelationKey] {
                mutate {
                    add_field => {
                        "group" => "%{[event][objectServer][CEACorrelationKey]}"
                    }
                }
            } else {
                mutate {
                    add_field => {
                        "group" => ""
                    }
                }
            }
            mutate { 
                remove_field => ["type", "@version", "event"]  
            }
        }
    }
    output {

        # add event occurrence
        elasticsearch {
            hosts => ["${ELASTICSEARCH_API}"]
            index => "logstash-event-reduction-%{+YYYY.MM.dd}"
            workers => 1
            document_id => "Occ-%{[id]}-%{[occurrence]}"
            action => "update"
            doc_as_upsert => true
        }

        if [group] == "" {
            # add ungrouped event
            elasticsearch {
                hosts => ["${ELASTICSEARCH_API}"]
                index => "logstash-event-reduction-%{+YYYY.MM.dd}"
                workers => 1
                document_id => "Ung-%{[id]}"
                action => "update"
                scripted_upsert => true
                script_type => "inline"
                script_lang => "painless"
                script => "

                    if ('create'.equals(ctx.op)) {
                        ctx._source['timestamp'] = params.event.timestamp;
                        ctx._source['tenantid'] = params.event.tenantid;
                        ctx._source['remaining_id'] = params.event.id;
                    }
                "
            }
        } else {
            # add event group
            elasticsearch {
                hosts => ["${ELASTICSEARCH_API}"]
                index => "logstash-event-reduction-groups"
                workers => 1
                document_id => "Grp-%{[group]}"
                action => "update"
                scripted_upsert => true
                script_type => "inline"
                script_lang => "painless"
                script => "

                    if ('create'.equals(ctx.op)) {
                        ctx._source['timestamp'] = params.event.timestamp;
                        ctx._source['tenantid'] = params.event.tenantid;
                        ctx._source['remaining_id'] = params.event.group;
                    }
                "
            }

            # remove ungrouped event (if present)
            elasticsearch {
                hosts => ["${ELASTICSEARCH_API}"]
                index => "logstash-event-reduction-%{+YYYY.MM.dd}"
                workers => 1
                document_id => "Ung-%{[id]}"
                action => "delete"
            }

        }

    }

  rba-statistics.conf: |2
    input {
        kafka {
            bootstrap_servers => "${KAFKA_BROKERS}"
            topics => ["rba-statistics"]
            sasl_jaas_config => "org.apache.kafka.common.security.plain.PlainLoginModule required username='${KAFKA_USER}' password='${KAFKA_PASSWORD}';"
            sasl_mechanism => "PLAIN"
            codec => "json"
            client_id => "logstash-rba-statistics"
            group_id => "logstash-rba-statistics"
        }
    }
    filter {
        if [subscription] != "cfd95b7e-3bc7-4006-a4a8-a73a79c71255" {
            drop { }
        }
        date {
            match => [ "statisticsTimestamp", "ISO8601" ]
        }
    }
    output {
        elasticsearch {
            hosts => ["${ELASTICSEARCH_API}"]
            index => "logstash-rba-statistics-%{+YYYY.MM.dd}"
            workers => 1
            document_id => "%{[statisticsTimestamp]}"
            action => "create"
        }
    }

  rba-execution-records.conf: |2
    input {
        kafka {
            bootstrap_servers => "${KAFKA_BROKERS}"
            topics => ["rba-execution-records"]
            sasl_jaas_config => "org.apache.kafka.common.security.plain.PlainLoginModule required username='${KAFKA_USER}' password='${KAFKA_PASSWORD}';"
            sasl_mechanism => "PLAIN"
            codec => "json"
            client_id => "logstash-rba-execution-records"
            group_id => "logstash-rba-execution-records"
        }
    }
    filter {
        if [subscription] != "cfd95b7e-3bc7-4006-a4a8-a73a79c71255" {
            drop { }
        }
        date {
            match => [ "createdAt", "ISO8601" ]
        }
        mutate {
            remove_field => [ "context" ]
        }
    }
    output {
        elasticsearch {
            hosts => ["${ELASTICSEARCH_API}"]
            index => "logstash-rba-execution-records-%{+YYYY.MM.dd}"
            workers => 1
            document_id => "%{[id]}"
            action => "update"
            doc_as_upsert => true
        }
    }

  pipelines.yml: |2
    - pipeline.id: incidents
      path.config: "/home/logstash/logstash/config/incidents.conf"
    - pipeline.id: ea-collatedactions
      path.config: "/home/logstash/logstash/config/ea-collatedactions.conf"
    - pipeline.id: noi-events
      path.config: "/home/logstash/logstash/config/noi-events.conf"
    - pipeline.id: event-reduction
      path.config: "/home/logstash/logstash/config/event-reduction.conf"
    - pipeline.id: rba-statistics
      path.config: "/home/logstash/logstash/config/rba-statistics.conf"
    - pipeline.id: rba-execution-records
      path.config: "/home/logstash/logstash/config/rba-execution-records.conf"


  logstash.yml: |2
    http.host: 0.0.0.0
    http.port: 9600

  ea-pattern-metrics-mapping.json: |2
    {
        "index_patterns": ["ea-pattern-metrics*"],
        "template": "ea-pattern-metrics",
        "mappings": {
            "doc": {
                "_source": {
                "enabled": true
                },
                "properties": {
                    "metadata": {
                        "properties": {
                            "alertId": {
                                "type": "keyword"
                            },
                            "groupId": {
                                "type": "keyword"
                            },
                            "detailsId": {
                                "type": "keyword"
                            },
                            "source": {
                                "type": "keyword"
                            }
                        }
                    },
                    "metricData": {
                        "properties": {
                            "resourceFields": {
                                "type": "keyword"
                            },
                            "resourceValues": {
                                "type": "keyword"
                            }
                        }
                    },
                    "id": {
                        "type": "keyword"
                    },
                    "policyTimestamp": {
                        "type": "long"
                    },
                    "type": {
                        "type": "keyword"
                    },
                    "policyId": {
                        "type": "keyword"
                    },
                    "tenantId": {
                        "type": "keyword"
                    }
                }
            }
        }
    }
