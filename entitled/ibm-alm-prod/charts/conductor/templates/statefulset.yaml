{{- include "sch.config.init" (list . "alm.sch.chart.config.values") -}}
{{- $rootData := fromYaml (include "parent.data" .) }}
{{- $rootMetering := $rootData.metering -}}
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: conductor
  labels:
{{ include "sch.metadata.labels.standard" (list . "") | indent 4 }}
spec:
  serviceName: "conductor"
  replicas: {{ .Values.app.numReplicas }}
  selector:
    matchLabels:
{{ include "sch.metadata.labels.standard" (list . "") | indent 6 }}
  podManagementPolicy: Parallel
  template:
    metadata:
      annotations:
        checksum/config: {{ include (print $.Template.BasePath "/configmap.yaml") . | sha256sum }}
        {{- include "sch.metadata.annotations.metering" (list . $rootMetering) | indent 8 }}
      labels:
{{ include "sch.metadata.labels.standard" (list . "") | indent 8 }}
    spec:
{{ include "alm.podSecurityContext" (list . "") | indent 6 }}
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - {{ .Values.arch }}
      initContainers:
      - name: wait-for-vault
        image: {{ include "alm.getImageRepo" . | trimSuffix "/" }}/{{.Values.baseImage.name}}:{{.Values.baseImage.tag}}
        imagePullPolicy: {{ .Values.global.image.pullPolicy }}
        env:
        - name: SERVICE
          value: {{ .Release.Name }}-vault
        - name: EXPECTED_COUNT
          value: "1"
        command:
        - /bin/sh
        - -c
        - python3 /scripts/check-service-endpoint-count.py
        securityContext:
{{ include "alm.containerSecurityContext" (list . "") | indent 10 }}
      containers:
      - name: conductor
        command:
        - /data/start.sh
        image: "{{ include "alm.getImageRepo" . | trimSuffix "/" }}/{{.Values.image.name}}:{{.Values.image.tag}}"
        imagePullPolicy: {{ .Values.global.image.pullPolicy }}
        ports:
        - containerPort: 8761
          protocol: TCP
        livenessProbe:
          failureThreshold: {{ .Values.app.livenessProbe.failureThreshold }}
          httpGet:
            path: /management/health
            port: 8761
            scheme: {{- if .Values.global.security.enabled}} HTTPS {{- else }} HTTP {{- end }}
          initialDelaySeconds: {{ .Values.app.livenessProbe.initialDelaySeconds }}
          periodSeconds: {{ .Values.app.livenessProbe.periodSeconds }}
        readinessProbe:
          failureThreshold: {{ .Values.app.readinessProbe.failureThreshold }}
          httpGet:
            path: /management/health
            port: 8761
            scheme: {{- if .Values.global.security.enabled}} HTTPS {{- else }} HTTP {{- end }}
          initialDelaySeconds: {{ .Values.app.readinessProbe.initialDelaySeconds }}
          periodSeconds: {{ .Values.app.readinessProbe.periodSeconds }}
        envFrom:
        - configMapRef:
            name: conductor
        env:
        # See https://stackoverflow.com/questions/40567429/eureka-and-kubernetes for information on Eureka/Kubernetes config
        # The hostnames must match with the the eureka serviceUrls, otherwise the replicas are reported as unavailable in the eureka dashboard
        - name: eureka_instance_hostname
          value: ${HOSTNAME}.conductor
        - name: eureka_client_serviceUrl_defaultZone
          value: {{ include "conductor.serviceUrl" . | trimSuffix "," }}
        # - name: numReplicas
        #   value: {{ .Values.app.numReplicas | quote }}
        # - name: secure
        #   value: {{ .Values.global.security.enabled | quote }}
        - name: JVM_OPTIONS
          value: {{ .Values.jvmOptions }}
        - name: LOG_FOLDER
          value: {{ .Values.logFolder }}
        - name: spring_cloud_vault_token
          valueFrom:
            secretKeyRef:
              name: {{ printf "%s-%s" .Release.Name "vault-keys" }}
              key: alm_token
        resources:
{{ include "alm.getResources" . | indent 10 }}
        securityContext:
{{ include "alm.containerSecurityContext" (list . "") | indent 10 }}
        volumeMounts:
        - name: vault-cert
          mountPath: /var/lm/vault/certs
        - name: lm-certs
          mountPath: /var/lm/certs
        - name: logs
          mountPath: {{ .Values.logFolder }}
        - name: tmp
          mountPath: /tmp
        - name: truststore
          mountPath: /var/lm/truststore
      volumes:
      - name: logs
        emptyDir: {}
      - name: tmp
        emptyDir: {}
      - name: truststore
        emptyDir: {}
      - name: lm-certs
        secret:
          secretName: {{ .Values.global.security.almCerts.secretName | default (printf "%s-%s" .Release.Name "security-default") }}
      - name: vault-cert
        secret:
          secretName: {{ .Values.global.security.vaultCerts.secretName | default (printf "%s-%s" .Release.Name "vault-default") }}
