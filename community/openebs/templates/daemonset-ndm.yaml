apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: {{ template "openebs.fullname" . }}-ndm
  labels:
    app: {{ template "openebs.name" . }}
    chart: {{ template "openebs.chart" . }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
    component: ndm
spec:
  template:
    metadata:
      labels:
        app: {{ template "openebs.name" . }}
        release: {{ .Release.Name }}
        component: ndm
      annotations:
        productName: "OpenEBS Node Disk Manager"
        productID: "io.openebs.chartscommunity.ndm.0.7.0"
        productVersion: "0.7.0"
    spec:
      serviceAccountName: {{ template "openebs.serviceAccountName" . }}
      hostNetwork: true
      containers:
      - name: {{ template "openebs.name" . }}-ndm
        image: "{{ .Values.ndm.image }}:{{ .Values.ndm.imageTag }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        command:
        - /usr/sbin/ndm
        - start
        securityContext:
          privileged: true
        env:
        # pass hostname as env variable using downward API to the NDM container
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
{{- if .Values.ndm.sparse }}
{{- if .Values.ndm.sparse.path }}
        # specify the directory where the sparse files need to be created.
        # if not specified, then sparse files will not be created.
        - name: SPARSE_FILE_DIR
          value: "{{ .Values.ndm.sparse.path }}"
{{- end }}
{{- if .Values.ndm.sparse.size }}
        # Size(bytes) of the sparse file to be created.
        - name: SPARSE_FILE_SIZE
          value: "{{ .Values.ndm.sparse.size }}"
{{- end }}
{{- if .Values.ndm.sparse.count }}
        # Specify the number of sparse files to be created
        - name: SPARSE_FILE_COUNT
          value: "{{ .Values.ndm.sparse.count }}"
{{- end }}
{{- end }}
        livenessProbe:
          exec:
            command:
            - pgrep
            - ".*ndm"
          initialDelaySeconds: 300
          periodSeconds: 60
        readinessProbe:
          exec:
            command:
            - pgrep
            - ".*ndm"
          initialDelaySeconds: 30
          periodSeconds: 60
        volumeMounts:
        - name: config
          mountPath: /host/node-disk-manager.config
          subPath: node-disk-manager.config
          readOnly: true
        - name: udev
          mountPath: /run/udev
        - name: procmount
          mountPath: /host/mounts
{{- if .Values.ndm.sparse }}
{{- if .Values.ndm.sparse.path }}
        - name: sparsepath
          mountPath: {{ .Values.ndm.sparse.path }}
{{- end }}
{{- end }}
      volumes:
      - name: config
        configMap:
          name: {{ template "openebs.fullname" . }}-ndm-config
      - name: udev
        hostPath:
          path: /run/udev
          type: Directory
      # mount /proc/1/mounts (mount file of process 1 of host) inside container
      # to read which partition is mounted on / path
      - name: procmount
        hostPath:
          path: /proc/1/mounts
{{- if .Values.ndm.sparse }}
{{- if .Values.ndm.sparse.path }}
      - name: sparsepath
        hostPath:
          path: {{ .Values.ndm.sparse.path }}
{{- end }}
{{- end }}
      # By default the node-disk-manager will be run on all kubernetes nodes
      # If you would like to limit this to only some nodes, say the nodes
      # that have storage attached, you could label those node and use
      # nodeSelector.
      #
      # e.g. label the storage nodes with - "openebs.io/nodegroup"="storage-node"
      # kubectl label node <node-name> "openebs.io/nodegroup"="storage-node"
      #nodeSelector:
      #  "openebs.io/nodegroup": "storage-node"
{{- if .Values.ndm.nodeSelector }}
      nodeSelector:
{{ toYaml .Values.ndm.nodeSelector | indent 8 }}
{{- end }}
{{- if .Values.ndm.tolerations }}
      tolerations:
{{ toYaml .Values.ndm.tolerations | indent 8 }}
{{- end }}
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          #If you specify multiple nodeSelectorTerms associated with nodeAffinity types,
          #then the pod can be scheduled onto a node if one of the nodeSelectorTerms is satisfied.
          #
          #If you specify multiple matchExpressions associated with nodeSelectorTerms,
          #then the pod can be scheduled onto a node only if all matchExpressions can be satisfied.
          #
          #valid operators: In, NotIn, Exists, DoesNotExist, Gt, Lt
            nodeSelectorTerms:
            - matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - {{ .Values.image.arch }}
